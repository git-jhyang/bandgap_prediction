{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, gc\n",
    "from tgnn.util import crystal_conv as cc\n",
    "import tgnn.util.trainer as tr\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tgnn.model.TGNN import TGNN\n",
    "from tgnn.model.CGCNN import CGCNN\n",
    "from tgnn.util.AdaBound import AdaBound\n",
    "\n",
    "root = 'c:/WORKSPACE_KRICT/DATA/data_snu/poscar'\n",
    "\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "cc.load_mat_atom_feats()\n",
    "list_test_mae = list()\n",
    "list_test_rmse = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading 100th crystal.\n",
      "Complete loading 200th crystal.\n",
      "Complete loading 300th crystal.\n",
      "Complete loading 400th crystal.\n",
      "Complete loading 500th crystal.\n",
      "Complete loading 600th crystal.\n",
      "Complete loading 700th crystal.\n",
      "Complete loading 800th crystal.\n",
      "Complete loading 900th crystal.\n",
      "Complete loading 1000th crystal.\n",
      "Complete loading 1100th crystal.\n",
      "Complete loading 1200th crystal.\n",
      "Complete loading 1300th crystal.\n",
      "Complete loading 1400th crystal.\n",
      "Complete loading 1500th crystal.\n",
      "Complete loading 1600th crystal.\n",
      "Complete loading 1700th crystal.\n",
      "Complete loading 1800th crystal.\n",
      "Complete loading 1900th crystal.\n",
      "Complete loading 2000th crystal.\n",
      "Complete loading 2100th crystal.\n",
      "Complete loading 2200th crystal.\n",
      "Complete loading 2300th crystal.\n",
      "Complete loading 2400th crystal.\n",
      "Complete loading 2500th crystal.\n",
      "Complete loading 2600th crystal.\n",
      "Complete loading 2700th crystal.\n",
      "Complete loading 2800th crystal.\n",
      "Complete loading 2900th crystal.\n",
      "Complete loading 3000th crystal.\n",
      "Complete loading 3100th crystal.\n",
      "Complete loading 3200th crystal.\n",
      "Complete loading 3300th crystal.\n",
      "Complete loading 3400th crystal.\n",
      "Complete loading 3500th crystal.\n",
      "Complete loading 3600th crystal.\n",
      "Complete loading 3700th crystal.\n",
      "Complete loading 3800th crystal.\n",
      "Complete loading 3900th crystal.\n",
      "Complete loading 4000th crystal.\n",
      "Complete loading 4100th crystal.\n",
      "Complete loading 4200th crystal.\n",
      "Complete loading 4300th crystal.\n",
      "Complete loading 4400th crystal.\n",
      "Complete loading 4500th crystal.\n",
      "Complete loading 4600th crystal.\n",
      "Complete loading 4700th crystal.\n",
      "Complete loading 4800th crystal.\n",
      "Complete loading 4900th crystal.\n",
      "Complete loading 5000th crystal.\n",
      "Complete loading 5100th crystal.\n",
      "Complete loading 5200th crystal.\n",
      "Complete loading 5300th crystal.\n",
      "Complete loading 5400th crystal.\n",
      "Complete loading 5500th crystal.\n",
      "Complete loading 5600th crystal.\n",
      "Complete loading 5700th crystal.\n",
      "Complete loading 5800th crystal.\n",
      "Complete loading 5900th crystal.\n",
      "Complete loading 6000th crystal.\n",
      "Complete loading 6100th crystal.\n",
      "Complete loading 6200th crystal.\n",
      "Complete loading 6300th crystal.\n",
      "Complete loading 6400th crystal.\n",
      "Complete loading 6500th crystal.\n",
      "Complete loading 6600th crystal.\n",
      "Complete loading 6700th crystal.\n",
      "Complete loading 6800th crystal.\n",
      "Complete loading 6900th crystal.\n",
      "Complete loading 7000th crystal.\n",
      "Complete loading 7100th crystal.\n",
      "Complete loading 7200th crystal.\n",
      "Complete loading 7300th crystal.\n",
      "Complete loading 7400th crystal.\n",
      "Complete loading 7500th crystal.\n",
      "Complete loading 7600th crystal.\n",
      "Complete loading 7700th crystal.\n",
      "Complete loading 7800th crystal.\n",
      "Complete loading 7900th crystal.\n",
      "Complete loading 8000th crystal.\n",
      "Complete loading 8100th crystal.\n",
      "Complete loading 8200th crystal.\n",
      "Complete loading 8300th crystal.\n",
      "Complete loading 8400th crystal.\n",
      "Complete loading 8500th crystal.\n",
      "Complete loading 8600th crystal.\n",
      "Complete loading 8700th crystal.\n",
      "Complete loading 8800th crystal.\n",
      "Complete loading 8900th crystal.\n",
      "Complete loading 9000th crystal.\n",
      "Complete loading 9100th crystal.\n",
      "Complete loading 9200th crystal.\n",
      "Complete loading 9300th crystal.\n",
      "Complete loading 9400th crystal.\n",
      "Complete loading 9500th crystal.\n",
      "Complete loading 9600th crystal.\n",
      "Complete loading 9700th crystal.\n",
      "Complete loading 9800th crystal.\n",
      "Complete loading 9900th crystal.\n",
      "Complete loading 10000th crystal.\n",
      "Complete loading 10100th crystal.\n",
      "Complete loading 10200th crystal.\n",
      "Complete loading 10300th crystal.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WORKSPACE_KRICT\\CODES\\band_gap_model\\notebooks\\tgnn\\util\\AdaBound.py:91: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\tTrain loss: 0.8487\tVal loss: 0.2924 (0.4375)\n",
      "Epoch [2/300]\tTrain loss: 0.3485\tVal loss: 0.5601 (0.6737)\n",
      "Epoch [3/300]\tTrain loss: 0.3350\tVal loss: 0.4528 (0.6445)\n",
      "Epoch [4/300]\tTrain loss: 0.3102\tVal loss: 0.2645 (0.3523)\n",
      "Epoch [5/300]\tTrain loss: 0.3147\tVal loss: 0.2771 (0.4373)\n",
      "Epoch [6/300]\tTrain loss: 0.3043\tVal loss: 0.2389 (0.3993)\n",
      "Epoch [7/300]\tTrain loss: 0.3214\tVal loss: 0.2716 (0.4567)\n",
      "Epoch [8/300]\tTrain loss: 0.2918\tVal loss: 0.2099 (0.3300)\n",
      "Epoch [9/300]\tTrain loss: 0.3196\tVal loss: 0.5267 (0.6726)\n",
      "Epoch [10/300]\tTrain loss: 0.2984\tVal loss: 0.3079 (0.3769)\n",
      "Epoch [11/300]\tTrain loss: 0.3151\tVal loss: 0.2429 (0.3611)\n",
      "Epoch [12/300]\tTrain loss: 0.3277\tVal loss: 0.2423 (0.3603)\n",
      "Epoch [13/300]\tTrain loss: 0.3193\tVal loss: 0.9558 (1.2236)\n",
      "Epoch [14/300]\tTrain loss: 0.2873\tVal loss: 0.2418 (0.3663)\n",
      "Epoch [15/300]\tTrain loss: 0.2903\tVal loss: 0.2190 (0.2997)\n",
      "Epoch [16/300]\tTrain loss: 0.3037\tVal loss: 0.3300 (0.4296)\n",
      "Epoch [17/300]\tTrain loss: 0.2968\tVal loss: 0.2607 (0.3575)\n",
      "Epoch [18/300]\tTrain loss: 0.2702\tVal loss: 0.2060 (0.3031)\n",
      "Epoch [19/300]\tTrain loss: 0.2966\tVal loss: 0.3325 (0.4222)\n",
      "Epoch [20/300]\tTrain loss: 0.2688\tVal loss: 0.2258 (0.3235)\n",
      "Epoch [21/300]\tTrain loss: 0.2628\tVal loss: 0.1988 (0.2934)\n",
      "Epoch [22/300]\tTrain loss: 0.2431\tVal loss: 0.2327 (0.3563)\n",
      "Epoch [23/300]\tTrain loss: 0.2425\tVal loss: 0.2064 (0.3083)\n",
      "Epoch [24/300]\tTrain loss: 0.2379\tVal loss: 0.1842 (0.2262)\n",
      "Epoch [25/300]\tTrain loss: 0.2388\tVal loss: 0.2051 (0.3143)\n",
      "Epoch [26/300]\tTrain loss: 0.2475\tVal loss: 0.2356 (0.2723)\n",
      "Epoch [27/300]\tTrain loss: 0.2419\tVal loss: 0.3378 (0.4389)\n",
      "Epoch [28/300]\tTrain loss: 0.2288\tVal loss: 0.1903 (0.2559)\n",
      "Epoch [29/300]\tTrain loss: 0.2361\tVal loss: 0.2295 (0.3534)\n",
      "Epoch [30/300]\tTrain loss: 0.2380\tVal loss: 0.2967 (0.4430)\n",
      "Epoch [31/300]\tTrain loss: 0.2515\tVal loss: 0.1909 (0.3026)\n",
      "Epoch [32/300]\tTrain loss: 0.2290\tVal loss: 0.2279 (0.3064)\n",
      "Epoch [33/300]\tTrain loss: 0.2443\tVal loss: 0.2101 (0.2954)\n",
      "Epoch [34/300]\tTrain loss: 0.2352\tVal loss: 0.2035 (0.2937)\n",
      "Epoch [35/300]\tTrain loss: 0.2295\tVal loss: 0.2454 (0.3060)\n",
      "Epoch [36/300]\tTrain loss: 0.2254\tVal loss: 0.1906 (0.2896)\n",
      "Epoch [37/300]\tTrain loss: 0.2273\tVal loss: 0.2039 (0.2575)\n",
      "Epoch [38/300]\tTrain loss: 0.2326\tVal loss: 0.1982 (0.3243)\n",
      "Epoch [39/300]\tTrain loss: 0.2268\tVal loss: 0.2427 (0.3487)\n",
      "Epoch [40/300]\tTrain loss: 0.2363\tVal loss: 0.1980 (0.2889)\n",
      "Epoch [41/300]\tTrain loss: 0.2270\tVal loss: 0.1781 (0.2417)\n",
      "Epoch [42/300]\tTrain loss: 0.2258\tVal loss: 0.2070 (0.2966)\n",
      "Epoch [43/300]\tTrain loss: 0.2398\tVal loss: 0.1855 (0.2867)\n",
      "Epoch [44/300]\tTrain loss: 0.2140\tVal loss: 0.1950 (0.2756)\n",
      "Epoch [45/300]\tTrain loss: 0.2317\tVal loss: 0.2011 (0.3133)\n",
      "Epoch [46/300]\tTrain loss: 0.2262\tVal loss: 0.1993 (0.2744)\n",
      "Epoch [47/300]\tTrain loss: 0.2190\tVal loss: 0.1963 (0.3061)\n",
      "Epoch [48/300]\tTrain loss: 0.2292\tVal loss: 0.1687 (0.2233)\n",
      "Epoch [49/300]\tTrain loss: 0.2314\tVal loss: 0.2414 (0.2868)\n",
      "Epoch [50/300]\tTrain loss: 0.2311\tVal loss: 0.2697 (0.3745)\n",
      "Epoch [51/300]\tTrain loss: 0.2212\tVal loss: 0.1679 (0.2826)\n",
      "Epoch [52/300]\tTrain loss: 0.2128\tVal loss: 0.2453 (0.3278)\n",
      "Epoch [53/300]\tTrain loss: 0.2264\tVal loss: 0.2038 (0.2774)\n",
      "Epoch [54/300]\tTrain loss: 0.2330\tVal loss: 0.2176 (0.3490)\n",
      "Epoch [55/300]\tTrain loss: 0.2303\tVal loss: 0.1793 (0.2899)\n",
      "Epoch [56/300]\tTrain loss: 0.2086\tVal loss: 0.1930 (0.2899)\n",
      "Epoch [57/300]\tTrain loss: 0.2280\tVal loss: 0.2100 (0.2990)\n",
      "Epoch [58/300]\tTrain loss: 0.2230\tVal loss: 0.2386 (0.2970)\n",
      "Epoch [59/300]\tTrain loss: 0.2281\tVal loss: 0.1974 (0.2789)\n",
      "Epoch [60/300]\tTrain loss: 0.2079\tVal loss: 0.1659 (0.2204)\n",
      "Epoch [61/300]\tTrain loss: 0.2098\tVal loss: 0.1701 (0.2495)\n",
      "Epoch [62/300]\tTrain loss: 0.2154\tVal loss: 0.1864 (0.2146)\n",
      "Epoch [63/300]\tTrain loss: 0.2326\tVal loss: 0.2683 (0.3973)\n",
      "Epoch [64/300]\tTrain loss: 0.2143\tVal loss: 0.2195 (0.3006)\n",
      "Epoch [65/300]\tTrain loss: 0.2191\tVal loss: 0.3322 (0.5680)\n",
      "Epoch [66/300]\tTrain loss: 0.2113\tVal loss: 0.3155 (0.4054)\n",
      "Epoch [67/300]\tTrain loss: 0.2036\tVal loss: 0.2177 (0.3170)\n",
      "Epoch [68/300]\tTrain loss: 0.2087\tVal loss: 0.2049 (0.2914)\n",
      "Epoch [69/300]\tTrain loss: 0.2071\tVal loss: 0.1957 (0.2927)\n",
      "Epoch [70/300]\tTrain loss: 0.2230\tVal loss: 0.1878 (0.2548)\n",
      "Epoch [71/300]\tTrain loss: 0.2181\tVal loss: 0.1834 (0.2773)\n",
      "Epoch [72/300]\tTrain loss: 0.2003\tVal loss: 0.1619 (0.3037)\n",
      "Epoch [73/300]\tTrain loss: 0.2187\tVal loss: 0.2250 (0.2518)\n",
      "Epoch [74/300]\tTrain loss: 0.2022\tVal loss: 0.2847 (0.3807)\n",
      "Epoch [75/300]\tTrain loss: 0.2154\tVal loss: 0.1731 (0.2468)\n",
      "Epoch [76/300]\tTrain loss: 0.2013\tVal loss: 0.1859 (0.2675)\n",
      "Epoch [77/300]\tTrain loss: 0.2031\tVal loss: 0.1785 (0.2538)\n",
      "Epoch [78/300]\tTrain loss: 0.2091\tVal loss: 0.1685 (0.2929)\n",
      "Epoch [79/300]\tTrain loss: 0.2055\tVal loss: 0.2229 (0.2795)\n",
      "Epoch [80/300]\tTrain loss: 0.2133\tVal loss: 0.2543 (0.3117)\n",
      "Epoch [81/300]\tTrain loss: 0.2206\tVal loss: 0.2081 (0.3295)\n",
      "Epoch [82/300]\tTrain loss: 0.1924\tVal loss: 0.1902 (0.3058)\n",
      "Epoch [83/300]\tTrain loss: 0.2078\tVal loss: 0.2971 (0.4670)\n",
      "Epoch [84/300]\tTrain loss: 0.1906\tVal loss: 0.2104 (0.2926)\n",
      "Epoch [85/300]\tTrain loss: 0.2020\tVal loss: 0.1992 (0.2532)\n",
      "Epoch [86/300]\tTrain loss: 0.1903\tVal loss: 0.2212 (0.3656)\n",
      "Epoch [87/300]\tTrain loss: 0.2117\tVal loss: 0.2135 (0.3210)\n",
      "Epoch [88/300]\tTrain loss: 0.1971\tVal loss: 0.1622 (0.2574)\n",
      "Epoch [89/300]\tTrain loss: 0.2063\tVal loss: 0.2067 (0.2628)\n",
      "Epoch [90/300]\tTrain loss: 0.1824\tVal loss: 0.2109 (0.3198)\n",
      "Epoch [91/300]\tTrain loss: 0.1963\tVal loss: 0.1913 (0.2835)\n",
      "Epoch [92/300]\tTrain loss: 0.1892\tVal loss: 0.2176 (0.2809)\n",
      "Epoch [93/300]\tTrain loss: 0.1859\tVal loss: 0.2057 (0.3318)\n",
      "Epoch [94/300]\tTrain loss: 0.1956\tVal loss: 0.1669 (0.2618)\n",
      "Epoch [95/300]\tTrain loss: 0.1955\tVal loss: 0.3232 (0.4215)\n",
      "Epoch [96/300]\tTrain loss: 0.2040\tVal loss: 0.2670 (0.4020)\n",
      "Epoch [97/300]\tTrain loss: 0.1888\tVal loss: 0.2223 (0.3092)\n",
      "Epoch [98/300]\tTrain loss: 0.1738\tVal loss: 0.1872 (0.3160)\n",
      "Epoch [99/300]\tTrain loss: 0.1855\tVal loss: 0.2216 (0.2513)\n",
      "Epoch [100/300]\tTrain loss: 0.1952\tVal loss: 0.1760 (0.2676)\n",
      "Epoch [101/300]\tTrain loss: 0.1883\tVal loss: 0.2997 (0.3795)\n",
      "Epoch [102/300]\tTrain loss: 0.1845\tVal loss: 0.1780 (0.2619)\n",
      "Epoch [103/300]\tTrain loss: 0.1940\tVal loss: 0.2446 (0.3930)\n",
      "Epoch [104/300]\tTrain loss: 0.1927\tVal loss: 0.1913 (0.2870)\n",
      "Epoch [105/300]\tTrain loss: 0.1943\tVal loss: 0.2093 (0.2939)\n",
      "Epoch [106/300]\tTrain loss: 0.1794\tVal loss: 0.2131 (0.3224)\n",
      "Epoch [107/300]\tTrain loss: 0.1776\tVal loss: 0.2139 (0.3206)\n",
      "Epoch [108/300]\tTrain loss: 0.1856\tVal loss: 0.2081 (0.2896)\n",
      "Epoch [109/300]\tTrain loss: 0.2097\tVal loss: 0.2095 (0.2539)\n",
      "Epoch [110/300]\tTrain loss: 0.1892\tVal loss: 0.1733 (0.2192)\n",
      "Epoch [111/300]\tTrain loss: 0.1872\tVal loss: 0.1656 (0.2735)\n",
      "Epoch [112/300]\tTrain loss: 0.2083\tVal loss: 0.2087 (0.2647)\n",
      "Epoch [113/300]\tTrain loss: 0.1777\tVal loss: 0.1663 (0.2433)\n",
      "Epoch [114/300]\tTrain loss: 0.1814\tVal loss: 0.2021 (0.2975)\n",
      "Epoch [115/300]\tTrain loss: 0.1828\tVal loss: 0.2265 (0.2544)\n",
      "Epoch [116/300]\tTrain loss: 0.1977\tVal loss: 0.2709 (0.3155)\n",
      "Epoch [117/300]\tTrain loss: 0.1893\tVal loss: 0.1691 (0.2610)\n",
      "Epoch [118/300]\tTrain loss: 0.1935\tVal loss: 0.2030 (0.2433)\n",
      "Epoch [119/300]\tTrain loss: 0.1832\tVal loss: 0.1689 (0.2939)\n",
      "Epoch [120/300]\tTrain loss: 0.1827\tVal loss: 0.1914 (0.2860)\n",
      "Epoch [121/300]\tTrain loss: 0.1880\tVal loss: 0.2135 (0.3079)\n",
      "Epoch [122/300]\tTrain loss: 0.1787\tVal loss: 0.2094 (0.3239)\n",
      "Epoch [123/300]\tTrain loss: 0.1842\tVal loss: 0.2239 (0.3094)\n",
      "Epoch [124/300]\tTrain loss: 0.1838\tVal loss: 0.1982 (0.2948)\n",
      "Epoch [125/300]\tTrain loss: 0.1768\tVal loss: 0.1938 (0.2720)\n",
      "Epoch [126/300]\tTrain loss: 0.1874\tVal loss: 0.2072 (0.2921)\n",
      "Epoch [127/300]\tTrain loss: 0.1706\tVal loss: 0.3000 (0.3819)\n",
      "Epoch [128/300]\tTrain loss: 0.1847\tVal loss: 0.1779 (0.2218)\n",
      "Epoch [129/300]\tTrain loss: 0.1696\tVal loss: 0.2560 (0.3376)\n",
      "Epoch [130/300]\tTrain loss: 0.1946\tVal loss: 0.2126 (0.2987)\n",
      "Epoch [131/300]\tTrain loss: 0.1750\tVal loss: 0.1714 (0.2659)\n",
      "Epoch [132/300]\tTrain loss: 0.1763\tVal loss: 0.1577 (0.2490)\n",
      "Epoch [133/300]\tTrain loss: 0.1831\tVal loss: 0.1606 (0.2469)\n",
      "Epoch [134/300]\tTrain loss: 0.1990\tVal loss: 0.1836 (0.2658)\n",
      "Epoch [135/300]\tTrain loss: 0.1698\tVal loss: 0.2084 (0.3032)\n",
      "Epoch [136/300]\tTrain loss: 0.1847\tVal loss: 0.2144 (0.2873)\n",
      "Epoch [137/300]\tTrain loss: 0.2053\tVal loss: 0.2443 (0.2953)\n",
      "Epoch [138/300]\tTrain loss: 0.1851\tVal loss: 0.1593 (0.2794)\n",
      "Epoch [139/300]\tTrain loss: 0.1768\tVal loss: 0.2083 (0.2462)\n",
      "Epoch [140/300]\tTrain loss: 0.1877\tVal loss: 0.1771 (0.2755)\n",
      "Epoch [141/300]\tTrain loss: 0.1672\tVal loss: 0.2119 (0.3051)\n",
      "Epoch [142/300]\tTrain loss: 0.1817\tVal loss: 0.1959 (0.2247)\n",
      "Epoch [143/300]\tTrain loss: 0.1781\tVal loss: 0.1544 (0.2575)\n",
      "Epoch [144/300]\tTrain loss: 0.1711\tVal loss: 0.1725 (0.2158)\n",
      "Epoch [145/300]\tTrain loss: 0.1701\tVal loss: 0.1848 (0.2658)\n",
      "Epoch [146/300]\tTrain loss: 0.1760\tVal loss: 0.2262 (0.3025)\n",
      "Epoch [147/300]\tTrain loss: 0.1823\tVal loss: 0.2247 (0.3159)\n",
      "Epoch [148/300]\tTrain loss: 0.1740\tVal loss: 0.2007 (0.3012)\n",
      "Epoch [149/300]\tTrain loss: 0.1640\tVal loss: 0.1526 (0.2086)\n",
      "Epoch [150/300]\tTrain loss: 0.1670\tVal loss: 0.2005 (0.2668)\n",
      "Epoch [151/300]\tTrain loss: 0.1660\tVal loss: 0.1579 (0.2265)\n",
      "Epoch [152/300]\tTrain loss: 0.1932\tVal loss: 0.1818 (0.2497)\n",
      "Epoch [153/300]\tTrain loss: 0.1709\tVal loss: 0.1888 (0.2358)\n",
      "Epoch [154/300]\tTrain loss: 0.1672\tVal loss: 0.1805 (0.3183)\n",
      "Epoch [155/300]\tTrain loss: 0.1875\tVal loss: 0.1667 (0.2750)\n",
      "Epoch [156/300]\tTrain loss: 0.1770\tVal loss: 0.2460 (0.3217)\n",
      "Epoch [157/300]\tTrain loss: 0.1818\tVal loss: 0.1880 (0.2834)\n",
      "Epoch [158/300]\tTrain loss: 0.1805\tVal loss: 0.1700 (0.2747)\n",
      "Epoch [159/300]\tTrain loss: 0.1647\tVal loss: 0.1610 (0.2481)\n",
      "Epoch [160/300]\tTrain loss: 0.1695\tVal loss: 0.1705 (0.2621)\n",
      "Epoch [161/300]\tTrain loss: 0.1756\tVal loss: 0.1806 (0.2571)\n",
      "Epoch [162/300]\tTrain loss: 0.1714\tVal loss: 0.1492 (0.1816)\n",
      "Epoch [163/300]\tTrain loss: 0.1731\tVal loss: 0.1493 (0.2235)\n",
      "Epoch [164/300]\tTrain loss: 0.1667\tVal loss: 0.2457 (0.3953)\n",
      "Epoch [165/300]\tTrain loss: 0.1691\tVal loss: 0.2627 (0.3781)\n",
      "Epoch [166/300]\tTrain loss: 0.1691\tVal loss: 0.1769 (0.2555)\n",
      "Epoch [167/300]\tTrain loss: 0.1647\tVal loss: 0.1608 (0.2310)\n",
      "Epoch [168/300]\tTrain loss: 0.1680\tVal loss: 0.1727 (0.2022)\n",
      "Epoch [169/300]\tTrain loss: 0.1619\tVal loss: 0.2082 (0.2449)\n",
      "Epoch [170/300]\tTrain loss: 0.1570\tVal loss: 0.1479 (0.2299)\n",
      "Epoch [171/300]\tTrain loss: 0.1617\tVal loss: 0.1689 (0.2641)\n",
      "Epoch [172/300]\tTrain loss: 0.1745\tVal loss: 0.1718 (0.2682)\n",
      "Epoch [173/300]\tTrain loss: 0.1591\tVal loss: 0.2452 (0.2782)\n",
      "Epoch [174/300]\tTrain loss: 0.1577\tVal loss: 0.1709 (0.2377)\n",
      "Epoch [175/300]\tTrain loss: 0.1664\tVal loss: 0.1897 (0.3099)\n",
      "Epoch [176/300]\tTrain loss: 0.1590\tVal loss: 0.1602 (0.2384)\n",
      "Epoch [177/300]\tTrain loss: 0.1596\tVal loss: 0.1690 (0.2547)\n",
      "Epoch [178/300]\tTrain loss: 0.1693\tVal loss: 0.1707 (0.2383)\n",
      "Epoch [179/300]\tTrain loss: 0.1697\tVal loss: 0.2439 (0.3282)\n",
      "Epoch [180/300]\tTrain loss: 0.1587\tVal loss: 0.1550 (0.2405)\n",
      "Epoch [181/300]\tTrain loss: 0.1641\tVal loss: 0.1698 (0.2433)\n",
      "Epoch [182/300]\tTrain loss: 0.1601\tVal loss: 0.2089 (0.3176)\n",
      "Epoch [183/300]\tTrain loss: 0.1591\tVal loss: 0.1776 (0.2850)\n",
      "Epoch [184/300]\tTrain loss: 0.1490\tVal loss: 0.2302 (0.3322)\n",
      "Epoch [185/300]\tTrain loss: 0.1620\tVal loss: 0.1670 (0.2677)\n",
      "Epoch [186/300]\tTrain loss: 0.1613\tVal loss: 0.2170 (0.3278)\n",
      "Epoch [187/300]\tTrain loss: 0.1540\tVal loss: 0.1800 (0.2690)\n",
      "Epoch [188/300]\tTrain loss: 0.1674\tVal loss: 0.1658 (0.2353)\n",
      "Epoch [189/300]\tTrain loss: 0.1659\tVal loss: 0.1649 (0.2597)\n",
      "Epoch [190/300]\tTrain loss: 0.1529\tVal loss: 0.2025 (0.3276)\n",
      "Epoch [191/300]\tTrain loss: 0.1613\tVal loss: 0.1627 (0.2630)\n",
      "Epoch [192/300]\tTrain loss: 0.1491\tVal loss: 0.1779 (0.2889)\n",
      "Epoch [193/300]\tTrain loss: 0.1496\tVal loss: 0.1762 (0.2552)\n",
      "Epoch [194/300]\tTrain loss: 0.1520\tVal loss: 0.1679 (0.2567)\n",
      "Epoch [195/300]\tTrain loss: 0.1636\tVal loss: 0.2108 (0.2544)\n",
      "Epoch [196/300]\tTrain loss: 0.1487\tVal loss: 0.1648 (0.1987)\n",
      "Epoch [197/300]\tTrain loss: 0.1581\tVal loss: 0.1526 (0.2533)\n",
      "Epoch [198/300]\tTrain loss: 0.1556\tVal loss: 0.1629 (0.2269)\n",
      "Epoch [199/300]\tTrain loss: 0.1535\tVal loss: 0.1783 (0.2716)\n",
      "Epoch [200/300]\tTrain loss: 0.1560\tVal loss: 0.1873 (0.3312)\n",
      "Epoch [201/300]\tTrain loss: 0.1668\tVal loss: 0.1851 (0.2979)\n",
      "Epoch [202/300]\tTrain loss: 0.1518\tVal loss: 0.1577 (0.2646)\n",
      "Epoch [203/300]\tTrain loss: 0.1577\tVal loss: 0.1624 (0.2194)\n",
      "Epoch [204/300]\tTrain loss: 0.1599\tVal loss: 0.1645 (0.2273)\n",
      "Epoch [205/300]\tTrain loss: 0.1508\tVal loss: 0.1611 (0.2331)\n",
      "Epoch [206/300]\tTrain loss: 0.1610\tVal loss: 0.1460 (0.2585)\n",
      "Epoch [207/300]\tTrain loss: 0.1604\tVal loss: 0.1432 (0.2186)\n",
      "Epoch [208/300]\tTrain loss: 0.1536\tVal loss: 0.1688 (0.2740)\n",
      "Epoch [209/300]\tTrain loss: 0.1441\tVal loss: 0.2440 (0.2944)\n",
      "Epoch [210/300]\tTrain loss: 0.1437\tVal loss: 0.1547 (0.2561)\n",
      "Epoch [211/300]\tTrain loss: 0.1459\tVal loss: 0.1530 (0.2628)\n",
      "Epoch [212/300]\tTrain loss: 0.1470\tVal loss: 0.1938 (0.2736)\n",
      "Epoch [213/300]\tTrain loss: 0.1566\tVal loss: 0.1699 (0.2388)\n",
      "Epoch [214/300]\tTrain loss: 0.1432\tVal loss: 0.1912 (0.3004)\n",
      "Epoch [215/300]\tTrain loss: 0.1642\tVal loss: 0.1629 (0.2208)\n",
      "Epoch [216/300]\tTrain loss: 0.1475\tVal loss: 0.1420 (0.2196)\n",
      "Epoch [217/300]\tTrain loss: 0.1374\tVal loss: 0.1646 (0.2256)\n",
      "Epoch [218/300]\tTrain loss: 0.1457\tVal loss: 0.1748 (0.2393)\n",
      "Epoch [219/300]\tTrain loss: 0.1500\tVal loss: 0.1915 (0.2927)\n",
      "Epoch [220/300]\tTrain loss: 0.1480\tVal loss: 0.1526 (0.2287)\n",
      "Epoch [221/300]\tTrain loss: 0.1424\tVal loss: 0.2356 (0.2809)\n",
      "Epoch [222/300]\tTrain loss: 0.1440\tVal loss: 0.1629 (0.2514)\n",
      "Epoch [223/300]\tTrain loss: 0.1488\tVal loss: 0.1658 (0.2400)\n",
      "Epoch [224/300]\tTrain loss: 0.1566\tVal loss: 0.1905 (0.2590)\n",
      "Epoch [225/300]\tTrain loss: 0.1572\tVal loss: 0.2068 (0.2642)\n",
      "Epoch [226/300]\tTrain loss: 0.1488\tVal loss: 0.1478 (0.1814)\n",
      "Epoch [227/300]\tTrain loss: 0.1419\tVal loss: 0.2151 (0.2997)\n",
      "Epoch [228/300]\tTrain loss: 0.1471\tVal loss: 0.1559 (0.2732)\n",
      "Epoch [229/300]\tTrain loss: 0.1445\tVal loss: 0.1593 (0.2641)\n",
      "Epoch [230/300]\tTrain loss: 0.1397\tVal loss: 0.1406 (0.2232)\n",
      "Epoch [231/300]\tTrain loss: 0.1405\tVal loss: 0.2032 (0.2665)\n",
      "Epoch [232/300]\tTrain loss: 0.1404\tVal loss: 0.1564 (0.2330)\n",
      "Epoch [233/300]\tTrain loss: 0.1431\tVal loss: 0.1576 (0.2102)\n",
      "Epoch [234/300]\tTrain loss: 0.1407\tVal loss: 0.1474 (0.1976)\n",
      "Epoch [235/300]\tTrain loss: 0.1329\tVal loss: 0.1495 (0.1717)\n",
      "Epoch [236/300]\tTrain loss: 0.1558\tVal loss: 0.1518 (0.1871)\n",
      "Epoch [237/300]\tTrain loss: 0.1328\tVal loss: 0.1866 (0.2318)\n",
      "Epoch [238/300]\tTrain loss: 0.1366\tVal loss: 0.1491 (0.2982)\n",
      "Epoch [239/300]\tTrain loss: 0.1348\tVal loss: 0.1694 (0.2361)\n",
      "Epoch [240/300]\tTrain loss: 0.1439\tVal loss: 0.1930 (0.2752)\n",
      "Epoch [241/300]\tTrain loss: 0.1504\tVal loss: 0.2100 (0.2794)\n",
      "Epoch [242/300]\tTrain loss: 0.1284\tVal loss: 0.1915 (0.2794)\n",
      "Epoch [243/300]\tTrain loss: 0.1391\tVal loss: 0.2234 (0.3103)\n",
      "Epoch [244/300]\tTrain loss: 0.1426\tVal loss: 0.1831 (0.2811)\n",
      "Epoch [245/300]\tTrain loss: 0.1411\tVal loss: 0.1746 (0.2841)\n",
      "Epoch [246/300]\tTrain loss: 0.1359\tVal loss: 0.1688 (0.2566)\n",
      "Epoch [247/300]\tTrain loss: 0.1385\tVal loss: 0.1644 (0.2566)\n",
      "Epoch [248/300]\tTrain loss: 0.1498\tVal loss: 0.1613 (0.1881)\n",
      "Epoch [249/300]\tTrain loss: 0.1384\tVal loss: 0.1483 (0.2522)\n",
      "Epoch [250/300]\tTrain loss: 0.1370\tVal loss: 0.1516 (0.2157)\n",
      "Epoch [251/300]\tTrain loss: 0.1315\tVal loss: 0.2064 (0.3106)\n",
      "Epoch [252/300]\tTrain loss: 0.1448\tVal loss: 0.1662 (0.3005)\n",
      "Epoch [253/300]\tTrain loss: 0.1402\tVal loss: 0.1539 (0.1593)\n",
      "Epoch [254/300]\tTrain loss: 0.1472\tVal loss: 0.2268 (0.3535)\n",
      "Epoch [255/300]\tTrain loss: 0.1304\tVal loss: 0.1547 (0.2225)\n",
      "Epoch [256/300]\tTrain loss: 0.1360\tVal loss: 0.1471 (0.1845)\n",
      "Epoch [257/300]\tTrain loss: 0.1379\tVal loss: 0.1533 (0.2602)\n",
      "Epoch [258/300]\tTrain loss: 0.1333\tVal loss: 0.1485 (0.1929)\n",
      "Epoch [259/300]\tTrain loss: 0.1273\tVal loss: 0.1576 (0.2442)\n",
      "Epoch [260/300]\tTrain loss: 0.1317\tVal loss: 0.2295 (0.2694)\n",
      "Epoch [261/300]\tTrain loss: 0.1395\tVal loss: 0.1710 (0.2045)\n",
      "Epoch [262/300]\tTrain loss: 0.1350\tVal loss: 0.1636 (0.2341)\n",
      "Epoch [263/300]\tTrain loss: 0.1348\tVal loss: 0.1522 (0.2034)\n",
      "Epoch [264/300]\tTrain loss: 0.1435\tVal loss: 0.1446 (0.2263)\n",
      "Epoch [265/300]\tTrain loss: 0.1339\tVal loss: 0.1465 (0.2186)\n",
      "Epoch [266/300]\tTrain loss: 0.1374\tVal loss: 0.1668 (0.2451)\n",
      "Epoch [267/300]\tTrain loss: 0.1337\tVal loss: 0.1961 (0.2809)\n",
      "Epoch [268/300]\tTrain loss: 0.1355\tVal loss: 0.1841 (0.2416)\n",
      "Epoch [269/300]\tTrain loss: 0.1417\tVal loss: 0.1498 (0.2127)\n",
      "Epoch [270/300]\tTrain loss: 0.1296\tVal loss: 0.1438 (0.2291)\n",
      "Epoch [271/300]\tTrain loss: 0.1287\tVal loss: 0.1595 (0.2644)\n",
      "Epoch [272/300]\tTrain loss: 0.1294\tVal loss: 0.1610 (0.2281)\n",
      "Epoch [273/300]\tTrain loss: 0.1256\tVal loss: 0.1494 (0.1713)\n",
      "Epoch [274/300]\tTrain loss: 0.1295\tVal loss: 0.1580 (0.2513)\n",
      "Epoch [275/300]\tTrain loss: 0.1264\tVal loss: 0.1426 (0.2096)\n",
      "Epoch [276/300]\tTrain loss: 0.1325\tVal loss: 0.1787 (0.2238)\n",
      "Epoch [277/300]\tTrain loss: 0.1442\tVal loss: 0.1595 (0.2142)\n",
      "Epoch [278/300]\tTrain loss: 0.1390\tVal loss: 0.1406 (0.1902)\n",
      "Epoch [279/300]\tTrain loss: 0.1352\tVal loss: 0.1860 (0.2274)\n",
      "Epoch [280/300]\tTrain loss: 0.1320\tVal loss: 0.1544 (0.1867)\n",
      "Epoch [281/300]\tTrain loss: 0.1292\tVal loss: 0.1844 (0.2037)\n",
      "Epoch [282/300]\tTrain loss: 0.1343\tVal loss: 0.2415 (0.3195)\n",
      "Epoch [283/300]\tTrain loss: 0.1263\tVal loss: 0.1440 (0.1986)\n",
      "Epoch [284/300]\tTrain loss: 0.1293\tVal loss: 0.1955 (0.2330)\n",
      "Epoch [285/300]\tTrain loss: 0.1225\tVal loss: 0.1448 (0.2356)\n",
      "Epoch [286/300]\tTrain loss: 0.1268\tVal loss: 0.1410 (0.2293)\n",
      "Epoch [287/300]\tTrain loss: 0.1277\tVal loss: 0.2060 (0.2624)\n",
      "Epoch [288/300]\tTrain loss: 0.1343\tVal loss: 0.1579 (0.2630)\n",
      "Epoch [289/300]\tTrain loss: 0.1274\tVal loss: 0.1657 (0.2117)\n",
      "Epoch [290/300]\tTrain loss: 0.1400\tVal loss: 0.1432 (0.2380)\n",
      "Epoch [291/300]\tTrain loss: 0.1318\tVal loss: 0.1767 (0.2498)\n",
      "Epoch [292/300]\tTrain loss: 0.1243\tVal loss: 0.1553 (0.2409)\n",
      "Epoch [293/300]\tTrain loss: 0.1350\tVal loss: 0.1757 (0.2546)\n",
      "Epoch [294/300]\tTrain loss: 0.1261\tVal loss: 0.1917 (0.2561)\n",
      "Epoch [295/300]\tTrain loss: 0.1258\tVal loss: 0.1517 (0.2091)\n",
      "Epoch [296/300]\tTrain loss: 0.1316\tVal loss: 0.1537 (0.2345)\n",
      "Epoch [297/300]\tTrain loss: 0.1233\tVal loss: 0.1417 (0.2162)\n",
      "Epoch [298/300]\tTrain loss: 0.1221\tVal loss: 0.1700 (0.2122)\n",
      "Epoch [299/300]\tTrain loss: 0.1225\tVal loss: 0.1543 (0.2344)\n",
      "Epoch [300/300]\tTrain loss: 0.1271\tVal loss: 0.1605 (0.2115)\n"
     ]
    }
   ],
   "source": [
    "train_data, val_data, test_data = cc.load_dataset(root, target_idx=2, ref_idx=1, radius=8, train_ratio=0.8, val_ratio=0.2, model_type='cgcnn')\n",
    "\n",
    "for n in range(0, 1):\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=tr.collate_cgcnn)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=tr.collate_cgcnn)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate_cgcnn)\n",
    "\n",
    "    model = CGCNN(cc.num_atom_feats, cc.num_edge_feats, 1).cuda()\n",
    "    optimizer = AdaBound(model.parameters(), lr=1e-6, weight_decay=1e-8)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_loss = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "        val_loss, val_rmse, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "        print('Epoch [{}/{}]\\tTrain loss: {:.4f}\\tVal loss: {:.4f} ({:.4f})'\n",
    "              .format(epoch + 1, num_epochs, train_loss, val_loss, val_rmse))\n",
    "\n",
    "        torch.save(model.state_dict(), 'output/cgcnn/model.pt.{:05d}'.format(epoch))\n",
    "        torch.save(model.state_dict(), 'output/cgcnn/model.pt')\n",
    "        test_loss, test_rmse, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "        numpy.savetxt('output/cgcnn/pred_results.{:05d}.csv'.format(epoch), numpy.hstack([idxs, targets, preds]), delimiter=',')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete loading 100th crystal.\n",
      "Complete loading 200th crystal.\n",
      "Complete loading 300th crystal.\n",
      "Complete loading 400th crystal.\n",
      "Complete loading 500th crystal.\n",
      "Complete loading 600th crystal.\n",
      "Complete loading 700th crystal.\n",
      "Complete loading 800th crystal.\n",
      "Complete loading 900th crystal.\n",
      "Complete loading 1000th crystal.\n",
      "Complete loading 1100th crystal.\n",
      "Complete loading 1200th crystal.\n",
      "Complete loading 1300th crystal.\n",
      "Complete loading 1400th crystal.\n",
      "Complete loading 1500th crystal.\n",
      "Complete loading 1600th crystal.\n",
      "Complete loading 1700th crystal.\n",
      "Complete loading 1800th crystal.\n",
      "Complete loading 1900th crystal.\n",
      "Complete loading 2000th crystal.\n",
      "Complete loading 2100th crystal.\n",
      "Complete loading 2200th crystal.\n",
      "Complete loading 2300th crystal.\n",
      "Complete loading 2400th crystal.\n",
      "Complete loading 2500th crystal.\n",
      "Complete loading 2600th crystal.\n",
      "Complete loading 2700th crystal.\n",
      "Complete loading 2800th crystal.\n",
      "Complete loading 2900th crystal.\n",
      "Complete loading 3000th crystal.\n",
      "Complete loading 3100th crystal.\n",
      "Complete loading 3200th crystal.\n",
      "Complete loading 3300th crystal.\n",
      "Complete loading 3400th crystal.\n",
      "Complete loading 3500th crystal.\n",
      "Complete loading 3600th crystal.\n",
      "Complete loading 3700th crystal.\n",
      "Complete loading 3800th crystal.\n",
      "Complete loading 3900th crystal.\n",
      "Complete loading 4000th crystal.\n",
      "Complete loading 4100th crystal.\n",
      "Complete loading 4200th crystal.\n",
      "Complete loading 4300th crystal.\n",
      "Complete loading 4400th crystal.\n",
      "Complete loading 4500th crystal.\n",
      "Complete loading 4600th crystal.\n",
      "Complete loading 4700th crystal.\n",
      "Complete loading 4800th crystal.\n",
      "Complete loading 4900th crystal.\n",
      "Complete loading 5000th crystal.\n",
      "Complete loading 5100th crystal.\n",
      "Complete loading 5200th crystal.\n",
      "Complete loading 5300th crystal.\n",
      "Complete loading 5400th crystal.\n",
      "Complete loading 5500th crystal.\n",
      "Complete loading 5600th crystal.\n",
      "Complete loading 5700th crystal.\n",
      "Complete loading 5800th crystal.\n",
      "Complete loading 5900th crystal.\n",
      "Complete loading 6000th crystal.\n",
      "Complete loading 6100th crystal.\n",
      "Complete loading 6200th crystal.\n",
      "Complete loading 6300th crystal.\n",
      "Complete loading 6400th crystal.\n",
      "Complete loading 6500th crystal.\n",
      "Complete loading 6600th crystal.\n",
      "Complete loading 6700th crystal.\n",
      "Complete loading 6800th crystal.\n",
      "Complete loading 6900th crystal.\n",
      "Complete loading 7000th crystal.\n",
      "Complete loading 7100th crystal.\n",
      "Complete loading 7200th crystal.\n",
      "Complete loading 7300th crystal.\n",
      "Complete loading 7400th crystal.\n",
      "Complete loading 7500th crystal.\n",
      "Complete loading 7600th crystal.\n",
      "Complete loading 7700th crystal.\n",
      "Complete loading 7800th crystal.\n",
      "Complete loading 7900th crystal.\n",
      "Complete loading 8000th crystal.\n",
      "Complete loading 8100th crystal.\n",
      "Complete loading 8200th crystal.\n",
      "Complete loading 8300th crystal.\n",
      "Complete loading 8400th crystal.\n",
      "Complete loading 8500th crystal.\n",
      "Complete loading 8600th crystal.\n",
      "Complete loading 8700th crystal.\n",
      "Complete loading 8800th crystal.\n",
      "Complete loading 8900th crystal.\n",
      "Complete loading 9000th crystal.\n",
      "Complete loading 9100th crystal.\n",
      "Complete loading 9200th crystal.\n",
      "Complete loading 9300th crystal.\n",
      "Complete loading 9400th crystal.\n",
      "Complete loading 9500th crystal.\n",
      "Complete loading 9600th crystal.\n",
      "Complete loading 9700th crystal.\n",
      "Complete loading 9800th crystal.\n",
      "Complete loading 9900th crystal.\n",
      "Complete loading 10000th crystal.\n",
      "Complete loading 10100th crystal.\n",
      "Complete loading 10200th crystal.\n",
      "Complete loading 10300th crystal.\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "cc.load_mat_atom_feats()\n",
    "list_test_mae = list()\n",
    "list_test_rmse = list()\n",
    "\n",
    "train_data, val_data, test_data = cc.load_dataset(root, target_idx=2, ref_idx=1, radius=4, train_ratio=0.8, val_ratio=0.2, model_type='tgnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\tTrain loss: 1.5356\tVal loss: 0.2776 (0.4177)\n",
      "Epoch [2/300]\tTrain loss: 0.2655\tVal loss: 0.2503 (0.3813)\n",
      "Epoch [3/300]\tTrain loss: 0.2556\tVal loss: 0.2415 (0.3188)\n",
      "Epoch [4/300]\tTrain loss: 0.2514\tVal loss: 0.2666 (0.3833)\n",
      "Epoch [5/300]\tTrain loss: 0.2517\tVal loss: 0.2464 (0.3092)\n",
      "Epoch [6/300]\tTrain loss: 0.2406\tVal loss: 0.2342 (0.3618)\n",
      "Epoch [7/300]\tTrain loss: 0.2522\tVal loss: 0.2275 (0.3171)\n",
      "Epoch [8/300]\tTrain loss: 0.2427\tVal loss: 0.2605 (0.3639)\n",
      "Epoch [9/300]\tTrain loss: 0.2409\tVal loss: 0.2397 (0.3348)\n",
      "Epoch [10/300]\tTrain loss: 0.2380\tVal loss: 0.2122 (0.3030)\n",
      "Epoch [11/300]\tTrain loss: 0.2368\tVal loss: 0.2395 (0.3021)\n",
      "Epoch [12/300]\tTrain loss: 0.2357\tVal loss: 0.2137 (0.3407)\n",
      "Epoch [13/300]\tTrain loss: 0.2368\tVal loss: 0.2611 (0.2991)\n",
      "Epoch [14/300]\tTrain loss: 0.2277\tVal loss: 0.2864 (0.3275)\n",
      "Epoch [15/300]\tTrain loss: 0.2359\tVal loss: 0.2484 (0.2989)\n",
      "Epoch [16/300]\tTrain loss: 0.2254\tVal loss: 0.2044 (0.2695)\n",
      "Epoch [17/300]\tTrain loss: 0.2348\tVal loss: 0.2034 (0.2471)\n",
      "Epoch [18/300]\tTrain loss: 0.2206\tVal loss: 0.2195 (0.3274)\n",
      "Epoch [19/300]\tTrain loss: 0.2148\tVal loss: 0.2413 (0.2732)\n",
      "Epoch [20/300]\tTrain loss: 0.2151\tVal loss: 0.1943 (0.2502)\n",
      "Epoch [21/300]\tTrain loss: 0.2229\tVal loss: 0.2226 (0.2632)\n",
      "Epoch [22/300]\tTrain loss: 0.2179\tVal loss: 0.2032 (0.3191)\n",
      "Epoch [23/300]\tTrain loss: 0.2100\tVal loss: 0.2371 (0.3235)\n",
      "Epoch [24/300]\tTrain loss: 0.2240\tVal loss: 0.2471 (0.3245)\n",
      "Epoch [25/300]\tTrain loss: 0.2162\tVal loss: 0.2140 (0.3292)\n",
      "Epoch [26/300]\tTrain loss: 0.2095\tVal loss: 0.1920 (0.2551)\n",
      "Epoch [27/300]\tTrain loss: 0.2177\tVal loss: 0.1863 (0.2349)\n",
      "Epoch [28/300]\tTrain loss: 0.2194\tVal loss: 0.1966 (0.2634)\n",
      "Epoch [29/300]\tTrain loss: 0.1971\tVal loss: 0.2305 (0.3070)\n",
      "Epoch [30/300]\tTrain loss: 0.2056\tVal loss: 0.1882 (0.2870)\n",
      "Epoch [31/300]\tTrain loss: 0.2034\tVal loss: 0.1933 (0.2369)\n",
      "Epoch [32/300]\tTrain loss: 0.1988\tVal loss: 0.2769 (0.3733)\n",
      "Epoch [33/300]\tTrain loss: 0.2012\tVal loss: 0.1897 (0.2336)\n",
      "Epoch [34/300]\tTrain loss: 0.1975\tVal loss: 0.2065 (0.2493)\n",
      "Epoch [35/300]\tTrain loss: 0.1958\tVal loss: 0.1856 (0.1927)\n",
      "Epoch [36/300]\tTrain loss: 0.1993\tVal loss: 0.2002 (0.2219)\n",
      "Epoch [37/300]\tTrain loss: 0.1956\tVal loss: 0.1833 (0.2089)\n",
      "Epoch [38/300]\tTrain loss: 0.1920\tVal loss: 0.1800 (0.2167)\n",
      "Epoch [39/300]\tTrain loss: 0.1930\tVal loss: 0.2017 (0.2273)\n",
      "Epoch [40/300]\tTrain loss: 0.1938\tVal loss: 0.1890 (0.2411)\n",
      "Epoch [41/300]\tTrain loss: 0.1922\tVal loss: 0.1992 (0.2733)\n",
      "Epoch [42/300]\tTrain loss: 0.1933\tVal loss: 0.1968 (0.2555)\n",
      "Epoch [43/300]\tTrain loss: 0.1887\tVal loss: 0.1757 (0.2145)\n",
      "Epoch [44/300]\tTrain loss: 0.1828\tVal loss: 0.2419 (0.3453)\n",
      "Epoch [45/300]\tTrain loss: 0.1878\tVal loss: 0.2056 (0.2297)\n",
      "Epoch [46/300]\tTrain loss: 0.1911\tVal loss: 0.2003 (0.2460)\n",
      "Epoch [47/300]\tTrain loss: 0.1840\tVal loss: 0.1759 (0.2119)\n",
      "Epoch [48/300]\tTrain loss: 0.1811\tVal loss: 0.1904 (0.2747)\n",
      "Epoch [49/300]\tTrain loss: 0.1913\tVal loss: 0.1849 (0.2252)\n",
      "Epoch [50/300]\tTrain loss: 0.1824\tVal loss: 0.1752 (0.2323)\n",
      "Epoch [51/300]\tTrain loss: 0.1814\tVal loss: 0.1849 (0.2238)\n",
      "Epoch [52/300]\tTrain loss: 0.1850\tVal loss: 0.1912 (0.2375)\n",
      "Epoch [53/300]\tTrain loss: 0.1855\tVal loss: 0.1736 (0.2037)\n",
      "Epoch [54/300]\tTrain loss: 0.1826\tVal loss: 0.1848 (0.1864)\n",
      "Epoch [55/300]\tTrain loss: 0.1877\tVal loss: 0.1740 (0.1933)\n",
      "Epoch [56/300]\tTrain loss: 0.1810\tVal loss: 0.1910 (0.2292)\n",
      "Epoch [57/300]\tTrain loss: 0.1773\tVal loss: 0.1773 (0.2235)\n",
      "Epoch [58/300]\tTrain loss: 0.1760\tVal loss: 0.1844 (0.2290)\n",
      "Epoch [59/300]\tTrain loss: 0.1838\tVal loss: 0.1798 (0.1954)\n",
      "Epoch [60/300]\tTrain loss: 0.1753\tVal loss: 0.1753 (0.1982)\n",
      "Epoch [61/300]\tTrain loss: 0.1725\tVal loss: 0.2189 (0.2825)\n",
      "Epoch [62/300]\tTrain loss: 0.1756\tVal loss: 0.1679 (0.2000)\n",
      "Epoch [63/300]\tTrain loss: 0.1747\tVal loss: 0.1731 (0.1978)\n",
      "Epoch [64/300]\tTrain loss: 0.1747\tVal loss: 0.1677 (0.1915)\n",
      "Epoch [65/300]\tTrain loss: 0.1732\tVal loss: 0.1671 (0.2019)\n",
      "Epoch [66/300]\tTrain loss: 0.1779\tVal loss: 0.1724 (0.2098)\n",
      "Epoch [67/300]\tTrain loss: 0.1795\tVal loss: 0.1790 (0.1887)\n",
      "Epoch [68/300]\tTrain loss: 0.1678\tVal loss: 0.1709 (0.2033)\n",
      "Epoch [69/300]\tTrain loss: 0.1677\tVal loss: 0.2025 (0.2108)\n",
      "Epoch [70/300]\tTrain loss: 0.1698\tVal loss: 0.1793 (0.1996)\n",
      "Epoch [71/300]\tTrain loss: 0.1665\tVal loss: 0.1731 (0.2048)\n",
      "Epoch [72/300]\tTrain loss: 0.1650\tVal loss: 0.1976 (0.2325)\n",
      "Epoch [73/300]\tTrain loss: 0.1698\tVal loss: 0.1672 (0.1956)\n",
      "Epoch [74/300]\tTrain loss: 0.1608\tVal loss: 0.1664 (0.1966)\n",
      "Epoch [75/300]\tTrain loss: 0.1704\tVal loss: 0.1981 (0.2250)\n",
      "Epoch [76/300]\tTrain loss: 0.1734\tVal loss: 0.1882 (0.2180)\n",
      "Epoch [77/300]\tTrain loss: 0.1648\tVal loss: 0.1729 (0.1949)\n",
      "Epoch [78/300]\tTrain loss: 0.1642\tVal loss: 0.1684 (0.2150)\n",
      "Epoch [79/300]\tTrain loss: 0.1671\tVal loss: 0.1668 (0.2114)\n",
      "Epoch [80/300]\tTrain loss: 0.1645\tVal loss: 0.1724 (0.1898)\n",
      "Epoch [81/300]\tTrain loss: 0.1660\tVal loss: 0.1717 (0.2018)\n",
      "Epoch [82/300]\tTrain loss: 0.1642\tVal loss: 0.1775 (0.2026)\n",
      "Epoch [83/300]\tTrain loss: 0.1664\tVal loss: 0.2081 (0.2358)\n",
      "Epoch [84/300]\tTrain loss: 0.1643\tVal loss: 0.1680 (0.1940)\n",
      "Epoch [85/300]\tTrain loss: 0.1569\tVal loss: 0.1740 (0.2208)\n",
      "Epoch [86/300]\tTrain loss: 0.1655\tVal loss: 0.1846 (0.2203)\n",
      "Epoch [87/300]\tTrain loss: 0.1650\tVal loss: 0.1694 (0.1940)\n",
      "Epoch [88/300]\tTrain loss: 0.1643\tVal loss: 0.1750 (0.2211)\n",
      "Epoch [89/300]\tTrain loss: 0.1624\tVal loss: 0.1658 (0.1902)\n",
      "Epoch [90/300]\tTrain loss: 0.1558\tVal loss: 0.1624 (0.2075)\n",
      "Epoch [91/300]\tTrain loss: 0.1680\tVal loss: 0.1646 (0.2003)\n",
      "Epoch [92/300]\tTrain loss: 0.1554\tVal loss: 0.1750 (0.2136)\n",
      "Epoch [93/300]\tTrain loss: 0.1613\tVal loss: 0.1679 (0.2057)\n",
      "Epoch [94/300]\tTrain loss: 0.1572\tVal loss: 0.1700 (0.1800)\n",
      "Epoch [95/300]\tTrain loss: 0.1616\tVal loss: 0.1713 (0.2088)\n",
      "Epoch [96/300]\tTrain loss: 0.1593\tVal loss: 0.1691 (0.1933)\n",
      "Epoch [97/300]\tTrain loss: 0.1572\tVal loss: 0.1736 (0.2109)\n",
      "Epoch [98/300]\tTrain loss: 0.1596\tVal loss: 0.1864 (0.2288)\n",
      "Epoch [99/300]\tTrain loss: 0.1614\tVal loss: 0.1723 (0.2098)\n",
      "Epoch [100/300]\tTrain loss: 0.1593\tVal loss: 0.1781 (0.2097)\n",
      "Epoch [101/300]\tTrain loss: 0.1539\tVal loss: 0.1595 (0.1905)\n",
      "Epoch [102/300]\tTrain loss: 0.1567\tVal loss: 0.1943 (0.2066)\n",
      "Epoch [103/300]\tTrain loss: 0.1528\tVal loss: 0.1657 (0.1772)\n",
      "Epoch [104/300]\tTrain loss: 0.1569\tVal loss: 0.1619 (0.1930)\n",
      "Epoch [105/300]\tTrain loss: 0.1567\tVal loss: 0.1622 (0.2052)\n",
      "Epoch [106/300]\tTrain loss: 0.1506\tVal loss: 0.1643 (0.2008)\n",
      "Epoch [107/300]\tTrain loss: 0.1562\tVal loss: 0.1665 (0.1989)\n",
      "Epoch [108/300]\tTrain loss: 0.1509\tVal loss: 0.1785 (0.2307)\n",
      "Epoch [109/300]\tTrain loss: 0.1607\tVal loss: 0.1642 (0.1878)\n",
      "Epoch [110/300]\tTrain loss: 0.1523\tVal loss: 0.1605 (0.1833)\n",
      "Epoch [111/300]\tTrain loss: 0.1512\tVal loss: 0.1677 (0.2220)\n",
      "Epoch [112/300]\tTrain loss: 0.1486\tVal loss: 0.1717 (0.2116)\n",
      "Epoch [113/300]\tTrain loss: 0.1522\tVal loss: 0.1708 (0.2004)\n",
      "Epoch [114/300]\tTrain loss: 0.1482\tVal loss: 0.1629 (0.1804)\n",
      "Epoch [115/300]\tTrain loss: 0.1465\tVal loss: 0.1697 (0.1867)\n",
      "Epoch [116/300]\tTrain loss: 0.1518\tVal loss: 0.1578 (0.1928)\n",
      "Epoch [117/300]\tTrain loss: 0.1482\tVal loss: 0.1701 (0.1724)\n",
      "Epoch [118/300]\tTrain loss: 0.1444\tVal loss: 0.1634 (0.1810)\n",
      "Epoch [119/300]\tTrain loss: 0.1493\tVal loss: 0.1726 (0.1820)\n",
      "Epoch [120/300]\tTrain loss: 0.1543\tVal loss: 0.1598 (0.1930)\n",
      "Epoch [121/300]\tTrain loss: 0.1457\tVal loss: 0.1725 (0.2008)\n",
      "Epoch [122/300]\tTrain loss: 0.1542\tVal loss: 0.1563 (0.1723)\n",
      "Epoch [123/300]\tTrain loss: 0.1494\tVal loss: 0.1677 (0.2118)\n",
      "Epoch [124/300]\tTrain loss: 0.1442\tVal loss: 0.1592 (0.1898)\n",
      "Epoch [125/300]\tTrain loss: 0.1453\tVal loss: 0.1579 (0.1842)\n",
      "Epoch [126/300]\tTrain loss: 0.1442\tVal loss: 0.1840 (0.1998)\n",
      "Epoch [127/300]\tTrain loss: 0.1470\tVal loss: 0.1638 (0.1888)\n",
      "Epoch [128/300]\tTrain loss: 0.1427\tVal loss: 0.1594 (0.1855)\n",
      "Epoch [129/300]\tTrain loss: 0.1434\tVal loss: 0.1610 (0.2004)\n",
      "Epoch [130/300]\tTrain loss: 0.1447\tVal loss: 0.1967 (0.2477)\n",
      "Epoch [131/300]\tTrain loss: 0.1450\tVal loss: 0.1569 (0.1604)\n",
      "Epoch [132/300]\tTrain loss: 0.1432\tVal loss: 0.1644 (0.1923)\n",
      "Epoch [133/300]\tTrain loss: 0.1470\tVal loss: 0.1855 (0.2124)\n",
      "Epoch [134/300]\tTrain loss: 0.1414\tVal loss: 0.1635 (0.1696)\n",
      "Epoch [135/300]\tTrain loss: 0.1428\tVal loss: 0.1552 (0.1737)\n",
      "Epoch [136/300]\tTrain loss: 0.1410\tVal loss: 0.1630 (0.1740)\n",
      "Epoch [137/300]\tTrain loss: 0.1437\tVal loss: 0.1840 (0.1895)\n",
      "Epoch [138/300]\tTrain loss: 0.1430\tVal loss: 0.1884 (0.1995)\n",
      "Epoch [139/300]\tTrain loss: 0.1498\tVal loss: 0.1759 (0.1955)\n",
      "Epoch [140/300]\tTrain loss: 0.1420\tVal loss: 0.1530 (0.1765)\n",
      "Epoch [141/300]\tTrain loss: 0.1488\tVal loss: 0.1622 (0.1956)\n",
      "Epoch [142/300]\tTrain loss: 0.1445\tVal loss: 0.1925 (0.2123)\n",
      "Epoch [143/300]\tTrain loss: 0.1391\tVal loss: 0.1578 (0.1752)\n",
      "Epoch [144/300]\tTrain loss: 0.1402\tVal loss: 0.1572 (0.1714)\n",
      "Epoch [145/300]\tTrain loss: 0.1436\tVal loss: 0.1811 (0.2029)\n",
      "Epoch [146/300]\tTrain loss: 0.1415\tVal loss: 0.1668 (0.1813)\n",
      "Epoch [147/300]\tTrain loss: 0.1421\tVal loss: 0.1743 (0.1934)\n",
      "Epoch [148/300]\tTrain loss: 0.1361\tVal loss: 0.1559 (0.1605)\n",
      "Epoch [149/300]\tTrain loss: 0.1345\tVal loss: 0.1613 (0.1950)\n",
      "Epoch [150/300]\tTrain loss: 0.1374\tVal loss: 0.1644 (0.1727)\n",
      "Epoch [151/300]\tTrain loss: 0.1381\tVal loss: 0.1602 (0.2144)\n",
      "Epoch [152/300]\tTrain loss: 0.1345\tVal loss: 0.1543 (0.1562)\n",
      "Epoch [153/300]\tTrain loss: 0.1359\tVal loss: 0.1543 (0.1704)\n",
      "Epoch [154/300]\tTrain loss: 0.1395\tVal loss: 0.1641 (0.2080)\n",
      "Epoch [155/300]\tTrain loss: 0.1379\tVal loss: 0.1629 (0.1841)\n",
      "Epoch [156/300]\tTrain loss: 0.1346\tVal loss: 0.1678 (0.1749)\n",
      "Epoch [157/300]\tTrain loss: 0.1327\tVal loss: 0.1590 (0.1597)\n",
      "Epoch [158/300]\tTrain loss: 0.1357\tVal loss: 0.1647 (0.1959)\n",
      "Epoch [159/300]\tTrain loss: 0.1369\tVal loss: 0.1612 (0.1697)\n",
      "Epoch [160/300]\tTrain loss: 0.1338\tVal loss: 0.1721 (0.1956)\n",
      "Epoch [161/300]\tTrain loss: 0.1444\tVal loss: 0.1689 (0.2181)\n",
      "Epoch [162/300]\tTrain loss: 0.1281\tVal loss: 0.1683 (0.1861)\n",
      "Epoch [163/300]\tTrain loss: 0.1335\tVal loss: 0.1568 (0.1557)\n",
      "Epoch [164/300]\tTrain loss: 0.1370\tVal loss: 0.1688 (0.2112)\n",
      "Epoch [165/300]\tTrain loss: 0.1368\tVal loss: 0.1647 (0.1746)\n",
      "Epoch [166/300]\tTrain loss: 0.1332\tVal loss: 0.1623 (0.1548)\n",
      "Epoch [167/300]\tTrain loss: 0.1342\tVal loss: 0.1565 (0.1760)\n",
      "Epoch [168/300]\tTrain loss: 0.1293\tVal loss: 0.1529 (0.1630)\n",
      "Epoch [169/300]\tTrain loss: 0.1324\tVal loss: 0.1618 (0.1646)\n",
      "Epoch [170/300]\tTrain loss: 0.1322\tVal loss: 0.1609 (0.1560)\n",
      "Epoch [171/300]\tTrain loss: 0.1316\tVal loss: 0.1669 (0.1654)\n",
      "Epoch [172/300]\tTrain loss: 0.1375\tVal loss: 0.1582 (0.1403)\n",
      "Epoch [173/300]\tTrain loss: 0.1304\tVal loss: 0.1625 (0.1841)\n",
      "Epoch [174/300]\tTrain loss: 0.1282\tVal loss: 0.1547 (0.1560)\n",
      "Epoch [175/300]\tTrain loss: 0.1293\tVal loss: 0.1643 (0.1801)\n",
      "Epoch [176/300]\tTrain loss: 0.1375\tVal loss: 0.1517 (0.1824)\n",
      "Epoch [177/300]\tTrain loss: 0.1266\tVal loss: 0.1541 (0.1783)\n",
      "Epoch [178/300]\tTrain loss: 0.1284\tVal loss: 0.1514 (0.1701)\n",
      "Epoch [179/300]\tTrain loss: 0.1295\tVal loss: 0.1782 (0.2128)\n",
      "Epoch [180/300]\tTrain loss: 0.1259\tVal loss: 0.1549 (0.1632)\n",
      "Epoch [181/300]\tTrain loss: 0.1297\tVal loss: 0.1581 (0.1817)\n",
      "Epoch [182/300]\tTrain loss: 0.1306\tVal loss: 0.1560 (0.1894)\n",
      "Epoch [183/300]\tTrain loss: 0.1306\tVal loss: 0.1649 (0.1777)\n",
      "Epoch [184/300]\tTrain loss: 0.1256\tVal loss: 0.1501 (0.1788)\n",
      "Epoch [185/300]\tTrain loss: 0.1265\tVal loss: 0.1483 (0.1649)\n",
      "Epoch [186/300]\tTrain loss: 0.1259\tVal loss: 0.1579 (0.1698)\n",
      "Epoch [187/300]\tTrain loss: 0.1287\tVal loss: 0.1811 (0.2211)\n",
      "Epoch [188/300]\tTrain loss: 0.1279\tVal loss: 0.1545 (0.1820)\n",
      "Epoch [189/300]\tTrain loss: 0.1335\tVal loss: 0.1601 (0.1759)\n",
      "Epoch [190/300]\tTrain loss: 0.1252\tVal loss: 0.1514 (0.1636)\n",
      "Epoch [191/300]\tTrain loss: 0.1258\tVal loss: 0.1659 (0.1744)\n",
      "Epoch [192/300]\tTrain loss: 0.1213\tVal loss: 0.1704 (0.2000)\n",
      "Epoch [193/300]\tTrain loss: 0.1269\tVal loss: 0.1582 (0.1931)\n",
      "Epoch [194/300]\tTrain loss: 0.1248\tVal loss: 0.1576 (0.1632)\n",
      "Epoch [195/300]\tTrain loss: 0.1251\tVal loss: 0.1790 (0.2034)\n",
      "Epoch [196/300]\tTrain loss: 0.1269\tVal loss: 0.1701 (0.1876)\n",
      "Epoch [197/300]\tTrain loss: 0.1201\tVal loss: 0.1552 (0.1728)\n",
      "Epoch [198/300]\tTrain loss: 0.1235\tVal loss: 0.1576 (0.1703)\n",
      "Epoch [199/300]\tTrain loss: 0.1228\tVal loss: 0.1551 (0.1653)\n",
      "Epoch [200/300]\tTrain loss: 0.1213\tVal loss: 0.1621 (0.1564)\n",
      "Epoch [201/300]\tTrain loss: 0.1209\tVal loss: 0.1699 (0.1803)\n",
      "Epoch [202/300]\tTrain loss: 0.1233\tVal loss: 0.1742 (0.2057)\n",
      "Epoch [203/300]\tTrain loss: 0.1232\tVal loss: 0.1797 (0.1621)\n",
      "Epoch [204/300]\tTrain loss: 0.1183\tVal loss: 0.1543 (0.1540)\n",
      "Epoch [205/300]\tTrain loss: 0.1249\tVal loss: 0.1538 (0.1623)\n",
      "Epoch [206/300]\tTrain loss: 0.1193\tVal loss: 0.1561 (0.1762)\n",
      "Epoch [207/300]\tTrain loss: 0.1220\tVal loss: 0.1644 (0.1931)\n",
      "Epoch [208/300]\tTrain loss: 0.1237\tVal loss: 0.1520 (0.1575)\n",
      "Epoch [209/300]\tTrain loss: 0.1207\tVal loss: 0.1502 (0.1654)\n",
      "Epoch [210/300]\tTrain loss: 0.1200\tVal loss: 0.1752 (0.1913)\n",
      "Epoch [211/300]\tTrain loss: 0.1217\tVal loss: 0.1724 (0.1657)\n",
      "Epoch [212/300]\tTrain loss: 0.1186\tVal loss: 0.1562 (0.1663)\n",
      "Epoch [213/300]\tTrain loss: 0.1265\tVal loss: 0.1691 (0.1697)\n",
      "Epoch [214/300]\tTrain loss: 0.1164\tVal loss: 0.1542 (0.1563)\n",
      "Epoch [215/300]\tTrain loss: 0.1212\tVal loss: 0.1726 (0.1637)\n",
      "Epoch [216/300]\tTrain loss: 0.1216\tVal loss: 0.1709 (0.1921)\n",
      "Epoch [217/300]\tTrain loss: 0.1153\tVal loss: 0.1521 (0.1753)\n",
      "Epoch [218/300]\tTrain loss: 0.1195\tVal loss: 0.1755 (0.2055)\n",
      "Epoch [219/300]\tTrain loss: 0.1174\tVal loss: 0.1526 (0.1649)\n",
      "Epoch [220/300]\tTrain loss: 0.1170\tVal loss: 0.1579 (0.1679)\n",
      "Epoch [221/300]\tTrain loss: 0.1196\tVal loss: 0.1778 (0.2084)\n",
      "Epoch [222/300]\tTrain loss: 0.1186\tVal loss: 0.1542 (0.1777)\n",
      "Epoch [223/300]\tTrain loss: 0.1163\tVal loss: 0.1536 (0.1524)\n",
      "Epoch [224/300]\tTrain loss: 0.1235\tVal loss: 0.1504 (0.1661)\n",
      "Epoch [225/300]\tTrain loss: 0.1148\tVal loss: 0.1527 (0.1697)\n",
      "Epoch [226/300]\tTrain loss: 0.1131\tVal loss: 0.1521 (0.1628)\n",
      "Epoch [227/300]\tTrain loss: 0.1166\tVal loss: 0.1587 (0.1697)\n",
      "Epoch [228/300]\tTrain loss: 0.1258\tVal loss: 0.1532 (0.1573)\n",
      "Epoch [229/300]\tTrain loss: 0.1165\tVal loss: 0.1555 (0.1538)\n",
      "Epoch [230/300]\tTrain loss: 0.1193\tVal loss: 0.1649 (0.1628)\n",
      "Epoch [231/300]\tTrain loss: 0.1217\tVal loss: 0.1539 (0.1734)\n",
      "Epoch [232/300]\tTrain loss: 0.1137\tVal loss: 0.1516 (0.1723)\n",
      "Epoch [233/300]\tTrain loss: 0.1155\tVal loss: 0.1799 (0.2223)\n",
      "Epoch [234/300]\tTrain loss: 0.1116\tVal loss: 0.1506 (0.1555)\n",
      "Epoch [235/300]\tTrain loss: 0.1117\tVal loss: 0.1622 (0.2075)\n",
      "Epoch [236/300]\tTrain loss: 0.1173\tVal loss: 0.1777 (0.2164)\n",
      "Epoch [237/300]\tTrain loss: 0.1147\tVal loss: 0.1590 (0.1741)\n",
      "Epoch [238/300]\tTrain loss: 0.1110\tVal loss: 0.1491 (0.1443)\n",
      "Epoch [239/300]\tTrain loss: 0.1116\tVal loss: 0.1523 (0.1844)\n",
      "Epoch [240/300]\tTrain loss: 0.1118\tVal loss: 0.1619 (0.1799)\n",
      "Epoch [241/300]\tTrain loss: 0.1119\tVal loss: 0.1531 (0.1908)\n",
      "Epoch [242/300]\tTrain loss: 0.1092\tVal loss: 0.1518 (0.1596)\n",
      "Epoch [243/300]\tTrain loss: 0.1110\tVal loss: 0.1729 (0.2044)\n",
      "Epoch [244/300]\tTrain loss: 0.1117\tVal loss: 0.1718 (0.1620)\n",
      "Epoch [245/300]\tTrain loss: 0.1078\tVal loss: 0.1543 (0.1599)\n",
      "Epoch [246/300]\tTrain loss: 0.1122\tVal loss: 0.1482 (0.1655)\n",
      "Epoch [247/300]\tTrain loss: 0.1156\tVal loss: 0.1636 (0.1615)\n",
      "Epoch [248/300]\tTrain loss: 0.1099\tVal loss: 0.1532 (0.1393)\n",
      "Epoch [249/300]\tTrain loss: 0.1110\tVal loss: 0.1710 (0.1855)\n",
      "Epoch [250/300]\tTrain loss: 0.1150\tVal loss: 0.1526 (0.1712)\n",
      "Epoch [251/300]\tTrain loss: 0.1124\tVal loss: 0.1477 (0.1675)\n",
      "Epoch [252/300]\tTrain loss: 0.1083\tVal loss: 0.1481 (0.1516)\n",
      "Epoch [253/300]\tTrain loss: 0.1094\tVal loss: 0.1551 (0.1839)\n",
      "Epoch [254/300]\tTrain loss: 0.1166\tVal loss: 0.1569 (0.1774)\n",
      "Epoch [255/300]\tTrain loss: 0.1089\tVal loss: 0.1472 (0.1758)\n",
      "Epoch [256/300]\tTrain loss: 0.1094\tVal loss: 0.1786 (0.1913)\n",
      "Epoch [257/300]\tTrain loss: 0.1055\tVal loss: 0.1575 (0.1821)\n",
      "Epoch [258/300]\tTrain loss: 0.1060\tVal loss: 0.1435 (0.1539)\n",
      "Epoch [259/300]\tTrain loss: 0.1064\tVal loss: 0.1508 (0.1685)\n",
      "Epoch [260/300]\tTrain loss: 0.1089\tVal loss: 0.1590 (0.1690)\n",
      "Epoch [261/300]\tTrain loss: 0.1074\tVal loss: 0.1570 (0.1882)\n",
      "Epoch [262/300]\tTrain loss: 0.1056\tVal loss: 0.1470 (0.1744)\n",
      "Epoch [263/300]\tTrain loss: 0.1058\tVal loss: 0.1494 (0.1421)\n",
      "Epoch [264/300]\tTrain loss: 0.1036\tVal loss: 0.1629 (0.1542)\n",
      "Epoch [265/300]\tTrain loss: 0.1056\tVal loss: 0.1579 (0.1693)\n",
      "Epoch [266/300]\tTrain loss: 0.1066\tVal loss: 0.1517 (0.1782)\n",
      "Epoch [267/300]\tTrain loss: 0.1126\tVal loss: 0.1906 (0.2061)\n",
      "Epoch [268/300]\tTrain loss: 0.1094\tVal loss: 0.1486 (0.1413)\n",
      "Epoch [269/300]\tTrain loss: 0.1069\tVal loss: 0.1513 (0.1718)\n",
      "Epoch [270/300]\tTrain loss: 0.0995\tVal loss: 0.1531 (0.1569)\n",
      "Epoch [271/300]\tTrain loss: 0.1060\tVal loss: 0.1657 (0.1954)\n",
      "Epoch [272/300]\tTrain loss: 0.1078\tVal loss: 0.1646 (0.1968)\n",
      "Epoch [273/300]\tTrain loss: 0.1041\tVal loss: 0.1470 (0.1603)\n",
      "Epoch [274/300]\tTrain loss: 0.1053\tVal loss: 0.1746 (0.2089)\n",
      "Epoch [275/300]\tTrain loss: 0.1089\tVal loss: 0.1590 (0.1689)\n",
      "Epoch [276/300]\tTrain loss: 0.1079\tVal loss: 0.1509 (0.1698)\n",
      "Epoch [277/300]\tTrain loss: 0.1014\tVal loss: 0.1462 (0.1512)\n",
      "Epoch [278/300]\tTrain loss: 0.1039\tVal loss: 0.1516 (0.1537)\n",
      "Epoch [279/300]\tTrain loss: 0.1117\tVal loss: 0.1529 (0.1554)\n",
      "Epoch [280/300]\tTrain loss: 0.1117\tVal loss: 0.1565 (0.1636)\n",
      "Epoch [281/300]\tTrain loss: 0.1053\tVal loss: 0.1546 (0.1458)\n",
      "Epoch [282/300]\tTrain loss: 0.1043\tVal loss: 0.1522 (0.1647)\n",
      "Epoch [283/300]\tTrain loss: 0.1036\tVal loss: 0.1778 (0.1963)\n",
      "Epoch [284/300]\tTrain loss: 0.1017\tVal loss: 0.1536 (0.1869)\n",
      "Epoch [285/300]\tTrain loss: 0.0985\tVal loss: 0.1583 (0.1636)\n",
      "Epoch [286/300]\tTrain loss: 0.1039\tVal loss: 0.1710 (0.2256)\n",
      "Epoch [287/300]\tTrain loss: 0.1040\tVal loss: 0.1488 (0.1721)\n",
      "Epoch [288/300]\tTrain loss: 0.1015\tVal loss: 0.1470 (0.1605)\n",
      "Epoch [289/300]\tTrain loss: 0.0975\tVal loss: 0.1522 (0.1654)\n",
      "Epoch [290/300]\tTrain loss: 0.1030\tVal loss: 0.1504 (0.1492)\n",
      "Epoch [291/300]\tTrain loss: 0.1018\tVal loss: 0.1461 (0.1701)\n",
      "Epoch [292/300]\tTrain loss: 0.0996\tVal loss: 0.1660 (0.1740)\n",
      "Epoch [293/300]\tTrain loss: 0.0999\tVal loss: 0.1574 (0.1933)\n",
      "Epoch [294/300]\tTrain loss: 0.1043\tVal loss: 0.1461 (0.1791)\n",
      "Epoch [295/300]\tTrain loss: 0.0996\tVal loss: 0.1456 (0.1625)\n",
      "Epoch [296/300]\tTrain loss: 0.0974\tVal loss: 0.1465 (0.1649)\n",
      "Epoch [297/300]\tTrain loss: 0.0969\tVal loss: 0.1482 (0.1503)\n",
      "Epoch [298/300]\tTrain loss: 0.1010\tVal loss: 0.1487 (0.1618)\n",
      "Epoch [299/300]\tTrain loss: 0.1026\tVal loss: 0.1540 (0.1604)\n",
      "Epoch [300/300]\tTrain loss: 0.1015\tVal loss: 0.1479 (0.1455)\n"
     ]
    }
   ],
   "source": [
    "for n in range(0, 1):\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=tr.collate)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=tr.collate)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate)\n",
    "\n",
    "    model = TGNN(cc.num_atom_feats, cc.num_bond_feats, 1).cuda()\n",
    "    optimizer = AdaBound(model.parameters(), lr=1e-6, weight_decay=1e-8)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_loss = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "        val_loss, val_rmse, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "        print('Epoch [{}/{}]\\tTrain loss: {:.4f}\\tVal loss: {:.4f} ({:.4f})'\n",
    "              .format(epoch + 1, num_epochs, train_loss, val_loss, val_rmse))\n",
    "\n",
    "        torch.save(model.state_dict(), 'output/tgnn/model.pt.{:05d}'.format(epoch))\n",
    "        torch.save(model.state_dict(), 'output/tgnn/model.pt')\n",
    "#        test_loss, test_rmse, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "#        numpy.savetxt('output/tgnn/pred_results.{:05d}.csv'.format(epoch), numpy.hstack([idxs, targets, preds]), delimiter=',')\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8d7440ee69033cedfb03ec9102fb4d61ffc9ff7ad7c8cf3ed72a45b7e84428f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('test01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
