{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, gc, json\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from util.input_data import Dataset\n",
    "from util.AdaBound import AdaBound\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def exec_model(\n",
    "    scale,\n",
    "    model_type,\n",
    "    comment='',\n",
    "    lr = 1e-5,\n",
    "    wd = 1e-7,\n",
    "    tries = 1,\n",
    "    root_model = 'd:/MODELS/202204/nmm',\n",
    "    root_data  = 'c:/WORKSPACE_KRICT/DATA/data_snu',\n",
    "    num_epochs = 300,\n",
    "    batch_size = 128,\n",
    "    train_ratio = 0.7,\n",
    "    valid_ratio = 0.2,\n",
    "):\n",
    "    gc.collect()\n",
    "\n",
    "    dataset = Dataset()\n",
    "    dataset.load_dataset(os.path.join(root_data, f'inputdata_{scale}.pickle'), silent=True)\n",
    "\n",
    "    for n in range(0, tries):\n",
    "        rseed  = 35 + n\n",
    "        train_data, valid_data, test_data = dataset.train_test_split(train_ratio=train_ratio, \n",
    "                                                                     valid_ratio=valid_ratio,\n",
    "                                                                     rseed=rseed)\n",
    "        train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                                    collate_fn=tr.collate_fn)\n",
    "        val_data_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "        test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "\n",
    "        model = DistNN(dataset.n_atom_feats, dataset.n_rdf_feature, dataset.n_bdf_feature).cuda()\n",
    "        optimizer = AdaBound(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = torch.nn.L1Loss()\n",
    "\n",
    "        for i in range(99):\n",
    "            root = os.path.join(root_model, model_type)\n",
    "            if not os.path.isdir(root):\n",
    "                os.makedirs(root)\n",
    "            if '{}_{:02d}'.format(scale, i) not in ' '.join(os.listdir(root)):\n",
    "                output_root = os.path.join(root, '{}_{:02d}'.format(scale, i))\n",
    "                if len(comment) > 0: output_root += f'_{comment}'\n",
    "                os.makedirs(output_root)\n",
    "                break\n",
    "        print(output_root)\n",
    "        with open(os.path.join(output_root, 'params.json'),'w') as f:\n",
    "            json.dump(dict(random_seed=rseed, learning_rate=lr, weight_decay=wd, \n",
    "                train_ratio=train_ratio, valid_ratio=valid_ratio, batch_size=batch_size), \n",
    "                f, indent=4)\n",
    "        writer = SummaryWriter(output_root)\n",
    "        #with torch.no_grad():\n",
    "        #    dummy = iter(test_data_loader).next()\n",
    "        #    writer.add_graph(model, dummy[:7])\n",
    "\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            train_loss, train_mae = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "            valid_loss, valid_mae, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "            if epoch > 20 and train_loss > 1: \n",
    "                break\n",
    "            print('Epoch [{}/{}]\\tTrain/Valid Loss: {:.4f} / {:.4f}\\tMAE: {:.4f} / {:.4f}'\n",
    "                    .format(epoch, num_epochs, train_loss, valid_loss, train_mae, valid_mae))\n",
    "\n",
    "            writer.add_scalar('train/loss', train_loss, epoch)\n",
    "            writer.add_scalar('train/MAE', train_mae, epoch)\n",
    "#            writer.add_scalar('train/F1', train_f1, epoch)\n",
    "            writer.add_scalar('valid/loss', valid_loss, epoch)\n",
    "            writer.add_scalar('valid/MAE', valid_mae, epoch)\n",
    "#            writer.add_scalar('valid/F1', valid_f1, epoch)\n",
    "\n",
    "            if epoch%20 == 0:\n",
    "                torch.save(model.state_dict(), \n",
    "                           os.path.join(output_root, 'model.{:05d}.pt'.format(epoch)))\n",
    "                _, _, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "                np.savetxt(os.path.join(output_root, 'pred.{:05d}.txt'.format(epoch)), \n",
    "                           np.hstack([idxs, targets, preds]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:/MODELS/202204/nmm\\M02R\\metal_FFF_01_L1_logL1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\WORKSPACE_KRICT\\CODES\\band_gap_model\\new_model.metal\\util\\AdaBound.py:91: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\tTrain/Valid Loss: 4.3221 / 3.2318\tMAE: 1.1447 / 0.8762\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.9084 / 2.6019\tMAE: 0.8394 / 0.7657\n",
      "Epoch [3/300]\tTrain/Valid Loss: 2.5202 / 3.2411\tMAE: 0.7573 / 0.9393\n",
      "Epoch [4/300]\tTrain/Valid Loss: 2.4580 / 2.3678\tMAE: 0.7176 / 0.8017\n",
      "Epoch [5/300]\tTrain/Valid Loss: 2.1267 / 1.9340\tMAE: 0.5674 / 0.3816\n",
      "Epoch [6/300]\tTrain/Valid Loss: 1.6722 / 1.5347\tMAE: 0.3072 / 0.2499\n",
      "Epoch [7/300]\tTrain/Valid Loss: 1.5612 / 2.5685\tMAE: 0.2470 / 0.3586\n",
      "Epoch [8/300]\tTrain/Valid Loss: 1.4781 / 1.4239\tMAE: 0.2285 / 0.2078\n",
      "Epoch [9/300]\tTrain/Valid Loss: 1.4108 / 1.3461\tMAE: 0.2106 / 0.1950\n",
      "Epoch [10/300]\tTrain/Valid Loss: 1.3079 / 1.4887\tMAE: 0.1949 / 0.1836\n",
      "Epoch [11/300]\tTrain/Valid Loss: 1.2645 / 1.1705\tMAE: 0.1846 / 0.1782\n",
      "Epoch [12/300]\tTrain/Valid Loss: 1.1752 / 1.2678\tMAE: 0.1795 / 0.2358\n",
      "Epoch [13/300]\tTrain/Valid Loss: 1.1282 / 0.9978\tMAE: 0.1742 / 0.1641\n",
      "Epoch [14/300]\tTrain/Valid Loss: 1.0796 / 1.1331\tMAE: 0.1678 / 0.1901\n",
      "Epoch [15/300]\tTrain/Valid Loss: 1.0083 / 0.8790\tMAE: 0.1637 / 0.1600\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.8954 / 0.9444\tMAE: 0.1549 / 0.1523\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.8009 / 0.7587\tMAE: 0.1505 / 0.1540\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.7587 / 0.7481\tMAE: 0.1485 / 0.1467\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.7347 / 0.6838\tMAE: 0.1446 / 0.1501\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.7325 / 0.7211\tMAE: 0.1438 / 0.1431\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.6384 / 0.6327\tMAE: 0.1437 / 0.1446\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.5716 / 0.5810\tMAE: 0.1365 / 0.1400\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.5628 / 0.5433\tMAE: 0.1351 / 0.1315\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.5297 / 0.4841\tMAE: 0.1353 / 0.1341\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.5079 / 0.4842\tMAE: 0.1300 / 0.1333\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.4933 / 0.4865\tMAE: 0.1337 / 0.1458\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.4641 / 0.4370\tMAE: 0.1285 / 0.1270\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.4445 / 0.4556\tMAE: 0.1268 / 0.1376\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.4490 / 0.4502\tMAE: 0.1263 / 0.1263\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.4315 / 0.4330\tMAE: 0.1244 / 0.1329\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.4177 / 0.4400\tMAE: 0.1257 / 0.1477\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.4120 / 0.4323\tMAE: 0.1218 / 0.1257\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.4172 / 0.4330\tMAE: 0.1221 / 0.1289\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.3973 / 0.4229\tMAE: 0.1211 / 0.1230\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.4264 / 0.4296\tMAE: 0.1237 / 0.1237\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.4106 / 0.4324\tMAE: 0.1230 / 0.1252\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.4071 / 0.4205\tMAE: 0.1219 / 0.1386\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.3908 / 0.3904\tMAE: 0.1217 / 0.1210\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.3990 / 0.3731\tMAE: 0.1200 / 0.1184\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.3819 / 0.3859\tMAE: 0.1171 / 0.1218\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.3824 / 0.4031\tMAE: 0.1165 / 0.1249\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.3701 / 0.4174\tMAE: 0.1152 / 0.1456\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.3667 / 0.4265\tMAE: 0.1176 / 0.1240\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.3917 / 0.3961\tMAE: 0.1165 / 0.1256\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.3635 / 0.3713\tMAE: 0.1155 / 0.1309\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.3792 / 0.4169\tMAE: 0.1158 / 0.1231\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.3916 / 0.3741\tMAE: 0.1156 / 0.1227\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.4071 / 0.3712\tMAE: 0.1200 / 0.1185\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.3751 / 0.3480\tMAE: 0.1121 / 0.1122\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.3515 / 0.3962\tMAE: 0.1130 / 0.1274\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.3750 / 0.3944\tMAE: 0.1133 / 0.1287\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.3511 / 0.4527\tMAE: 0.1110 / 0.1772\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.3488 / 0.3746\tMAE: 0.1111 / 0.1209\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.3474 / 0.4042\tMAE: 0.1090 / 0.1209\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.3590 / 0.4160\tMAE: 0.1137 / 0.1509\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.3608 / 0.4530\tMAE: 0.1172 / 0.1611\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.3478 / 0.3492\tMAE: 0.1122 / 0.1132\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.3475 / 0.4192\tMAE: 0.1094 / 0.1521\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.3478 / 0.3984\tMAE: 0.1099 / 0.1134\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.3512 / 0.3830\tMAE: 0.1083 / 0.1120\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.3541 / 0.3712\tMAE: 0.1080 / 0.1184\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.3424 / 0.3699\tMAE: 0.1109 / 0.1309\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.3373 / 0.3465\tMAE: 0.1075 / 0.1081\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.3341 / 0.3373\tMAE: 0.1069 / 0.1137\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.3282 / 0.3515\tMAE: 0.1065 / 0.1165\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.3347 / 0.3952\tMAE: 0.1068 / 0.1212\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.3250 / 0.3614\tMAE: 0.1031 / 0.1207\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.3437 / 0.4492\tMAE: 0.1076 / 0.1461\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.3418 / 0.4073\tMAE: 0.1085 / 0.1212\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.3344 / 0.3980\tMAE: 0.1052 / 0.1345\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.3282 / 0.3462\tMAE: 0.1040 / 0.1171\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.3277 / 0.4363\tMAE: 0.1038 / 0.1274\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.3262 / 0.3787\tMAE: 0.1007 / 0.1387\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.3340 / 0.4055\tMAE: 0.1019 / 0.1160\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.3138 / 0.3320\tMAE: 0.1009 / 0.1079\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.3325 / 0.3669\tMAE: 0.1001 / 0.1179\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.3093 / 0.4114\tMAE: 0.1027 / 0.1160\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.3221 / 0.3765\tMAE: 0.1026 / 0.1124\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.3368 / 0.4692\tMAE: 0.1081 / 0.1295\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.3336 / 0.4149\tMAE: 0.1047 / 0.1248\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.3213 / 0.4039\tMAE: 0.1045 / 0.1241\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.3502 / 0.4025\tMAE: 0.1056 / 0.1271\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.3190 / 0.3343\tMAE: 0.1005 / 0.1110\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.3284 / 0.3917\tMAE: 0.1008 / 0.1184\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.3308 / 0.3923\tMAE: 0.1073 / 0.1080\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.3081 / 0.3902\tMAE: 0.0972 / 0.1108\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.3237 / 0.3799\tMAE: 0.0976 / 0.1174\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.3037 / 0.3630\tMAE: 0.1019 / 0.1201\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.2992 / 0.3685\tMAE: 0.0976 / 0.1168\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.2926 / 0.4455\tMAE: 0.0949 / 0.1395\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.3092 / 0.3370\tMAE: 0.1031 / 0.1092\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.3219 / 0.4175\tMAE: 0.1008 / 0.1147\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.2909 / 0.3645\tMAE: 0.0973 / 0.1114\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.2853 / 0.3458\tMAE: 0.0941 / 0.1141\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.3023 / 0.3881\tMAE: 0.0984 / 0.1271\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.3231 / 0.3470\tMAE: 0.1012 / 0.1072\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.3085 / 0.3953\tMAE: 0.0980 / 0.1077\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.2802 / 0.3462\tMAE: 0.0965 / 0.1135\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.2801 / 0.4450\tMAE: 0.0926 / 0.1181\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.2949 / 0.3300\tMAE: 0.0972 / 0.1134\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.3098 / 0.3255\tMAE: 0.0967 / 0.1082\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.3259 / 0.3809\tMAE: 0.0981 / 0.1241\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.2822 / 0.3978\tMAE: 0.0946 / 0.1075\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.2901 / 0.4677\tMAE: 0.0936 / 0.1124\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.2732 / 0.3388\tMAE: 0.0945 / 0.1066\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.2707 / 0.3629\tMAE: 0.0918 / 0.1093\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.2805 / 0.3253\tMAE: 0.0918 / 0.1112\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.2595 / 0.3931\tMAE: 0.0896 / 0.1223\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.2941 / 0.3775\tMAE: 0.0922 / 0.1195\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.2856 / 0.3630\tMAE: 0.0901 / 0.1136\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.2911 / 0.3338\tMAE: 0.0950 / 0.1090\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.2679 / 0.3311\tMAE: 0.0896 / 0.1142\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.2784 / 0.3395\tMAE: 0.0913 / 0.1126\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.2782 / 0.4245\tMAE: 0.0919 / 0.1093\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.2848 / 0.4504\tMAE: 0.0884 / 0.1061\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.2793 / 0.3784\tMAE: 0.0900 / 0.1382\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.2662 / 0.3308\tMAE: 0.0904 / 0.1053\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2519 / 0.3519\tMAE: 0.0853 / 0.1049\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.2619 / 0.3741\tMAE: 0.0844 / 0.1045\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.2959 / 0.3645\tMAE: 0.0925 / 0.1102\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.2541 / 0.3126\tMAE: 0.0884 / 0.1062\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.2525 / 0.3142\tMAE: 0.0841 / 0.1046\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.2612 / 0.3384\tMAE: 0.0858 / 0.1281\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2917 / 0.3730\tMAE: 0.0899 / 0.1100\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.2763 / 0.3711\tMAE: 0.0855 / 0.1091\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.2601 / 0.3642\tMAE: 0.0859 / 0.1062\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.2525 / 0.3498\tMAE: 0.0852 / 0.1122\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.2776 / 0.3658\tMAE: 0.0877 / 0.1029\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.2580 / 0.3400\tMAE: 0.0878 / 0.1158\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.2396 / 0.4126\tMAE: 0.0817 / 0.1089\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.2761 / 0.3370\tMAE: 0.0840 / 0.1129\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2439 / 0.3745\tMAE: 0.0813 / 0.1047\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.2635 / 0.3433\tMAE: 0.0854 / 0.1086\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.2569 / 0.3494\tMAE: 0.0845 / 0.1062\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.2453 / 0.4109\tMAE: 0.0819 / 0.1437\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.2600 / 0.3635\tMAE: 0.0866 / 0.1129\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.2530 / 0.3827\tMAE: 0.0838 / 0.1161\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.2643 / 0.4395\tMAE: 0.0871 / 0.1161\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.2796 / 0.3746\tMAE: 0.0902 / 0.1087\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.2522 / 0.4065\tMAE: 0.0819 / 0.1367\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.2576 / 0.3773\tMAE: 0.0860 / 0.1128\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.2702 / 0.3581\tMAE: 0.0881 / 0.1182\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.2515 / 0.4034\tMAE: 0.0808 / 0.1106\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.2615 / 0.3051\tMAE: 0.0835 / 0.1074\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.2581 / 0.3155\tMAE: 0.0847 / 0.1126\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.2655 / 0.3629\tMAE: 0.0829 / 0.1071\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.2556 / 0.4184\tMAE: 0.0804 / 0.1015\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.2636 / 0.3291\tMAE: 0.0826 / 0.1066\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.2305 / 0.3502\tMAE: 0.0808 / 0.1119\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.2875 / 0.5165\tMAE: 0.0814 / 0.1115\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.2583 / 0.3275\tMAE: 0.0758 / 0.1126\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.2274 / 0.4337\tMAE: 0.0762 / 0.1073\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.2419 / 0.3982\tMAE: 0.0787 / 0.1088\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.2369 / 0.3795\tMAE: 0.0810 / 0.1071\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.2345 / 0.3590\tMAE: 0.0762 / 0.1194\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.2142 / 0.3806\tMAE: 0.0757 / 0.1110\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.2165 / 0.3435\tMAE: 0.0759 / 0.1067\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.2118 / 0.3223\tMAE: 0.0749 / 0.1011\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.2202 / 0.3449\tMAE: 0.0780 / 0.1053\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.2372 / 0.4199\tMAE: 0.0726 / 0.1359\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.2368 / 0.3324\tMAE: 0.0811 / 0.1034\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.2262 / 0.3657\tMAE: 0.0734 / 0.1122\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.2542 / 0.3973\tMAE: 0.0758 / 0.1057\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.2593 / 0.3529\tMAE: 0.0739 / 0.1039\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.2420 / 0.3979\tMAE: 0.0787 / 0.1019\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.2154 / 0.3308\tMAE: 0.0730 / 0.1033\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.2169 / 0.4127\tMAE: 0.0739 / 0.1077\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.2476 / 0.3381\tMAE: 0.0746 / 0.1076\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.2227 / 0.4168\tMAE: 0.0751 / 0.1240\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.2236 / 0.3816\tMAE: 0.0760 / 0.1191\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.2017 / 0.3531\tMAE: 0.0726 / 0.1023\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.2051 / 0.3961\tMAE: 0.0743 / 0.1162\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.2178 / 0.3839\tMAE: 0.0723 / 0.1154\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.2010 / 0.3630\tMAE: 0.0694 / 0.1038\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.2049 / 0.3490\tMAE: 0.0688 / 0.1114\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.2277 / 0.3228\tMAE: 0.0740 / 0.1021\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.2300 / 0.3474\tMAE: 0.0695 / 0.1039\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.1964 / 0.3958\tMAE: 0.0696 / 0.1081\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.2547 / 0.4706\tMAE: 0.0723 / 0.1346\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.2086 / 0.4031\tMAE: 0.0740 / 0.1074\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.2195 / 0.3828\tMAE: 0.0707 / 0.1053\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.1966 / 0.3912\tMAE: 0.0678 / 0.1013\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.2409 / 0.3642\tMAE: 0.0699 / 0.1066\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.2058 / 0.4044\tMAE: 0.0716 / 0.1063\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.2469 / 0.3411\tMAE: 0.0710 / 0.1054\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.1961 / 0.4124\tMAE: 0.0678 / 0.1251\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.2323 / 0.5156\tMAE: 0.0727 / 0.1002\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.2531 / 0.4166\tMAE: 0.0691 / 0.1029\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.2327 / 0.3685\tMAE: 0.0694 / 0.1024\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.1867 / 0.3494\tMAE: 0.0662 / 0.1041\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.1856 / 0.3799\tMAE: 0.0647 / 0.1001\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.2047 / 0.3835\tMAE: 0.0746 / 0.1023\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.2019 / 0.4175\tMAE: 0.0692 / 0.1078\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.2231 / 0.3763\tMAE: 0.0661 / 0.1089\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.1989 / 0.3618\tMAE: 0.0668 / 0.1142\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.1999 / 0.3717\tMAE: 0.0663 / 0.0971\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.2004 / 0.4035\tMAE: 0.0634 / 0.1487\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.1745 / 0.3339\tMAE: 0.0617 / 0.1023\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.1975 / 0.3674\tMAE: 0.0668 / 0.1069\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.2366 / 0.3743\tMAE: 0.0639 / 0.1016\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.2274 / 0.3291\tMAE: 0.0686 / 0.1035\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.1891 / 0.3841\tMAE: 0.0667 / 0.1219\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.2012 / 0.4261\tMAE: 0.0659 / 0.1082\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.2047 / 0.4219\tMAE: 0.0694 / 0.1073\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.2191 / 0.4315\tMAE: 0.0629 / 0.1040\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.2113 / 0.3996\tMAE: 0.0635 / 0.1061\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.2038 / 0.4076\tMAE: 0.0684 / 0.1335\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.1854 / 0.3434\tMAE: 0.0666 / 0.1025\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.1759 / 0.3809\tMAE: 0.0635 / 0.1168\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.1804 / 0.3455\tMAE: 0.0617 / 0.1045\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.1923 / 0.4135\tMAE: 0.0624 / 0.1129\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.1905 / 0.3758\tMAE: 0.0659 / 0.1265\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.1698 / 0.3928\tMAE: 0.0614 / 0.1043\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.1680 / 0.3300\tMAE: 0.0601 / 0.1032\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.1732 / 0.4134\tMAE: 0.0602 / 0.1014\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1932 / 0.3961\tMAE: 0.0620 / 0.1059\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.2104 / 0.4366\tMAE: 0.0716 / 0.1137\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.1886 / 0.3682\tMAE: 0.0613 / 0.1064\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.1895 / 0.3718\tMAE: 0.0667 / 0.1012\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.1847 / 0.3843\tMAE: 0.0584 / 0.1031\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.1937 / 0.4019\tMAE: 0.0619 / 0.0999\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.1952 / 0.3473\tMAE: 0.0669 / 0.1064\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.1840 / 0.4155\tMAE: 0.0602 / 0.1270\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.2025 / 0.4351\tMAE: 0.0614 / 0.1061\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.2083 / 0.4089\tMAE: 0.0615 / 0.1023\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.2059 / 0.3870\tMAE: 0.0601 / 0.1055\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.2213 / 0.3457\tMAE: 0.0652 / 0.1013\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.1945 / 0.4177\tMAE: 0.0643 / 0.1267\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.1762 / 0.3283\tMAE: 0.0630 / 0.1042\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.1512 / 0.3492\tMAE: 0.0559 / 0.1109\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.1863 / 0.4251\tMAE: 0.0684 / 0.1261\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.2146 / 0.3960\tMAE: 0.0587 / 0.1031\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.1744 / 0.3693\tMAE: 0.0577 / 0.1026\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.1860 / 0.3664\tMAE: 0.0617 / 0.1106\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.1645 / 0.3291\tMAE: 0.0550 / 0.1019\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.1979 / 0.3319\tMAE: 0.0585 / 0.1007\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.1790 / 0.4127\tMAE: 0.0576 / 0.1036\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.1771 / 0.3279\tMAE: 0.0609 / 0.1027\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.1588 / 0.3267\tMAE: 0.0551 / 0.0982\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.1712 / 0.3401\tMAE: 0.0565 / 0.0991\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1745 / 0.3428\tMAE: 0.0579 / 0.1066\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.1656 / 0.3279\tMAE: 0.0592 / 0.0996\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.1510 / 0.3672\tMAE: 0.0531 / 0.1030\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.1479 / 0.3277\tMAE: 0.0513 / 0.1062\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.1722 / 0.3344\tMAE: 0.0583 / 0.1058\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.1600 / 0.3447\tMAE: 0.0578 / 0.1104\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.1688 / 0.4037\tMAE: 0.0571 / 0.1039\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.1550 / 0.4610\tMAE: 0.0538 / 0.1271\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.1687 / 0.3312\tMAE: 0.0566 / 0.1034\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.1558 / 0.3250\tMAE: 0.0541 / 0.1023\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.1463 / 0.3152\tMAE: 0.0512 / 0.0987\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.1459 / 0.3568\tMAE: 0.0510 / 0.1050\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.1673 / 0.3374\tMAE: 0.0520 / 0.0995\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.1595 / 0.3456\tMAE: 0.0503 / 0.1016\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.1796 / 0.3220\tMAE: 0.0512 / 0.0982\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.1584 / 0.3241\tMAE: 0.0540 / 0.1013\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.1505 / 0.3779\tMAE: 0.0510 / 0.0998\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.1498 / 0.3383\tMAE: 0.0533 / 0.1101\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.1545 / 0.3184\tMAE: 0.0560 / 0.0994\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.1590 / 0.3812\tMAE: 0.0520 / 0.1185\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.1687 / 0.3775\tMAE: 0.0528 / 0.1077\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.1636 / 0.3555\tMAE: 0.0535 / 0.1013\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.1637 / 0.3932\tMAE: 0.0534 / 0.1009\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.1649 / 0.3337\tMAE: 0.0560 / 0.1019\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.1597 / 0.3928\tMAE: 0.0503 / 0.1020\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.1602 / 0.3746\tMAE: 0.0519 / 0.1066\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.1637 / 0.3704\tMAE: 0.0522 / 0.1084\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.1553 / 0.3776\tMAE: 0.0504 / 0.1023\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.1509 / 0.3505\tMAE: 0.0495 / 0.0986\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.1384 / 0.3110\tMAE: 0.0483 / 0.0978\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.1465 / 0.3229\tMAE: 0.0489 / 0.1051\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.1832 / 0.3317\tMAE: 0.0538 / 0.1006\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.1337 / 0.3592\tMAE: 0.0488 / 0.1025\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.1369 / 0.4050\tMAE: 0.0464 / 0.1098\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.1835 / 0.4120\tMAE: 0.0517 / 0.1067\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.1684 / 0.3482\tMAE: 0.0480 / 0.1081\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.1431 / 0.3745\tMAE: 0.0488 / 0.1028\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.1414 / 0.3791\tMAE: 0.0462 / 0.1032\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.1512 / 0.3383\tMAE: 0.0466 / 0.1029\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.1241 / 0.3117\tMAE: 0.0452 / 0.1003\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.1317 / 0.3611\tMAE: 0.0479 / 0.1118\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.1393 / 0.3419\tMAE: 0.0462 / 0.0985\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.1471 / 0.3806\tMAE: 0.0478 / 0.1125\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.1643 / 0.4412\tMAE: 0.0505 / 0.1061\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.1648 / 0.3293\tMAE: 0.0531 / 0.1002\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.1376 / 0.3408\tMAE: 0.0449 / 0.1018\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.1625 / 0.3559\tMAE: 0.0488 / 0.1035\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.1548 / 0.3286\tMAE: 0.0461 / 0.0983\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.1401 / 0.3548\tMAE: 0.0482 / 0.0988\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.1392 / 0.3720\tMAE: 0.0467 / 0.1049\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.1436 / 0.3758\tMAE: 0.0498 / 0.1041\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.1588 / 0.4010\tMAE: 0.0479 / 0.1002\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.1741 / 0.3426\tMAE: 0.0469 / 0.1022\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.1693 / 0.3453\tMAE: 0.0473 / 0.0993\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.1421 / 0.3350\tMAE: 0.0448 / 0.1012\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.1450 / 0.4048\tMAE: 0.0510 / 0.0991\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.1596 / 0.3928\tMAE: 0.0487 / 0.1173\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.1480 / 0.3466\tMAE: 0.0473 / 0.1104\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.1427 / 0.3676\tMAE: 0.0492 / 0.1060\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.1401 / 0.3136\tMAE: 0.0469 / 0.1003\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TFF_01_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 5.1119 / 5.0433\tMAE: 1.5920 / 1.5746\n",
      "Epoch [2/300]\tTrain/Valid Loss: 4.5279 / 3.7149\tMAE: 1.5914 / 1.5737\n",
      "Epoch [3/300]\tTrain/Valid Loss: 3.0397 / 2.1710\tMAE: 1.3329 / 0.6429\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.8307 / 1.6624\tMAE: 0.5440 / 0.4715\n",
      "Epoch [5/300]\tTrain/Valid Loss: 1.5035 / 1.5543\tMAE: 0.3557 / 0.3211\n",
      "Epoch [6/300]\tTrain/Valid Loss: 1.2650 / 1.3491\tMAE: 0.2200 / 0.1955\n",
      "Epoch [7/300]\tTrain/Valid Loss: 1.1657 / 1.2402\tMAE: 0.1834 / 0.1892\n",
      "Epoch [8/300]\tTrain/Valid Loss: 1.1063 / 1.2497\tMAE: 0.1766 / 0.1815\n",
      "Epoch [9/300]\tTrain/Valid Loss: 1.0462 / 1.1706\tMAE: 0.1722 / 0.1961\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.9713 / 0.9454\tMAE: 0.1678 / 0.1707\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.9338 / 0.9104\tMAE: 0.1596 / 0.1715\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.8336 / 1.1610\tMAE: 0.1533 / 0.1711\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.8815 / 0.7819\tMAE: 0.1531 / 0.1486\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.7618 / 0.7663\tMAE: 0.1471 / 0.1453\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.7103 / 0.7586\tMAE: 0.1422 / 0.1434\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.7307 / 0.6936\tMAE: 0.1416 / 0.1425\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.6963 / 0.7707\tMAE: 0.1397 / 0.1539\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.6211 / 0.7065\tMAE: 0.1364 / 0.1731\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.6767 / 0.6531\tMAE: 0.1370 / 0.1459\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.5770 / 0.7215\tMAE: 0.1312 / 0.1356\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.5623 / 0.5919\tMAE: 0.1299 / 0.1287\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.6040 / 0.5688\tMAE: 0.1272 / 0.1287\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.5246 / 0.5441\tMAE: 0.1265 / 0.1286\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.4716 / 0.4852\tMAE: 0.1241 / 0.1275\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.4663 / 0.4852\tMAE: 0.1207 / 0.1262\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.4441 / 0.5094\tMAE: 0.1201 / 0.1261\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.4258 / 0.4757\tMAE: 0.1167 / 0.1361\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.4060 / 0.4627\tMAE: 0.1148 / 0.1252\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.4176 / 0.4612\tMAE: 0.1160 / 0.1215\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.3903 / 0.4393\tMAE: 0.1138 / 0.1291\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.3902 / 0.4285\tMAE: 0.1132 / 0.1268\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.3584 / 0.4210\tMAE: 0.1098 / 0.1192\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.3662 / 0.5036\tMAE: 0.1106 / 0.1389\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.3703 / 0.4076\tMAE: 0.1121 / 0.1163\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.3473 / 0.4080\tMAE: 0.1100 / 0.1189\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.3547 / 0.4280\tMAE: 0.1094 / 0.1272\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.3688 / 0.3988\tMAE: 0.1122 / 0.1339\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.3416 / 0.4476\tMAE: 0.1094 / 0.1242\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.3272 / 0.4103\tMAE: 0.1069 / 0.1219\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.3299 / 0.5145\tMAE: 0.1066 / 0.1612\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.3265 / 0.4048\tMAE: 0.1064 / 0.1243\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.3134 / 0.4109\tMAE: 0.1031 / 0.1312\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.3156 / 0.3941\tMAE: 0.1043 / 0.1188\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.3079 / 0.4149\tMAE: 0.1029 / 0.1284\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.2981 / 0.3890\tMAE: 0.1034 / 0.1238\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.3033 / 0.4545\tMAE: 0.1005 / 0.1504\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.2940 / 0.3833\tMAE: 0.1015 / 0.1140\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.2864 / 0.3764\tMAE: 0.0995 / 0.1204\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.2936 / 0.3975\tMAE: 0.1001 / 0.1114\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.2932 / 0.3892\tMAE: 0.0975 / 0.1108\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.3175 / 0.4198\tMAE: 0.1027 / 0.1193\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.2939 / 0.3563\tMAE: 0.0983 / 0.1135\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.3093 / 0.4581\tMAE: 0.0970 / 0.1122\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.2944 / 0.4782\tMAE: 0.0962 / 0.1628\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.2881 / 0.3569\tMAE: 0.0966 / 0.1190\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.2786 / 0.3748\tMAE: 0.0946 / 0.1099\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.2805 / 0.3683\tMAE: 0.0940 / 0.1167\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.2653 / 0.4390\tMAE: 0.0947 / 0.1201\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.2603 / 0.3620\tMAE: 0.0929 / 0.1095\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.2606 / 0.3576\tMAE: 0.0915 / 0.1077\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.2679 / 0.3589\tMAE: 0.0928 / 0.1234\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.2409 / 0.3993\tMAE: 0.0900 / 0.1132\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.2652 / 0.4549\tMAE: 0.0922 / 0.1278\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.2438 / 0.4585\tMAE: 0.0922 / 0.1141\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.2652 / 0.4158\tMAE: 0.0891 / 0.1168\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.2479 / 0.3877\tMAE: 0.0902 / 0.1075\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.2381 / 0.4145\tMAE: 0.0871 / 0.1374\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.2507 / 0.3776\tMAE: 0.0912 / 0.1124\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.2625 / 0.4381\tMAE: 0.0891 / 0.1166\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.2610 / 0.4718\tMAE: 0.0869 / 0.1152\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.2577 / 0.3982\tMAE: 0.0892 / 0.1133\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.2297 / 0.4031\tMAE: 0.0898 / 0.1176\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.2520 / 0.3846\tMAE: 0.0855 / 0.1139\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.2286 / 0.4124\tMAE: 0.0835 / 0.1165\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.2607 / 0.3537\tMAE: 0.0851 / 0.1062\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.2397 / 0.4038\tMAE: 0.0837 / 0.1197\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.2374 / 0.3979\tMAE: 0.0860 / 0.1149\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.2115 / 0.3499\tMAE: 0.0812 / 0.1067\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.2278 / 0.3563\tMAE: 0.0823 / 0.1092\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.2352 / 0.3383\tMAE: 0.0837 / 0.1084\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.2133 / 0.4377\tMAE: 0.0816 / 0.1117\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.2405 / 0.4064\tMAE: 0.0855 / 0.1213\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.2688 / 0.4228\tMAE: 0.0809 / 0.1083\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.2357 / 0.4489\tMAE: 0.0833 / 0.1138\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.2358 / 0.3711\tMAE: 0.0820 / 0.1092\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.2187 / 0.3620\tMAE: 0.0777 / 0.1120\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.2346 / 0.3582\tMAE: 0.0807 / 0.1075\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.1995 / 0.4240\tMAE: 0.0767 / 0.1103\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.2115 / 0.3797\tMAE: 0.0773 / 0.1192\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.2262 / 0.3567\tMAE: 0.0829 / 0.1070\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.2151 / 0.4111\tMAE: 0.0798 / 0.1240\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.2200 / 0.3779\tMAE: 0.0765 / 0.1102\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.2188 / 0.3921\tMAE: 0.0788 / 0.1080\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.2091 / 0.3744\tMAE: 0.0758 / 0.1154\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.2105 / 0.3746\tMAE: 0.0774 / 0.1062\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.2202 / 0.4849\tMAE: 0.0809 / 0.1123\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.2507 / 0.4358\tMAE: 0.0784 / 0.1143\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.2106 / 0.3663\tMAE: 0.0746 / 0.1072\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.2374 / 0.3670\tMAE: 0.0813 / 0.1181\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.2131 / 0.3505\tMAE: 0.0852 / 0.1088\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.2182 / 0.3686\tMAE: 0.0786 / 0.1087\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.2307 / 0.4078\tMAE: 0.0795 / 0.1164\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.1880 / 0.3401\tMAE: 0.0755 / 0.1068\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.2114 / 0.3324\tMAE: 0.0738 / 0.1067\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.2133 / 0.4070\tMAE: 0.0767 / 0.1279\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.1858 / 0.3706\tMAE: 0.0710 / 0.1038\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.1998 / 0.3626\tMAE: 0.0706 / 0.1079\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.1806 / 0.4398\tMAE: 0.0696 / 0.1058\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.1837 / 0.3820\tMAE: 0.0703 / 0.1027\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.1895 / 0.4101\tMAE: 0.0708 / 0.1250\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.2054 / 0.4358\tMAE: 0.0693 / 0.1064\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.2171 / 0.3725\tMAE: 0.0769 / 0.1045\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.1939 / 0.3779\tMAE: 0.0685 / 0.1083\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.1787 / 0.3293\tMAE: 0.0667 / 0.1045\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.1895 / 0.3715\tMAE: 0.0699 / 0.1018\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.2071 / 0.3394\tMAE: 0.0687 / 0.1047\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.1846 / 0.3749\tMAE: 0.0684 / 0.1132\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2044 / 0.3720\tMAE: 0.0704 / 0.1041\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.1836 / 0.3862\tMAE: 0.0652 / 0.1150\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.2058 / 0.4482\tMAE: 0.0723 / 0.1041\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.1859 / 0.3402\tMAE: 0.0661 / 0.1008\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.1832 / 0.3469\tMAE: 0.0653 / 0.1037\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.1928 / 0.4029\tMAE: 0.0621 / 0.1043\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2002 / 0.4550\tMAE: 0.0650 / 0.1068\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.1931 / 0.3941\tMAE: 0.0657 / 0.1062\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.1977 / 0.3785\tMAE: 0.0664 / 0.1106\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.1768 / 0.3660\tMAE: 0.0612 / 0.1040\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.1679 / 0.3877\tMAE: 0.0625 / 0.1240\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.1899 / 0.3768\tMAE: 0.0632 / 0.1175\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.1843 / 0.3546\tMAE: 0.0686 / 0.1084\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.1942 / 0.4143\tMAE: 0.0682 / 0.1115\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2092 / 0.3650\tMAE: 0.0649 / 0.1038\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.1542 / 0.3514\tMAE: 0.0568 / 0.1027\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.1531 / 0.3824\tMAE: 0.0614 / 0.1075\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.1775 / 0.3612\tMAE: 0.0613 / 0.1049\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.1806 / 0.3541\tMAE: 0.0630 / 0.1050\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.1879 / 0.3796\tMAE: 0.0644 / 0.1070\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.2019 / 0.3692\tMAE: 0.0732 / 0.1080\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.1633 / 0.3366\tMAE: 0.0612 / 0.1118\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.1749 / 0.3539\tMAE: 0.0619 / 0.1035\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.1477 / 0.3869\tMAE: 0.0590 / 0.1052\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.1839 / 0.3692\tMAE: 0.0681 / 0.1051\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.1556 / 0.3657\tMAE: 0.0624 / 0.1057\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.1726 / 0.4202\tMAE: 0.0595 / 0.1047\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.1693 / 0.3946\tMAE: 0.0627 / 0.1085\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.1805 / 0.3286\tMAE: 0.0631 / 0.1070\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.1634 / 0.3369\tMAE: 0.0619 / 0.1069\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.1517 / 0.3317\tMAE: 0.0562 / 0.1020\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.1664 / 0.3722\tMAE: 0.0601 / 0.1038\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.1938 / 0.3602\tMAE: 0.0599 / 0.1200\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.1793 / 0.3999\tMAE: 0.0552 / 0.1076\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.1710 / 0.3666\tMAE: 0.0585 / 0.1030\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.1661 / 0.4336\tMAE: 0.0570 / 0.1239\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.1905 / 0.3453\tMAE: 0.0620 / 0.1080\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.1649 / 0.3407\tMAE: 0.0547 / 0.1051\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.1673 / 0.3694\tMAE: 0.0586 / 0.1033\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.1733 / 0.3911\tMAE: 0.0540 / 0.1048\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.1651 / 0.3419\tMAE: 0.0573 / 0.1064\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.1489 / 0.3574\tMAE: 0.0562 / 0.1027\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.1618 / 0.3619\tMAE: 0.0590 / 0.1040\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.1523 / 0.4022\tMAE: 0.0550 / 0.1047\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.2008 / 0.4197\tMAE: 0.0566 / 0.1031\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.1855 / 0.3594\tMAE: 0.0558 / 0.1118\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.1653 / 0.3858\tMAE: 0.0578 / 0.1051\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.1607 / 0.4308\tMAE: 0.0565 / 0.1309\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.1683 / 0.3931\tMAE: 0.0598 / 0.1026\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.1495 / 0.3499\tMAE: 0.0533 / 0.1076\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.1365 / 0.3418\tMAE: 0.0503 / 0.1029\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.1621 / 0.3992\tMAE: 0.0505 / 0.1031\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.1430 / 0.3412\tMAE: 0.0522 / 0.1037\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.1746 / 0.3456\tMAE: 0.0534 / 0.1020\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.1724 / 0.3433\tMAE: 0.0527 / 0.1029\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.1525 / 0.4120\tMAE: 0.0524 / 0.1047\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.1630 / 0.4007\tMAE: 0.0531 / 0.1036\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.1623 / 0.3687\tMAE: 0.0588 / 0.1009\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.1592 / 0.3347\tMAE: 0.0557 / 0.1028\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.1560 / 0.4479\tMAE: 0.0486 / 0.1022\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.1742 / 0.3960\tMAE: 0.0584 / 0.1051\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.1540 / 0.3383\tMAE: 0.0490 / 0.1160\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.1650 / 0.3798\tMAE: 0.0542 / 0.1057\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.1395 / 0.3599\tMAE: 0.0470 / 0.1141\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.1531 / 0.4041\tMAE: 0.0494 / 0.1001\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.1713 / 0.3945\tMAE: 0.0503 / 0.1037\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.1464 / 0.3349\tMAE: 0.0469 / 0.1073\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.1544 / 0.3784\tMAE: 0.0520 / 0.1013\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.1316 / 0.3588\tMAE: 0.0457 / 0.1083\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.1426 / 0.3761\tMAE: 0.0475 / 0.1018\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.1376 / 0.3483\tMAE: 0.0476 / 0.1025\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.1250 / 0.3407\tMAE: 0.0456 / 0.1027\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.1349 / 0.4006\tMAE: 0.0455 / 0.1028\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.1520 / 0.3804\tMAE: 0.0513 / 0.1162\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.1440 / 0.3771\tMAE: 0.0520 / 0.1028\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.1414 / 0.3461\tMAE: 0.0486 / 0.1025\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.1380 / 0.3968\tMAE: 0.0462 / 0.1007\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.1317 / 0.3875\tMAE: 0.0444 / 0.1023\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.1413 / 0.3580\tMAE: 0.0500 / 0.1037\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.1227 / 0.3384\tMAE: 0.0446 / 0.1022\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.1476 / 0.3286\tMAE: 0.0469 / 0.1019\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.1223 / 0.3795\tMAE: 0.0465 / 0.1022\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.1443 / 0.3389\tMAE: 0.0439 / 0.1008\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.1191 / 0.3370\tMAE: 0.0430 / 0.1016\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.1517 / 0.4077\tMAE: 0.0514 / 0.1162\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.1534 / 0.3699\tMAE: 0.0534 / 0.1059\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.1388 / 0.3330\tMAE: 0.0478 / 0.1025\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.1408 / 0.3442\tMAE: 0.0468 / 0.1026\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.1642 / 0.3384\tMAE: 0.0579 / 0.1037\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.1393 / 0.3523\tMAE: 0.0454 / 0.1011\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.1261 / 0.3716\tMAE: 0.0431 / 0.1043\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.1233 / 0.3640\tMAE: 0.0409 / 0.1036\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.1298 / 0.3519\tMAE: 0.0445 / 0.1053\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.1392 / 0.3989\tMAE: 0.0471 / 0.1041\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.1267 / 0.3654\tMAE: 0.0438 / 0.1059\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.1326 / 0.3500\tMAE: 0.0483 / 0.1017\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.1331 / 0.3721\tMAE: 0.0422 / 0.1106\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.1235 / 0.4180\tMAE: 0.0432 / 0.1026\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1414 / 0.3697\tMAE: 0.0421 / 0.1080\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.1385 / 0.3683\tMAE: 0.0429 / 0.1089\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.1426 / 0.4063\tMAE: 0.0441 / 0.1097\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.1410 / 0.3542\tMAE: 0.0467 / 0.1127\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.1224 / 0.3253\tMAE: 0.0449 / 0.1011\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.1262 / 0.3484\tMAE: 0.0432 / 0.1060\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.1319 / 0.3455\tMAE: 0.0414 / 0.1033\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.1238 / 0.3729\tMAE: 0.0411 / 0.1032\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.1267 / 0.3605\tMAE: 0.0421 / 0.1032\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.1342 / 0.3543\tMAE: 0.0414 / 0.1043\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.1248 / 0.3339\tMAE: 0.0428 / 0.1014\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.1251 / 0.3726\tMAE: 0.0430 / 0.1078\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.1505 / 0.3790\tMAE: 0.0425 / 0.1019\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.1341 / 0.3395\tMAE: 0.0430 / 0.1030\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.1294 / 0.3675\tMAE: 0.0398 / 0.1018\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.1235 / 0.3443\tMAE: 0.0429 / 0.1090\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.1304 / 0.3862\tMAE: 0.0411 / 0.1052\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.1372 / 0.3332\tMAE: 0.0446 / 0.1016\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.1339 / 0.3405\tMAE: 0.0440 / 0.1046\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.1178 / 0.3696\tMAE: 0.0412 / 0.1053\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.1170 / 0.3617\tMAE: 0.0390 / 0.1019\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.1249 / 0.3417\tMAE: 0.0440 / 0.1008\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.1283 / 0.4029\tMAE: 0.0403 / 0.1154\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.1325 / 0.3384\tMAE: 0.0444 / 0.1071\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.1261 / 0.3340\tMAE: 0.0436 / 0.1031\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1332 / 0.3914\tMAE: 0.0425 / 0.1035\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.1233 / 0.3455\tMAE: 0.0421 / 0.1016\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.1172 / 0.3494\tMAE: 0.0418 / 0.1041\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.1312 / 0.4073\tMAE: 0.0426 / 0.1040\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.1570 / 0.3493\tMAE: 0.0391 / 0.1038\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.1205 / 0.3291\tMAE: 0.0371 / 0.1020\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.1162 / 0.3434\tMAE: 0.0413 / 0.1031\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.1217 / 0.3792\tMAE: 0.0401 / 0.1060\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.1189 / 0.3520\tMAE: 0.0417 / 0.1027\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.1156 / 0.3347\tMAE: 0.0399 / 0.1036\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.1300 / 0.3397\tMAE: 0.0398 / 0.1059\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.1383 / 0.4062\tMAE: 0.0419 / 0.1065\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.1283 / 0.3820\tMAE: 0.0394 / 0.1045\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.1290 / 0.3658\tMAE: 0.0404 / 0.1024\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.1115 / 0.3773\tMAE: 0.0365 / 0.1047\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.1234 / 0.3853\tMAE: 0.0394 / 0.1059\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.1361 / 0.3765\tMAE: 0.0389 / 0.1009\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.1160 / 0.3622\tMAE: 0.0376 / 0.1060\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.1261 / 0.3829\tMAE: 0.0367 / 0.1010\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.1240 / 0.3647\tMAE: 0.0370 / 0.1024\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.1232 / 0.3633\tMAE: 0.0384 / 0.1034\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.1198 / 0.3336\tMAE: 0.0414 / 0.1048\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.1086 / 0.3485\tMAE: 0.0359 / 0.1045\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.1248 / 0.3574\tMAE: 0.0380 / 0.1135\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.1335 / 0.3550\tMAE: 0.0373 / 0.1030\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.1119 / 0.3388\tMAE: 0.0422 / 0.1027\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.1262 / 0.4080\tMAE: 0.0356 / 0.1022\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.1193 / 0.3681\tMAE: 0.0351 / 0.1033\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.1088 / 0.3459\tMAE: 0.0349 / 0.1008\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.1264 / 0.3560\tMAE: 0.0350 / 0.1134\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.1054 / 0.3384\tMAE: 0.0369 / 0.1028\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.1314 / 0.3509\tMAE: 0.0441 / 0.1086\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.1156 / 0.3348\tMAE: 0.0376 / 0.1031\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.1068 / 0.3350\tMAE: 0.0338 / 0.1064\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.1272 / 0.3773\tMAE: 0.0379 / 0.1051\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.1180 / 0.3943\tMAE: 0.0379 / 0.1117\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.1094 / 0.3462\tMAE: 0.0376 / 0.1032\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.1100 / 0.3528\tMAE: 0.0346 / 0.1065\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.1145 / 0.3807\tMAE: 0.0377 / 0.1018\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.1420 / 0.3737\tMAE: 0.0342 / 0.1033\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.1210 / 0.3678\tMAE: 0.0406 / 0.1126\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.1025 / 0.3690\tMAE: 0.0374 / 0.1067\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.1087 / 0.3525\tMAE: 0.0373 / 0.1066\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.1216 / 0.3577\tMAE: 0.0351 / 0.1037\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.1105 / 0.3367\tMAE: 0.0357 / 0.1016\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.1202 / 0.3599\tMAE: 0.0375 / 0.1078\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.1108 / 0.3513\tMAE: 0.0356 / 0.1043\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.1273 / 0.3643\tMAE: 0.0418 / 0.1129\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.1246 / 0.3599\tMAE: 0.0418 / 0.1149\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.1106 / 0.3343\tMAE: 0.0361 / 0.1048\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.1138 / 0.3649\tMAE: 0.0360 / 0.1022\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.1011 / 0.3860\tMAE: 0.0338 / 0.1033\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.1167 / 0.3652\tMAE: 0.0351 / 0.1025\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.1105 / 0.3652\tMAE: 0.0331 / 0.1157\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.1155 / 0.3519\tMAE: 0.0360 / 0.1016\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.1115 / 0.3763\tMAE: 0.0328 / 0.1018\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.1145 / 0.3652\tMAE: 0.0370 / 0.1082\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.1203 / 0.3669\tMAE: 0.0367 / 0.1038\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.0974 / 0.3385\tMAE: 0.0324 / 0.1027\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.1074 / 0.3720\tMAE: 0.0364 / 0.1103\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TTT_01_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 5.0960 / 4.9607\tMAE: 1.5286 / 1.4213\n",
      "Epoch [2/300]\tTrain/Valid Loss: 4.1572 / 2.8412\tMAE: 1.0882 / 0.7399\n",
      "Epoch [3/300]\tTrain/Valid Loss: 2.2982 / 2.1156\tMAE: 0.6218 / 0.5536\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.7589 / 1.7240\tMAE: 0.4415 / 0.3699\n",
      "Epoch [5/300]\tTrain/Valid Loss: 1.4741 / 1.4124\tMAE: 0.2806 / 0.2331\n",
      "Epoch [6/300]\tTrain/Valid Loss: 1.3269 / 1.3451\tMAE: 0.2179 / 0.2141\n",
      "Epoch [7/300]\tTrain/Valid Loss: 1.2176 / 1.3293\tMAE: 0.1982 / 0.2083\n",
      "Epoch [8/300]\tTrain/Valid Loss: 1.1557 / 1.3321\tMAE: 0.1929 / 0.2177\n",
      "Epoch [9/300]\tTrain/Valid Loss: 1.0970 / 1.1417\tMAE: 0.1842 / 0.1765\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.9906 / 1.0535\tMAE: 0.1710 / 0.1676\n",
      "Epoch [11/300]\tTrain/Valid Loss: 1.0246 / 1.0245\tMAE: 0.1666 / 0.1623\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.8522 / 1.0955\tMAE: 0.1572 / 0.1763\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.8035 / 0.9669\tMAE: 0.1523 / 0.1615\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.8377 / 0.8996\tMAE: 0.1488 / 0.1537\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.7148 / 0.8243\tMAE: 0.1417 / 0.1459\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.6508 / 0.9052\tMAE: 0.1408 / 0.1431\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.6706 / 0.8392\tMAE: 0.1341 / 0.1446\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.5778 / 0.6328\tMAE: 0.1335 / 0.1518\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.5505 / 0.6431\tMAE: 0.1282 / 0.1433\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.5304 / 0.7933\tMAE: 0.1243 / 0.1409\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.5250 / 0.6158\tMAE: 0.1227 / 0.1327\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.4749 / 0.6084\tMAE: 0.1173 / 0.1308\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.4629 / 0.5700\tMAE: 0.1184 / 0.1278\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.4436 / 0.7016\tMAE: 0.1149 / 0.1371\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.4025 / 0.6881\tMAE: 0.1139 / 0.1328\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.3862 / 0.4926\tMAE: 0.1183 / 0.1284\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.3411 / 0.5108\tMAE: 0.1078 / 0.1367\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.3738 / 0.4799\tMAE: 0.1083 / 0.1241\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.3211 / 0.4580\tMAE: 0.1034 / 0.1250\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.3111 / 0.5361\tMAE: 0.1027 / 0.1337\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.2949 / 0.4496\tMAE: 0.1034 / 0.1270\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.2751 / 0.5110\tMAE: 0.0983 / 0.1251\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.2603 / 0.4490\tMAE: 0.0953 / 0.1247\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.2741 / 0.4432\tMAE: 0.0993 / 0.1262\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.2732 / 0.4920\tMAE: 0.0988 / 0.1251\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.2680 / 0.5012\tMAE: 0.1013 / 0.1229\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.2417 / 0.4217\tMAE: 0.0908 / 0.1260\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.2394 / 0.4344\tMAE: 0.0888 / 0.1190\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.2239 / 0.5809\tMAE: 0.0892 / 0.1338\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.2300 / 0.4227\tMAE: 0.0899 / 0.1217\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.2507 / 0.4918\tMAE: 0.0916 / 0.1237\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.2442 / 0.4247\tMAE: 0.0874 / 0.1189\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.2381 / 0.4446\tMAE: 0.0910 / 0.1206\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.2215 / 0.4490\tMAE: 0.0878 / 0.1279\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.2059 / 0.4008\tMAE: 0.0846 / 0.1216\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.2085 / 0.4451\tMAE: 0.0814 / 0.1271\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.2250 / 0.4267\tMAE: 0.0883 / 0.1174\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.2077 / 0.4325\tMAE: 0.0828 / 0.1361\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.2055 / 0.4574\tMAE: 0.0809 / 0.1212\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.1893 / 0.4138\tMAE: 0.0792 / 0.1174\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.1911 / 0.4675\tMAE: 0.0778 / 0.1135\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.1910 / 0.5096\tMAE: 0.0781 / 0.1223\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.1894 / 0.5132\tMAE: 0.0795 / 0.1366\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.1907 / 0.4404\tMAE: 0.0810 / 0.1136\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.1878 / 0.4234\tMAE: 0.0722 / 0.1157\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.1773 / 0.3859\tMAE: 0.0738 / 0.1150\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.1694 / 0.4185\tMAE: 0.0705 / 0.1152\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.1760 / 0.5070\tMAE: 0.0703 / 0.1298\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.1781 / 0.4198\tMAE: 0.0739 / 0.1157\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.1698 / 0.4199\tMAE: 0.0730 / 0.1180\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.1865 / 0.5304\tMAE: 0.0726 / 0.1199\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.1971 / 0.6050\tMAE: 0.0756 / 0.1341\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.1828 / 0.4331\tMAE: 0.0714 / 0.1131\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.1772 / 0.4291\tMAE: 0.0699 / 0.1114\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.1992 / 0.4481\tMAE: 0.0701 / 0.1193\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.1971 / 0.4450\tMAE: 0.0691 / 0.1152\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.1626 / 0.3843\tMAE: 0.0642 / 0.1145\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.1813 / 0.5012\tMAE: 0.0664 / 0.1383\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.1673 / 0.4500\tMAE: 0.0682 / 0.1146\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.1694 / 0.4399\tMAE: 0.0646 / 0.1175\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.1635 / 0.4602\tMAE: 0.0670 / 0.1264\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.1624 / 0.4182\tMAE: 0.0652 / 0.1109\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.1525 / 0.4446\tMAE: 0.0619 / 0.1142\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.1614 / 0.4130\tMAE: 0.0689 / 0.1365\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.1741 / 0.4220\tMAE: 0.0678 / 0.1154\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.1588 / 0.4671\tMAE: 0.0622 / 0.1203\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.1599 / 0.4015\tMAE: 0.0634 / 0.1134\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.1706 / 0.5129\tMAE: 0.0645 / 0.1389\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.1769 / 0.4865\tMAE: 0.0669 / 0.1170\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.1550 / 0.4000\tMAE: 0.0655 / 0.1220\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.1382 / 0.4224\tMAE: 0.0593 / 0.1124\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.1420 / 0.4186\tMAE: 0.0558 / 0.1150\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.1539 / 0.3970\tMAE: 0.0596 / 0.1124\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.1504 / 0.5036\tMAE: 0.0550 / 0.1199\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.1428 / 0.4208\tMAE: 0.0537 / 0.1092\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.1843 / 0.5211\tMAE: 0.0580 / 0.1137\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.1461 / 0.4098\tMAE: 0.0572 / 0.1151\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.1412 / 0.4131\tMAE: 0.0545 / 0.1395\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.1315 / 0.3692\tMAE: 0.0526 / 0.1102\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.1523 / 0.3991\tMAE: 0.0601 / 0.1107\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.1761 / 0.4632\tMAE: 0.0524 / 0.1214\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.1535 / 0.4558\tMAE: 0.0589 / 0.1184\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.1414 / 0.5377\tMAE: 0.0549 / 0.1203\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.1294 / 0.4360\tMAE: 0.0542 / 0.1170\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.1397 / 0.4810\tMAE: 0.0597 / 0.1151\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.1342 / 0.4012\tMAE: 0.0520 / 0.1112\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.1265 / 0.4906\tMAE: 0.0529 / 0.1437\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.1402 / 0.4711\tMAE: 0.0554 / 0.1181\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.1462 / 0.4113\tMAE: 0.0586 / 0.1150\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.1253 / 0.4844\tMAE: 0.0488 / 0.1188\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.1448 / 0.4281\tMAE: 0.0531 / 0.1130\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.1231 / 0.3681\tMAE: 0.0513 / 0.1138\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.1352 / 0.4205\tMAE: 0.0492 / 0.1179\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.1268 / 0.4036\tMAE: 0.0500 / 0.1124\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.1167 / 0.3828\tMAE: 0.0467 / 0.1117\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.1248 / 0.3984\tMAE: 0.0470 / 0.1138\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.1339 / 0.4117\tMAE: 0.0512 / 0.1127\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.1175 / 0.4295\tMAE: 0.0462 / 0.1158\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.1234 / 0.4402\tMAE: 0.0463 / 0.1145\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.1248 / 0.4362\tMAE: 0.0447 / 0.1205\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.1249 / 0.4302\tMAE: 0.0473 / 0.1234\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.1352 / 0.4057\tMAE: 0.0454 / 0.1209\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.1235 / 0.3943\tMAE: 0.0468 / 0.1173\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.1173 / 0.4334\tMAE: 0.0440 / 0.1159\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.1145 / 0.4771\tMAE: 0.0436 / 0.1272\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.1076 / 0.4243\tMAE: 0.0413 / 0.1114\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.1341 / 0.4204\tMAE: 0.0501 / 0.1308\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.1270 / 0.4647\tMAE: 0.0500 / 0.1294\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.1252 / 0.4350\tMAE: 0.0454 / 0.1148\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.1189 / 0.4075\tMAE: 0.0466 / 0.1170\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.1169 / 0.3850\tMAE: 0.0505 / 0.1097\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.1077 / 0.4074\tMAE: 0.0390 / 0.1128\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.1248 / 0.3832\tMAE: 0.0427 / 0.1162\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.1100 / 0.4136\tMAE: 0.0414 / 0.1216\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.1074 / 0.4133\tMAE: 0.0422 / 0.1208\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.1130 / 0.4246\tMAE: 0.0376 / 0.1124\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.1167 / 0.4285\tMAE: 0.0378 / 0.1126\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.1157 / 0.4769\tMAE: 0.0415 / 0.1173\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.1144 / 0.3975\tMAE: 0.0419 / 0.1154\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.1140 / 0.4038\tMAE: 0.0417 / 0.1203\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.0993 / 0.3975\tMAE: 0.0387 / 0.1179\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.1036 / 0.4219\tMAE: 0.0439 / 0.1143\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.1103 / 0.4151\tMAE: 0.0384 / 0.1239\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.1100 / 0.3992\tMAE: 0.0377 / 0.1094\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.0961 / 0.4038\tMAE: 0.0394 / 0.1126\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.1171 / 0.4729\tMAE: 0.0410 / 0.1172\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.1416 / 0.4515\tMAE: 0.0445 / 0.1130\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.1268 / 0.4070\tMAE: 0.0400 / 0.1147\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.0937 / 0.4041\tMAE: 0.0358 / 0.1159\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.0959 / 0.3924\tMAE: 0.0370 / 0.1146\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.1263 / 0.4405\tMAE: 0.0378 / 0.1137\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.1253 / 0.4283\tMAE: 0.0419 / 0.1219\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.1036 / 0.4339\tMAE: 0.0379 / 0.1200\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.1348 / 0.4761\tMAE: 0.0491 / 0.1179\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.1128 / 0.3847\tMAE: 0.0394 / 0.1112\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.1100 / 0.3832\tMAE: 0.0374 / 0.1115\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.1092 / 0.4470\tMAE: 0.0367 / 0.1471\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.0968 / 0.4111\tMAE: 0.0375 / 0.1119\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.0992 / 0.4034\tMAE: 0.0351 / 0.1104\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.0954 / 0.4214\tMAE: 0.0372 / 0.1219\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.1146 / 0.4367\tMAE: 0.0425 / 0.1163\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.1177 / 0.4054\tMAE: 0.0427 / 0.1137\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.1001 / 0.4050\tMAE: 0.0368 / 0.1116\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.1020 / 0.5128\tMAE: 0.0353 / 0.1147\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.1072 / 0.4192\tMAE: 0.0314 / 0.1167\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.1020 / 0.4694\tMAE: 0.0338 / 0.1138\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.1171 / 0.4092\tMAE: 0.0412 / 0.1118\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.1122 / 0.4045\tMAE: 0.0372 / 0.1147\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.0941 / 0.4002\tMAE: 0.0328 / 0.1124\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.0993 / 0.4605\tMAE: 0.0372 / 0.1259\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.1149 / 0.4076\tMAE: 0.0415 / 0.1192\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.1047 / 0.4077\tMAE: 0.0401 / 0.1158\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.0962 / 0.4647\tMAE: 0.0358 / 0.1151\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.0994 / 0.4279\tMAE: 0.0350 / 0.1193\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.0937 / 0.3972\tMAE: 0.0373 / 0.1121\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.0879 / 0.4042\tMAE: 0.0315 / 0.1112\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.0974 / 0.3980\tMAE: 0.0354 / 0.1123\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.0962 / 0.4338\tMAE: 0.0327 / 0.1131\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.1174 / 0.3982\tMAE: 0.0341 / 0.1204\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.1389 / 0.4360\tMAE: 0.0357 / 0.1249\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.1152 / 0.4695\tMAE: 0.0368 / 0.1163\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.1030 / 0.4141\tMAE: 0.0337 / 0.1153\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.1228 / 0.4590\tMAE: 0.0378 / 0.1165\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.1155 / 0.4391\tMAE: 0.0365 / 0.1128\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.1011 / 0.4640\tMAE: 0.0349 / 0.1369\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.0991 / 0.3956\tMAE: 0.0394 / 0.1122\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.1447 / 0.4388\tMAE: 0.0573 / 0.1144\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.1015 / 0.3921\tMAE: 0.0364 / 0.1121\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.0906 / 0.5114\tMAE: 0.0359 / 0.1171\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.0981 / 0.3842\tMAE: 0.0326 / 0.1131\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.0937 / 0.4067\tMAE: 0.0354 / 0.1166\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.1048 / 0.3936\tMAE: 0.0439 / 0.1111\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.0887 / 0.4076\tMAE: 0.0326 / 0.1153\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.1164 / 0.4020\tMAE: 0.0398 / 0.1145\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.1175 / 0.4073\tMAE: 0.0384 / 0.1146\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.1006 / 0.4245\tMAE: 0.0319 / 0.1106\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.1105 / 0.3967\tMAE: 0.0377 / 0.1122\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.1088 / 0.3947\tMAE: 0.0379 / 0.1132\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.0928 / 0.4123\tMAE: 0.0316 / 0.1109\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.1268 / 0.4221\tMAE: 0.0385 / 0.1189\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.1148 / 0.4192\tMAE: 0.0366 / 0.1186\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.0845 / 0.4117\tMAE: 0.0311 / 0.1122\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.0972 / 0.4752\tMAE: 0.0354 / 0.1199\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.1084 / 0.3960\tMAE: 0.0324 / 0.1198\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.0859 / 0.4069\tMAE: 0.0338 / 0.1194\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.0839 / 0.3933\tMAE: 0.0307 / 0.1137\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.1070 / 0.4573\tMAE: 0.0402 / 0.1177\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.0890 / 0.4437\tMAE: 0.0312 / 0.1190\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.1066 / 0.4178\tMAE: 0.0303 / 0.1129\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.0890 / 0.4529\tMAE: 0.0291 / 0.1195\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.0823 / 0.4078\tMAE: 0.0295 / 0.1198\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.0783 / 0.3930\tMAE: 0.0300 / 0.1139\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.0991 / 0.4095\tMAE: 0.0336 / 0.1204\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.0940 / 0.4248\tMAE: 0.0302 / 0.1127\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.0976 / 0.4082\tMAE: 0.0334 / 0.1146\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.0844 / 0.4725\tMAE: 0.0306 / 0.1147\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.0903 / 0.3881\tMAE: 0.0366 / 0.1140\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.0916 / 0.4447\tMAE: 0.0332 / 0.1137\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.0938 / 0.4076\tMAE: 0.0310 / 0.1112\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.0984 / 0.4161\tMAE: 0.0326 / 0.1128\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.0782 / 0.4276\tMAE: 0.0293 / 0.1134\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.1005 / 0.4227\tMAE: 0.0300 / 0.1147\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.0798 / 0.4215\tMAE: 0.0280 / 0.1198\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.1004 / 0.4245\tMAE: 0.0349 / 0.1259\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.1027 / 0.4594\tMAE: 0.0310 / 0.1145\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1105 / 0.4583\tMAE: 0.0390 / 0.1120\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.1065 / 0.4599\tMAE: 0.0317 / 0.1204\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.0791 / 0.4386\tMAE: 0.0271 / 0.1192\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.0870 / 0.3937\tMAE: 0.0309 / 0.1138\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.0964 / 0.4004\tMAE: 0.0377 / 0.1133\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.1119 / 0.4558\tMAE: 0.0308 / 0.1153\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.0995 / 0.4410\tMAE: 0.0346 / 0.1215\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.0910 / 0.4131\tMAE: 0.0268 / 0.1137\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.0911 / 0.4141\tMAE: 0.0350 / 0.1198\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.0894 / 0.3879\tMAE: 0.0312 / 0.1147\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.0999 / 0.4246\tMAE: 0.0311 / 0.1148\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.0867 / 0.4247\tMAE: 0.0269 / 0.1174\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.0796 / 0.3905\tMAE: 0.0309 / 0.1117\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.0801 / 0.4069\tMAE: 0.0270 / 0.1143\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.0972 / 0.4046\tMAE: 0.0341 / 0.1131\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.0846 / 0.4334\tMAE: 0.0310 / 0.1138\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.0994 / 0.4047\tMAE: 0.0329 / 0.1139\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.0810 / 0.4266\tMAE: 0.0330 / 0.1194\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.0998 / 0.4456\tMAE: 0.0340 / 0.1136\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.1088 / 0.4243\tMAE: 0.0307 / 0.1156\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.0901 / 0.3958\tMAE: 0.0321 / 0.1130\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.0909 / 0.3997\tMAE: 0.0299 / 0.1233\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.0868 / 0.4256\tMAE: 0.0313 / 0.1137\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.0921 / 0.4085\tMAE: 0.0277 / 0.1286\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.1018 / 0.4666\tMAE: 0.0281 / 0.1204\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1033 / 0.4160\tMAE: 0.0375 / 0.1147\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.0848 / 0.4401\tMAE: 0.0298 / 0.1149\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.0948 / 0.4491\tMAE: 0.0281 / 0.1154\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.0905 / 0.4056\tMAE: 0.0307 / 0.1148\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.0720 / 0.3979\tMAE: 0.0277 / 0.1135\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.0870 / 0.4323\tMAE: 0.0307 / 0.1208\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.1093 / 0.4275\tMAE: 0.0281 / 0.1168\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.0703 / 0.4090\tMAE: 0.0263 / 0.1174\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.0776 / 0.4196\tMAE: 0.0296 / 0.1145\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.0727 / 0.4658\tMAE: 0.0238 / 0.1130\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.0874 / 0.3870\tMAE: 0.0279 / 0.1137\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.0927 / 0.4153\tMAE: 0.0303 / 0.1206\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.1023 / 0.4372\tMAE: 0.0381 / 0.1163\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.0791 / 0.4083\tMAE: 0.0284 / 0.1183\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.0889 / 0.4119\tMAE: 0.0343 / 0.1127\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.0887 / 0.4266\tMAE: 0.0263 / 0.1199\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.1027 / 0.4590\tMAE: 0.0264 / 0.1184\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.0788 / 0.4142\tMAE: 0.0291 / 0.1128\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.0667 / 0.3996\tMAE: 0.0257 / 0.1118\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.0695 / 0.4280\tMAE: 0.0247 / 0.1140\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.0822 / 0.4085\tMAE: 0.0257 / 0.1177\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.0917 / 0.4671\tMAE: 0.0358 / 0.1175\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.0870 / 0.4108\tMAE: 0.0265 / 0.1194\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.0854 / 0.3982\tMAE: 0.0250 / 0.1130\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.0906 / 0.3916\tMAE: 0.0289 / 0.1128\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.0782 / 0.4154\tMAE: 0.0262 / 0.1168\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.0965 / 0.4294\tMAE: 0.0342 / 0.1183\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.1004 / 0.4735\tMAE: 0.0296 / 0.1132\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.0842 / 0.4080\tMAE: 0.0273 / 0.1189\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.0833 / 0.4184\tMAE: 0.0287 / 0.1185\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.0953 / 0.4351\tMAE: 0.0373 / 0.1226\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.0874 / 0.4383\tMAE: 0.0296 / 0.1143\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.1039 / 0.4531\tMAE: 0.0287 / 0.1142\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.0836 / 0.4626\tMAE: 0.0239 / 0.1142\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.0866 / 0.4263\tMAE: 0.0262 / 0.1217\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.0857 / 0.4353\tMAE: 0.0324 / 0.1132\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.0888 / 0.4258\tMAE: 0.0278 / 0.1120\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.0812 / 0.4129\tMAE: 0.0227 / 0.1130\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.0954 / 0.3946\tMAE: 0.0279 / 0.1149\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.0788 / 0.3945\tMAE: 0.0239 / 0.1124\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.0704 / 0.4033\tMAE: 0.0262 / 0.1125\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.0810 / 0.4655\tMAE: 0.0234 / 0.1138\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.0916 / 0.4353\tMAE: 0.0270 / 0.1120\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.0734 / 0.4270\tMAE: 0.0230 / 0.1137\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.0950 / 0.4121\tMAE: 0.0298 / 0.1130\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.0730 / 0.4187\tMAE: 0.0274 / 0.1121\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.0816 / 0.4241\tMAE: 0.0258 / 0.1181\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.0988 / 0.4264\tMAE: 0.0346 / 0.1163\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.0754 / 0.4161\tMAE: 0.0256 / 0.1185\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.0788 / 0.4233\tMAE: 0.0255 / 0.1174\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.1011 / 0.4021\tMAE: 0.0344 / 0.1175\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.0821 / 0.4192\tMAE: 0.0267 / 0.1179\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.0785 / 0.4058\tMAE: 0.0243 / 0.1133\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.0769 / 0.4222\tMAE: 0.0263 / 0.1180\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.0894 / 0.4155\tMAE: 0.0282 / 0.1153\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.0792 / 0.4185\tMAE: 0.0243 / 0.1154\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.0711 / 0.4403\tMAE: 0.0243 / 0.1164\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.0971 / 0.4658\tMAE: 0.0349 / 0.1133\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.0797 / 0.3982\tMAE: 0.0262 / 0.1149\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.0830 / 0.4571\tMAE: 0.0262 / 0.1137\n"
     ]
    }
   ],
   "source": [
    "from model.model_02r import DistNN\n",
    "from util import trainer_mix as tr\n",
    "\n",
    "for scale in ['metal_FFF','metal_TFF','metal_TTT']:\n",
    "    exec_model(scale=scale, model_type='M02R', comment='L1_logL1', batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfb2e760b55082f7e18274ad9b6beeb89af4df0a5c88a9ce379e413b137aeb47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ex01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
