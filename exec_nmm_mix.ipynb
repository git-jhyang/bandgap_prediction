{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, gc, json\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from util.input_data import Dataset\n",
    "from util.AdaBound import AdaBound\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def exec_model(\n",
    "    scale,\n",
    "    model_type,\n",
    "    dataset,\n",
    "    comment='',\n",
    "    lr = 1e-5,\n",
    "    wd = 1e-7,\n",
    "    tries = 1,\n",
    "    root_model = 'd:/MODELS/202204/nmm',\n",
    "    num_epochs = 300,\n",
    "    batch_size = 128,\n",
    "    train_ratio = 0.7,\n",
    "    valid_ratio = 0.2,\n",
    "    metal_ratio = 1,\n",
    "):\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    for n in range(0, tries):\n",
    "        rseed  = 35 + n\n",
    "        train_data, valid_data, test_data = dataset.train_test_split(train_ratio=train_ratio, \n",
    "                                                                     valid_ratio=valid_ratio,\n",
    "                                                                     rseed=rseed,\n",
    "                                                                     metal_ratio=metal_ratio)\n",
    "        train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                                    collate_fn=tr.collate_fn)\n",
    "        val_data_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "        test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "\n",
    "        model = DistNN(dataset.n_atom_feats, dataset.n_rdf_feature, dataset.n_bdf_feature).cuda()\n",
    "        optimizer = AdaBound(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = torch.nn.L1Loss()\n",
    "\n",
    "        for i in range(99):\n",
    "            root = os.path.join(root_model, model_type)\n",
    "            if not os.path.isdir(root):\n",
    "                os.makedirs(root)\n",
    "            if '{}_{:02d}'.format(scale, i) not in ' '.join(os.listdir(root)):\n",
    "                output_root = os.path.join(root, '{}_{:02d}'.format(scale, i))\n",
    "                if len(comment) > 0: output_root += f'_{comment}'\n",
    "                os.makedirs(output_root)\n",
    "                break\n",
    "        print(output_root)\n",
    "        with open(os.path.join(output_root, 'params.json'),'w') as f:\n",
    "            json.dump(dict(random_seed=rseed, learning_rate=lr, weight_decay=wd, \n",
    "                train_ratio=train_ratio, valid_ratio=valid_ratio, batch_size=batch_size,\n",
    "                metal_ratio=metal_ratio), \n",
    "                f, indent=4)\n",
    "        writer = SummaryWriter(output_root)\n",
    "        #with torch.no_grad():\n",
    "        #    dummy = iter(test_data_loader).next()\n",
    "        #    writer.add_graph(model, dummy[:7])\n",
    "\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            train_loss, train_mae = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "            valid_loss, valid_mae, valid_rmse, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "\n",
    "            print('Epoch [{}/{}]\\tTrain / Valid Loss: {:.4f} / {:.4f}\\tMAE: {:.4f} / {:.4f}'\n",
    "                    .format(epoch, num_epochs, train_loss, valid_loss, train_mae, valid_mae))\n",
    "\n",
    "            writer.add_scalar('train/loss', train_loss, epoch)\n",
    "            writer.add_scalar('train/MAE', train_mae, epoch)\n",
    "#            writer.add_scalar('train/F1', train_f1, epoch)\n",
    "            writer.add_scalar('valid/loss', valid_loss, epoch)\n",
    "            writer.add_scalar('valid/MAE', valid_mae, epoch)\n",
    "#            writer.add_scalar('valid/F1', valid_f1, epoch)\n",
    "\n",
    "            if epoch%20 == 0:\n",
    "                torch.save(model.state_dict(), \n",
    "                           os.path.join(output_root, 'model.{:05d}.pt'.format(epoch)))\n",
    "                _, _, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "                np.savetxt(os.path.join(output_root, 'pred.{:05d}.txt'.format(epoch)), \n",
    "                           np.hstack([idxs, targets, preds]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20% of metal data used - 1649 metal with 10387 insulator (14%)\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TTT_06_M2_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 4.2565 / 3.4001\tMAE: 2.1070 / 1.8546\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.6597 / 2.1957\tMAE: 1.3223 / 1.0979\n",
      "Epoch [3/300]\tTrain/Valid Loss: 2.0696 / 1.8098\tMAE: 0.9336 / 0.8652\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.7087 / 1.3129\tMAE: 0.7272 / 0.6201\n",
      "Epoch [5/300]\tTrain/Valid Loss: 1.1370 / 0.8584\tMAE: 0.4950 / 0.4508\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.8620 / 0.6081\tMAE: 0.3279 / 0.2629\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.7275 / 0.5668\tMAE: 0.2685 / 0.2646\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.7508 / 0.5897\tMAE: 0.2801 / 0.2563\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.6589 / 0.5572\tMAE: 0.2419 / 0.2456\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.6443 / 0.5483\tMAE: 0.2325 / 0.2495\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.6292 / 0.4905\tMAE: 0.2254 / 0.2288\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.6141 / 0.5017\tMAE: 0.2215 / 0.2240\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.6047 / 0.5226\tMAE: 0.2170 / 0.2177\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.6245 / 0.4815\tMAE: 0.2267 / 0.2297\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.5897 / 0.4824\tMAE: 0.2087 / 0.2223\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.5640 / 0.4884\tMAE: 0.1990 / 0.2237\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.5655 / 0.4595\tMAE: 0.2039 / 0.2106\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.5533 / 0.4808\tMAE: 0.1945 / 0.2177\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.5298 / 0.4727\tMAE: 0.1878 / 0.2262\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.5162 / 0.4634\tMAE: 0.1830 / 0.2209\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.5224 / 0.5218\tMAE: 0.1878 / 0.2399\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.5230 / 0.4735\tMAE: 0.1833 / 0.2384\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.4965 / 0.4239\tMAE: 0.1756 / 0.1983\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.4994 / 0.4737\tMAE: 0.1755 / 0.2296\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.5052 / 0.4823\tMAE: 0.1789 / 0.2361\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.4908 / 0.4890\tMAE: 0.1805 / 0.2276\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.5257 / 0.6328\tMAE: 0.1872 / 0.2337\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.5487 / 0.4586\tMAE: 0.1917 / 0.2236\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.5228 / 0.4334\tMAE: 0.1934 / 0.1953\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.4821 / 0.5748\tMAE: 0.1731 / 0.3089\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.5087 / 0.4465\tMAE: 0.1926 / 0.2147\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.5896 / 0.4407\tMAE: 0.2236 / 0.2057\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.5005 / 0.4061\tMAE: 0.1803 / 0.1906\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.4917 / 0.4427\tMAE: 0.1782 / 0.2169\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.4928 / 0.4078\tMAE: 0.1835 / 0.1943\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.4461 / 0.3998\tMAE: 0.1634 / 0.1848\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.4415 / 0.5035\tMAE: 0.1582 / 0.2665\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.4397 / 0.4831\tMAE: 0.1609 / 0.2386\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.4945 / 0.5018\tMAE: 0.1942 / 0.2664\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.4725 / 0.4055\tMAE: 0.1808 / 0.1924\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.4418 / 0.4362\tMAE: 0.1637 / 0.2070\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.4327 / 0.3962\tMAE: 0.1635 / 0.1915\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.4190 / 0.4147\tMAE: 0.1511 / 0.1872\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.4256 / 0.4028\tMAE: 0.1529 / 0.1879\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.4047 / 0.4083\tMAE: 0.1453 / 0.1899\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.4136 / 0.4531\tMAE: 0.1583 / 0.2229\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.3992 / 0.5010\tMAE: 0.1465 / 0.2609\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.3898 / 0.4323\tMAE: 0.1426 / 0.2099\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.4792 / 0.4672\tMAE: 0.1737 / 0.2121\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.4558 / 0.4050\tMAE: 0.1643 / 0.1964\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.4176 / 0.3926\tMAE: 0.1530 / 0.1811\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.4026 / 0.3999\tMAE: 0.1497 / 0.1858\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.4282 / 0.3954\tMAE: 0.1575 / 0.1875\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.3993 / 0.3970\tMAE: 0.1464 / 0.1803\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.3884 / 0.3838\tMAE: 0.1408 / 0.1806\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.4020 / 0.4290\tMAE: 0.1428 / 0.2016\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.4068 / 0.4230\tMAE: 0.1447 / 0.1976\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.3845 / 0.4134\tMAE: 0.1374 / 0.1889\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.3825 / 0.4072\tMAE: 0.1372 / 0.1846\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.3669 / 0.3785\tMAE: 0.1315 / 0.1751\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.3668 / 0.4408\tMAE: 0.1297 / 0.2305\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.3686 / 0.3686\tMAE: 0.1358 / 0.1748\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.3566 / 0.3784\tMAE: 0.1275 / 0.1782\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.3704 / 0.4326\tMAE: 0.1353 / 0.2111\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.3583 / 0.3863\tMAE: 0.1271 / 0.1813\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.3567 / 0.3749\tMAE: 0.1309 / 0.1780\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.3483 / 0.4040\tMAE: 0.1239 / 0.1946\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.3468 / 0.3758\tMAE: 0.1227 / 0.1718\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.3461 / 0.3766\tMAE: 0.1202 / 0.1731\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.3748 / 0.3687\tMAE: 0.1331 / 0.1741\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.3413 / 0.3733\tMAE: 0.1180 / 0.1752\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.3360 / 0.4332\tMAE: 0.1148 / 0.2144\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.3723 / 0.4303\tMAE: 0.1287 / 0.1950\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.3814 / 0.3912\tMAE: 0.1350 / 0.1830\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.3552 / 0.3764\tMAE: 0.1246 / 0.1750\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.3467 / 0.3756\tMAE: 0.1226 / 0.1710\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.3245 / 0.3715\tMAE: 0.1081 / 0.1716\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.3365 / 0.4095\tMAE: 0.1170 / 0.1989\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.3484 / 0.3891\tMAE: 0.1259 / 0.1767\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.3253 / 0.3701\tMAE: 0.1094 / 0.1677\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.3436 / 0.3879\tMAE: 0.1191 / 0.1823\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.3908 / 0.4070\tMAE: 0.1421 / 0.1942\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.3450 / 0.3779\tMAE: 0.1191 / 0.1764\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.3297 / 0.3692\tMAE: 0.1145 / 0.1721\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.3524 / 0.3734\tMAE: 0.1280 / 0.1740\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.3403 / 0.3764\tMAE: 0.1173 / 0.1717\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.3449 / 0.3713\tMAE: 0.1228 / 0.1688\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.3386 / 0.4303\tMAE: 0.1205 / 0.2020\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.3282 / 0.3684\tMAE: 0.1103 / 0.1684\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.3165 / 0.3665\tMAE: 0.1049 / 0.1717\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.3113 / 0.3760\tMAE: 0.1055 / 0.1711\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.3015 / 0.4135\tMAE: 0.0977 / 0.2059\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.3235 / 0.3688\tMAE: 0.1084 / 0.1741\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.2992 / 0.3785\tMAE: 0.0940 / 0.1722\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.3109 / 0.3705\tMAE: 0.1029 / 0.1723\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.3179 / 0.4458\tMAE: 0.1050 / 0.2277\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.3337 / 0.3762\tMAE: 0.1128 / 0.1763\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.3436 / 0.4274\tMAE: 0.1203 / 0.2106\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.3316 / 0.4043\tMAE: 0.1112 / 0.1936\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.3201 / 0.4065\tMAE: 0.1089 / 0.1980\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.3002 / 0.3734\tMAE: 0.0962 / 0.1709\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.2917 / 0.3731\tMAE: 0.0893 / 0.1675\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.3028 / 0.3684\tMAE: 0.0986 / 0.1711\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.3286 / 0.3633\tMAE: 0.1132 / 0.1698\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.2975 / 0.3823\tMAE: 0.0962 / 0.1828\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.2923 / 0.4045\tMAE: 0.0925 / 0.1882\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.3165 / 0.4023\tMAE: 0.0993 / 0.1956\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.3211 / 0.4000\tMAE: 0.1036 / 0.1883\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.3460 / 0.3888\tMAE: 0.1215 / 0.1793\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.2996 / 0.3520\tMAE: 0.0945 / 0.1652\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.3043 / 0.3745\tMAE: 0.0941 / 0.1713\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.3134 / 0.3938\tMAE: 0.1046 / 0.1839\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.3096 / 0.3723\tMAE: 0.1008 / 0.1734\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.2889 / 0.3711\tMAE: 0.0907 / 0.1710\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.2866 / 0.3924\tMAE: 0.0911 / 0.1910\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.3087 / 0.3927\tMAE: 0.1010 / 0.1962\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.3024 / 0.3867\tMAE: 0.0993 / 0.1814\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2825 / 0.3491\tMAE: 0.0845 / 0.1643\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.2795 / 0.3590\tMAE: 0.0828 / 0.1711\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.3283 / 0.4099\tMAE: 0.1139 / 0.1963\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.3155 / 0.4202\tMAE: 0.1068 / 0.2089\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.2876 / 0.3601\tMAE: 0.0891 / 0.1678\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.2862 / 0.4250\tMAE: 0.0861 / 0.2142\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2940 / 0.3635\tMAE: 0.0924 / 0.1726\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.2911 / 0.3750\tMAE: 0.0903 / 0.1717\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.3316 / 0.4670\tMAE: 0.1215 / 0.2521\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.3216 / 0.3886\tMAE: 0.1150 / 0.1873\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.2816 / 0.3635\tMAE: 0.0854 / 0.1714\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.2692 / 0.4221\tMAE: 0.0798 / 0.1993\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.2860 / 0.3916\tMAE: 0.0856 / 0.1931\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.2835 / 0.3571\tMAE: 0.0855 / 0.1704\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2703 / 0.3567\tMAE: 0.0760 / 0.1652\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.3714 / 0.4282\tMAE: 0.1254 / 0.1948\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.3691 / 0.3650\tMAE: 0.1218 / 0.1720\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.3677 / 0.4137\tMAE: 0.1255 / 0.1890\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.3473 / 0.3862\tMAE: 0.1223 / 0.1891\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.3529 / 0.4091\tMAE: 0.1255 / 0.1837\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.3393 / 0.4397\tMAE: 0.1140 / 0.2049\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.4408 / 0.4885\tMAE: 0.1556 / 0.2341\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.3897 / 0.3994\tMAE: 0.1341 / 0.1771\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.3724 / 0.4092\tMAE: 0.1242 / 0.1920\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.3688 / 0.3906\tMAE: 0.1252 / 0.1746\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.3469 / 0.3711\tMAE: 0.1196 / 0.1727\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.3149 / 0.3956\tMAE: 0.1028 / 0.1923\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.2920 / 0.3554\tMAE: 0.0905 / 0.1669\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.2858 / 0.3812\tMAE: 0.0864 / 0.1818\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.2803 / 0.3611\tMAE: 0.0838 / 0.1678\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.3032 / 0.4357\tMAE: 0.1022 / 0.2208\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.3168 / 0.3770\tMAE: 0.1087 / 0.1754\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.2719 / 0.3585\tMAE: 0.0776 / 0.1664\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.2985 / 0.3929\tMAE: 0.0923 / 0.1839\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.2875 / 0.3764\tMAE: 0.0884 / 0.1709\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.2768 / 0.3782\tMAE: 0.0823 / 0.1740\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.2681 / 0.3704\tMAE: 0.0779 / 0.1679\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.2983 / 0.3627\tMAE: 0.0993 / 0.1695\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.2782 / 0.4410\tMAE: 0.0852 / 0.2175\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.3475 / 0.3652\tMAE: 0.1367 / 0.1681\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.2655 / 0.3640\tMAE: 0.0751 / 0.1681\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.2641 / 0.3639\tMAE: 0.0762 / 0.1710\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.3540 / 0.4765\tMAE: 0.1209 / 0.2333\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.3212 / 0.3828\tMAE: 0.1000 / 0.1836\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.3214 / 0.3761\tMAE: 0.1105 / 0.1700\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.2778 / 0.3778\tMAE: 0.0821 / 0.1718\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.2763 / 0.4497\tMAE: 0.0803 / 0.2203\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.2713 / 0.3586\tMAE: 0.0785 / 0.1691\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.3014 / 0.3739\tMAE: 0.1009 / 0.1695\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.2950 / 0.4104\tMAE: 0.0932 / 0.1951\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.2889 / 0.3910\tMAE: 0.0917 / 0.1921\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.2639 / 0.3837\tMAE: 0.0736 / 0.1745\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.2677 / 0.3694\tMAE: 0.0760 / 0.1662\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.2873 / 0.3820\tMAE: 0.0911 / 0.1858\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.2893 / 0.3678\tMAE: 0.0925 / 0.1674\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.2589 / 0.3567\tMAE: 0.0674 / 0.1659\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.2461 / 0.3675\tMAE: 0.0608 / 0.1691\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.2610 / 0.4007\tMAE: 0.0723 / 0.1931\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.2845 / 0.3888\tMAE: 0.0877 / 0.1939\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.2731 / 0.4085\tMAE: 0.0829 / 0.1932\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.2800 / 0.3633\tMAE: 0.0878 / 0.1763\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.2856 / 0.3797\tMAE: 0.0923 / 0.1765\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.2521 / 0.3967\tMAE: 0.0672 / 0.1950\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.2700 / 0.3851\tMAE: 0.0795 / 0.1779\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.2594 / 0.3662\tMAE: 0.0732 / 0.1699\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.2579 / 0.4059\tMAE: 0.0726 / 0.1850\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.2558 / 0.3646\tMAE: 0.0686 / 0.1703\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.2754 / 0.4258\tMAE: 0.0842 / 0.2181\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.2681 / 0.3892\tMAE: 0.0759 / 0.1869\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.3006 / 0.3573\tMAE: 0.1038 / 0.1655\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.2637 / 0.3980\tMAE: 0.0755 / 0.1834\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.2593 / 0.3852\tMAE: 0.0724 / 0.1771\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.4968 / 0.9284\tMAE: 0.1923 / 0.5153\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.5829 / 0.4418\tMAE: 0.2206 / 0.2041\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.5691 / 0.6409\tMAE: 0.2049 / 0.3075\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.5632 / 0.4233\tMAE: 0.1962 / 0.2038\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.4686 / 0.4382\tMAE: 0.1624 / 0.2088\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.4473 / 0.4311\tMAE: 0.1585 / 0.1849\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.4236 / 0.4399\tMAE: 0.1427 / 0.1932\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.3924 / 0.4223\tMAE: 0.1346 / 0.1875\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.3825 / 0.3765\tMAE: 0.1305 / 0.1746\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.3569 / 0.3869\tMAE: 0.1199 / 0.1770\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.3440 / 0.4423\tMAE: 0.1168 / 0.2180\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.3600 / 0.3955\tMAE: 0.1277 / 0.1948\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.4100 / 0.4787\tMAE: 0.1478 / 0.2341\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.3856 / 0.3959\tMAE: 0.1342 / 0.1762\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.3257 / 0.3710\tMAE: 0.1054 / 0.1744\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.3104 / 0.3672\tMAE: 0.1005 / 0.1710\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.3410 / 0.3843\tMAE: 0.1262 / 0.1893\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.3161 / 0.3663\tMAE: 0.1097 / 0.1712\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.2921 / 0.4009\tMAE: 0.0903 / 0.1826\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.2835 / 0.3691\tMAE: 0.0862 / 0.1722\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.2887 / 0.3733\tMAE: 0.0909 / 0.1764\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.3031 / 0.3666\tMAE: 0.0959 / 0.1765\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.2969 / 0.3666\tMAE: 0.0943 / 0.1718\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.2836 / 0.3751\tMAE: 0.0873 / 0.1668\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.2866 / 0.3622\tMAE: 0.0882 / 0.1760\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.2610 / 0.3729\tMAE: 0.0722 / 0.1683\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.2687 / 0.3626\tMAE: 0.0770 / 0.1673\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.2594 / 0.3729\tMAE: 0.0710 / 0.1701\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.2524 / 0.3629\tMAE: 0.0673 / 0.1706\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.2487 / 0.3700\tMAE: 0.0651 / 0.1736\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.2597 / 0.3703\tMAE: 0.0725 / 0.1754\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.2787 / 0.3712\tMAE: 0.0868 / 0.1744\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.2802 / 0.3587\tMAE: 0.0869 / 0.1665\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.2521 / 0.3675\tMAE: 0.0670 / 0.1678\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.2609 / 0.3700\tMAE: 0.0712 / 0.1747\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.2765 / 0.3783\tMAE: 0.0827 / 0.1727\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.2811 / 0.3549\tMAE: 0.0882 / 0.1658\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.2588 / 0.4420\tMAE: 0.0716 / 0.2192\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.2530 / 0.3608\tMAE: 0.0695 / 0.1670\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.2717 / 0.3692\tMAE: 0.0813 / 0.1768\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.2499 / 0.3659\tMAE: 0.0659 / 0.1713\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.2473 / 0.3779\tMAE: 0.0639 / 0.1810\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.2455 / 0.4067\tMAE: 0.0636 / 0.1858\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.2512 / 0.3716\tMAE: 0.0658 / 0.1764\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.2561 / 0.3676\tMAE: 0.0714 / 0.1698\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.2688 / 0.3624\tMAE: 0.0726 / 0.1774\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.2639 / 0.3703\tMAE: 0.0737 / 0.1708\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.2548 / 0.3859\tMAE: 0.0706 / 0.1811\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.2648 / 0.3815\tMAE: 0.0771 / 0.1836\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.2532 / 0.3623\tMAE: 0.0700 / 0.1651\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.2589 / 0.3623\tMAE: 0.0717 / 0.1672\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.2384 / 0.3703\tMAE: 0.0587 / 0.1705\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.2533 / 0.3587\tMAE: 0.0690 / 0.1645\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.2509 / 0.3701\tMAE: 0.0668 / 0.1718\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.2663 / 0.3950\tMAE: 0.0786 / 0.1880\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.2357 / 0.3702\tMAE: 0.0565 / 0.1705\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.2474 / 0.4025\tMAE: 0.0643 / 0.1959\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.2329 / 0.3616\tMAE: 0.0579 / 0.1684\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.2541 / 0.4101\tMAE: 0.0694 / 0.2013\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.3090 / 0.3975\tMAE: 0.1099 / 0.1947\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.2383 / 0.3719\tMAE: 0.0599 / 0.1704\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.2474 / 0.3584\tMAE: 0.0644 / 0.1643\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.2384 / 0.3594\tMAE: 0.0579 / 0.1659\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.2364 / 0.3632\tMAE: 0.0551 / 0.1652\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.2340 / 0.3947\tMAE: 0.0572 / 0.1919\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.2490 / 0.3642\tMAE: 0.0677 / 0.1682\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.2635 / 0.3700\tMAE: 0.0752 / 0.1678\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.2450 / 0.3760\tMAE: 0.0626 / 0.1751\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.2396 / 0.3585\tMAE: 0.0598 / 0.1651\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.2558 / 0.3811\tMAE: 0.0716 / 0.1835\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.2568 / 0.3685\tMAE: 0.0714 / 0.1730\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.2673 / 0.3810\tMAE: 0.0804 / 0.1717\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.2590 / 0.3912\tMAE: 0.0693 / 0.1824\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.2473 / 0.3920\tMAE: 0.0652 / 0.1867\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.2361 / 0.3635\tMAE: 0.0580 / 0.1652\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.2221 / 0.3691\tMAE: 0.0483 / 0.1638\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.2377 / 0.3774\tMAE: 0.0586 / 0.1729\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.2332 / 0.3812\tMAE: 0.0587 / 0.1819\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.2482 / 0.3693\tMAE: 0.0674 / 0.1672\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.2323 / 0.4021\tMAE: 0.0559 / 0.1961\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.2323 / 0.3622\tMAE: 0.0550 / 0.1656\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.2249 / 0.3699\tMAE: 0.0504 / 0.1720\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.2178 / 0.3700\tMAE: 0.0469 / 0.1637\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.2343 / 0.3658\tMAE: 0.0558 / 0.1734\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.2390 / 0.3603\tMAE: 0.0584 / 0.1687\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.2404 / 0.3699\tMAE: 0.0606 / 0.1743\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.2336 / 0.3651\tMAE: 0.0586 / 0.1715\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.2632 / 0.3902\tMAE: 0.0811 / 0.1860\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.2825 / 0.3621\tMAE: 0.0920 / 0.1678\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.2231 / 0.3559\tMAE: 0.0499 / 0.1633\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.2399 / 0.3835\tMAE: 0.0619 / 0.1780\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.2651 / 0.3657\tMAE: 0.0802 / 0.1676\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.2404 / 0.3966\tMAE: 0.0607 / 0.1842\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.2498 / 0.3953\tMAE: 0.0654 / 0.1943\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.2357 / 0.3703\tMAE: 0.0580 / 0.1748\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.2549 / 0.3995\tMAE: 0.0707 / 0.1890\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.2677 / 0.4331\tMAE: 0.0821 / 0.2223\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.2409 / 0.3609\tMAE: 0.0607 / 0.1677\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.2331 / 0.3749\tMAE: 0.0570 / 0.1754\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.2311 / 0.3716\tMAE: 0.0564 / 0.1733\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.2225 / 0.3552\tMAE: 0.0482 / 0.1621\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.2216 / 0.3800\tMAE: 0.0487 / 0.1765\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.2281 / 0.3591\tMAE: 0.0528 / 0.1643\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.2186 / 0.3727\tMAE: 0.0475 / 0.1756\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.2253 / 0.3621\tMAE: 0.0512 / 0.1670\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.2755 / 0.3853\tMAE: 0.0773 / 0.1762\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.2566 / 0.3740\tMAE: 0.0663 / 0.1777\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.2746 / 0.3814\tMAE: 0.0704 / 0.1776\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.2710 / 0.3624\tMAE: 0.0728 / 0.1718\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.2568 / 0.3666\tMAE: 0.0677 / 0.1797\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.2538 / 0.3598\tMAE: 0.0668 / 0.1698\n",
      "40% of metal data used - 3285 metal with 10387 insulator (24%)\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TTT_07_M2_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 4.3198 / 3.3878\tMAE: 1.8802 / 1.6158\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.9076 / 1.9284\tMAE: 1.1386 / 0.8406\n",
      "Epoch [3/300]\tTrain/Valid Loss: 1.7735 / 1.2226\tMAE: 0.7245 / 0.5993\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.2919 / 0.8623\tMAE: 0.4757 / 0.3619\n",
      "Epoch [5/300]\tTrain/Valid Loss: 0.9546 / 0.6518\tMAE: 0.2943 / 0.2514\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.8950 / 0.7964\tMAE: 0.2696 / 0.2839\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.8261 / 0.5710\tMAE: 0.2470 / 0.2523\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.7901 / 0.5798\tMAE: 0.2369 / 0.2287\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.7578 / 0.5380\tMAE: 0.2224 / 0.2184\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.7456 / 0.5087\tMAE: 0.2186 / 0.2263\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.7201 / 0.5373\tMAE: 0.2138 / 0.2135\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.7283 / 0.5334\tMAE: 0.2124 / 0.2418\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.7139 / 0.4775\tMAE: 0.2198 / 0.2076\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.6703 / 0.5054\tMAE: 0.1921 / 0.2221\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.6895 / 0.4468\tMAE: 0.1998 / 0.1933\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.6642 / 0.4173\tMAE: 0.1879 / 0.1938\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.6400 / 0.4429\tMAE: 0.1828 / 0.1865\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.6514 / 0.4146\tMAE: 0.1907 / 0.1845\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.5993 / 0.4726\tMAE: 0.1715 / 0.2077\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.6754 / 0.4496\tMAE: 0.2026 / 0.2064\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.6253 / 0.4088\tMAE: 0.1793 / 0.1732\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.6170 / 0.4064\tMAE: 0.1739 / 0.1841\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.6608 / 0.6344\tMAE: 0.1876 / 0.3152\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.6957 / 0.4530\tMAE: 0.2061 / 0.2148\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.6353 / 0.4129\tMAE: 0.1775 / 0.2010\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.6614 / 0.4197\tMAE: 0.1953 / 0.1919\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.6201 / 0.3664\tMAE: 0.1713 / 0.1694\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.5921 / 0.3790\tMAE: 0.1634 / 0.1811\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.5789 / 0.3851\tMAE: 0.1598 / 0.1837\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.6026 / 0.4027\tMAE: 0.1671 / 0.1802\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.6071 / 0.4460\tMAE: 0.1660 / 0.2175\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.6139 / 0.3732\tMAE: 0.1766 / 0.1777\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.5848 / 0.3433\tMAE: 0.1668 / 0.1642\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.5596 / 0.4047\tMAE: 0.1545 / 0.1926\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.5830 / 0.3746\tMAE: 0.1622 / 0.1727\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.5487 / 0.3539\tMAE: 0.1494 / 0.1602\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.5474 / 0.3947\tMAE: 0.1455 / 0.1801\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.5421 / 0.3469\tMAE: 0.1463 / 0.1638\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.5481 / 0.5033\tMAE: 0.1502 / 0.2652\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.5845 / 0.4192\tMAE: 0.1657 / 0.2071\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.5339 / 0.3507\tMAE: 0.1486 / 0.1578\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.5155 / 0.3486\tMAE: 0.1429 / 0.1613\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.5097 / 0.3671\tMAE: 0.1400 / 0.1638\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.4893 / 0.3256\tMAE: 0.1314 / 0.1506\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.5280 / 0.4730\tMAE: 0.1420 / 0.1942\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.6331 / 0.3432\tMAE: 0.1761 / 0.1591\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.5456 / 0.3399\tMAE: 0.1481 / 0.1594\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.5320 / 0.3572\tMAE: 0.1414 / 0.1599\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.5315 / 0.3807\tMAE: 0.1438 / 0.1602\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.5197 / 0.3512\tMAE: 0.1439 / 0.1637\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.4972 / 0.3285\tMAE: 0.1319 / 0.1517\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.5124 / 0.3490\tMAE: 0.1360 / 0.1598\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.4899 / 0.3473\tMAE: 0.1252 / 0.1521\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.4857 / 0.3217\tMAE: 0.1286 / 0.1528\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.4816 / 0.3593\tMAE: 0.1214 / 0.1564\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.4939 / 0.3585\tMAE: 0.1304 / 0.1596\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.5246 / 0.3528\tMAE: 0.1374 / 0.1543\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.5039 / 0.3415\tMAE: 0.1327 / 0.1584\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.4898 / 0.3356\tMAE: 0.1298 / 0.1511\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.4735 / 0.3444\tMAE: 0.1246 / 0.1483\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.4622 / 0.3233\tMAE: 0.1207 / 0.1502\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.5083 / 0.3369\tMAE: 0.1366 / 0.1535\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.5122 / 0.3604\tMAE: 0.1333 / 0.1577\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.6547 / 0.4141\tMAE: 0.1643 / 0.1940\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.5821 / 0.3935\tMAE: 0.1593 / 0.1693\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.5406 / 0.3908\tMAE: 0.1458 / 0.1691\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.5547 / 0.3562\tMAE: 0.1392 / 0.1588\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.5175 / 0.3579\tMAE: 0.1319 / 0.1601\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.5016 / 0.3791\tMAE: 0.1288 / 0.1563\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.4746 / 0.3317\tMAE: 0.1204 / 0.1540\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.4930 / 0.4465\tMAE: 0.1298 / 0.1754\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.4915 / 0.3230\tMAE: 0.1264 / 0.1454\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.4629 / 0.3539\tMAE: 0.1173 / 0.1669\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.4624 / 0.3500\tMAE: 0.1170 / 0.1644\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.4622 / 0.5042\tMAE: 0.1169 / 0.2537\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.4597 / 0.3236\tMAE: 0.1203 / 0.1488\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.4448 / 0.3208\tMAE: 0.1112 / 0.1486\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.4499 / 0.3362\tMAE: 0.1103 / 0.1515\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.4576 / 0.3645\tMAE: 0.1133 / 0.1625\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.4495 / 0.3449\tMAE: 0.1109 / 0.1599\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.4629 / 0.3437\tMAE: 0.1220 / 0.1523\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.4353 / 0.3287\tMAE: 0.1113 / 0.1504\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.4410 / 0.3433\tMAE: 0.1037 / 0.1627\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.4299 / 0.3261\tMAE: 0.0992 / 0.1425\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.4267 / 0.3429\tMAE: 0.0998 / 0.1652\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.4361 / 0.3321\tMAE: 0.1107 / 0.1514\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.4253 / 0.3251\tMAE: 0.1007 / 0.1452\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.4277 / 0.3159\tMAE: 0.1011 / 0.1433\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.4355 / 0.3395\tMAE: 0.1048 / 0.1509\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.4288 / 0.3385\tMAE: 0.1036 / 0.1547\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.4151 / 0.3372\tMAE: 0.0949 / 0.1543\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.4020 / 0.3178\tMAE: 0.0907 / 0.1404\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.4079 / 0.3280\tMAE: 0.0917 / 0.1531\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.4127 / 0.3231\tMAE: 0.0909 / 0.1475\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.4082 / 0.3235\tMAE: 0.0917 / 0.1501\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.4145 / 0.3335\tMAE: 0.0950 / 0.1469\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.4111 / 0.3443\tMAE: 0.0934 / 0.1443\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.4135 / 0.3261\tMAE: 0.0957 / 0.1501\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.4155 / 0.3521\tMAE: 0.0951 / 0.1712\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.4045 / 0.3326\tMAE: 0.0918 / 0.1462\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.3942 / 0.3429\tMAE: 0.0858 / 0.1570\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.4236 / 0.5020\tMAE: 0.0989 / 0.2103\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.5535 / 0.3625\tMAE: 0.1431 / 0.1659\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.4909 / 0.3634\tMAE: 0.1343 / 0.1513\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.4479 / 0.3588\tMAE: 0.1094 / 0.1552\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.4407 / 0.3603\tMAE: 0.1010 / 0.1634\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.4142 / 0.3605\tMAE: 0.0944 / 0.1709\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.4249 / 0.3357\tMAE: 0.1011 / 0.1496\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.4291 / 0.3309\tMAE: 0.1013 / 0.1455\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.4171 / 0.3422\tMAE: 0.0947 / 0.1480\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.4201 / 0.3200\tMAE: 0.0977 / 0.1461\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.4261 / 0.3238\tMAE: 0.0957 / 0.1482\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.4929 / 0.3465\tMAE: 0.1236 / 0.1534\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.4552 / 0.3201\tMAE: 0.1068 / 0.1476\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.4318 / 0.3205\tMAE: 0.1010 / 0.1462\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.4121 / 0.3788\tMAE: 0.0940 / 0.1574\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.5053 / 0.3396\tMAE: 0.1308 / 0.1631\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.4385 / 0.3247\tMAE: 0.1017 / 0.1463\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.4155 / 0.3514\tMAE: 0.0951 / 0.1595\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.4050 / 0.3306\tMAE: 0.0874 / 0.1516\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.4079 / 0.3356\tMAE: 0.0901 / 0.1509\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.4085 / 0.3332\tMAE: 0.0916 / 0.1594\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.4100 / 0.3293\tMAE: 0.0908 / 0.1469\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.4002 / 0.3197\tMAE: 0.0830 / 0.1508\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.3907 / 0.3368\tMAE: 0.0839 / 0.1557\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.3936 / 0.3852\tMAE: 0.0860 / 0.1940\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.3967 / 0.3578\tMAE: 0.0874 / 0.1706\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.3911 / 0.3334\tMAE: 0.0808 / 0.1524\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.4170 / 0.3247\tMAE: 0.0973 / 0.1442\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.3976 / 0.3440\tMAE: 0.0804 / 0.1576\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.3957 / 0.3289\tMAE: 0.0849 / 0.1517\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.3985 / 0.3364\tMAE: 0.0850 / 0.1557\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.4107 / 0.3618\tMAE: 0.0962 / 0.1788\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.3897 / 0.3144\tMAE: 0.0821 / 0.1422\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.3754 / 0.3120\tMAE: 0.0735 / 0.1407\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.3740 / 0.3882\tMAE: 0.0758 / 0.1983\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.4001 / 0.3193\tMAE: 0.0917 / 0.1426\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.3671 / 0.3331\tMAE: 0.0681 / 0.1563\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.3640 / 0.3242\tMAE: 0.0671 / 0.1422\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.3707 / 0.3218\tMAE: 0.0736 / 0.1413\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.3651 / 0.3249\tMAE: 0.0687 / 0.1479\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.3604 / 0.3415\tMAE: 0.0665 / 0.1550\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.4147 / 0.3674\tMAE: 0.1048 / 0.1813\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.4266 / 0.3427\tMAE: 0.1016 / 0.1649\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.3905 / 0.3451\tMAE: 0.0795 / 0.1539\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.3930 / 0.3461\tMAE: 0.0847 / 0.1645\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.3920 / 0.3234\tMAE: 0.0805 / 0.1443\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.3831 / 0.3463\tMAE: 0.0786 / 0.1673\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.3864 / 0.3236\tMAE: 0.0819 / 0.1441\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.3756 / 0.3302\tMAE: 0.0738 / 0.1501\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.3865 / 0.3551\tMAE: 0.0772 / 0.1657\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.3774 / 0.3270\tMAE: 0.0783 / 0.1507\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.3649 / 0.3323\tMAE: 0.0666 / 0.1420\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.3704 / 0.3291\tMAE: 0.0676 / 0.1471\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.3612 / 0.3325\tMAE: 0.0662 / 0.1476\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.3697 / 0.3281\tMAE: 0.0694 / 0.1416\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.3657 / 0.3227\tMAE: 0.0665 / 0.1415\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.3734 / 0.3157\tMAE: 0.0701 / 0.1442\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.3675 / 0.3558\tMAE: 0.0661 / 0.1594\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.3718 / 0.3354\tMAE: 0.0713 / 0.1558\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.3904 / 0.3512\tMAE: 0.0861 / 0.1654\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.3995 / 0.3667\tMAE: 0.0949 / 0.1697\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.4040 / 0.3665\tMAE: 0.0901 / 0.1662\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.3968 / 0.3436\tMAE: 0.0796 / 0.1538\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.3786 / 0.3616\tMAE: 0.0731 / 0.1533\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.3689 / 0.3154\tMAE: 0.0685 / 0.1430\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.3555 / 0.3268\tMAE: 0.0603 / 0.1489\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.3613 / 0.3354\tMAE: 0.0639 / 0.1570\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.3664 / 0.3310\tMAE: 0.0719 / 0.1451\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.3906 / 0.3920\tMAE: 0.0710 / 0.1697\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.3963 / 0.3793\tMAE: 0.0788 / 0.1787\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.4191 / 0.3383\tMAE: 0.0946 / 0.1478\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.4655 / 0.4117\tMAE: 0.1101 / 0.1951\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.4194 / 0.3391\tMAE: 0.0932 / 0.1520\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.3899 / 0.3408\tMAE: 0.0735 / 0.1454\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.3821 / 0.3682\tMAE: 0.0794 / 0.1743\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.3864 / 0.3510\tMAE: 0.0790 / 0.1639\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.4599 / 0.3592\tMAE: 0.1046 / 0.1658\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.4051 / 0.3362\tMAE: 0.0891 / 0.1478\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.3778 / 0.3262\tMAE: 0.0712 / 0.1453\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.3769 / 0.3301\tMAE: 0.0734 / 0.1438\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.3690 / 0.3276\tMAE: 0.0664 / 0.1477\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.3832 / 0.3354\tMAE: 0.0802 / 0.1479\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.3505 / 0.3338\tMAE: 0.0597 / 0.1469\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.3630 / 0.3300\tMAE: 0.0670 / 0.1464\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.3632 / 0.3456\tMAE: 0.0671 / 0.1539\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.3574 / 0.3294\tMAE: 0.0591 / 0.1468\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.3456 / 0.3465\tMAE: 0.0599 / 0.1562\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.3643 / 0.3777\tMAE: 0.0687 / 0.1856\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.3630 / 0.3223\tMAE: 0.0670 / 0.1421\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.3495 / 0.3973\tMAE: 0.0561 / 0.2002\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.3629 / 0.3469\tMAE: 0.0688 / 0.1499\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.3961 / 0.3439\tMAE: 0.0858 / 0.1591\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.3627 / 0.3356\tMAE: 0.0640 / 0.1467\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.3516 / 0.3274\tMAE: 0.0592 / 0.1474\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.3475 / 0.3342\tMAE: 0.0576 / 0.1491\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.3475 / 0.3589\tMAE: 0.0580 / 0.1656\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.3650 / 0.3283\tMAE: 0.0658 / 0.1479\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.3533 / 0.3228\tMAE: 0.0605 / 0.1441\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.3847 / 0.3261\tMAE: 0.0858 / 0.1442\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.3603 / 0.3212\tMAE: 0.0652 / 0.1434\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.3494 / 0.3294\tMAE: 0.0605 / 0.1487\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.3486 / 0.3396\tMAE: 0.0567 / 0.1546\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.3508 / 0.3258\tMAE: 0.0605 / 0.1469\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.3369 / 0.3506\tMAE: 0.0522 / 0.1554\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.3703 / 0.3362\tMAE: 0.0728 / 0.1471\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.3633 / 0.3823\tMAE: 0.0675 / 0.1592\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.3689 / 0.3428\tMAE: 0.0672 / 0.1550\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.3564 / 0.3344\tMAE: 0.0634 / 0.1551\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.3627 / 0.3334\tMAE: 0.0675 / 0.1518\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.3503 / 0.3205\tMAE: 0.0563 / 0.1491\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.3472 / 0.3322\tMAE: 0.0568 / 0.1465\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.3439 / 0.3313\tMAE: 0.0590 / 0.1514\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.3518 / 0.3215\tMAE: 0.0596 / 0.1474\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.3394 / 0.3370\tMAE: 0.0535 / 0.1569\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.3358 / 0.3464\tMAE: 0.0503 / 0.1675\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.3370 / 0.3463\tMAE: 0.0493 / 0.1670\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.3469 / 0.3338\tMAE: 0.0608 / 0.1576\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.3446 / 0.3277\tMAE: 0.0604 / 0.1468\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.3367 / 0.3337\tMAE: 0.0531 / 0.1536\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.3679 / 0.3154\tMAE: 0.0739 / 0.1425\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.3306 / 0.3350\tMAE: 0.0498 / 0.1536\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.3700 / 0.3213\tMAE: 0.0693 / 0.1475\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.3444 / 0.3377\tMAE: 0.0554 / 0.1511\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.3721 / 0.3594\tMAE: 0.0762 / 0.1756\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.3363 / 0.3177\tMAE: 0.0539 / 0.1422\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.3354 / 0.3220\tMAE: 0.0486 / 0.1459\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.3460 / 0.3374\tMAE: 0.0585 / 0.1446\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.3438 / 0.3293\tMAE: 0.0557 / 0.1499\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.3310 / 0.3202\tMAE: 0.0500 / 0.1412\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.3491 / 0.3421\tMAE: 0.0638 / 0.1525\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.3431 / 0.3253\tMAE: 0.0536 / 0.1446\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.3351 / 0.3332\tMAE: 0.0477 / 0.1458\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.3381 / 0.3222\tMAE: 0.0544 / 0.1433\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.3310 / 0.3316\tMAE: 0.0497 / 0.1456\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.3340 / 0.3471\tMAE: 0.0487 / 0.1635\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.3366 / 0.3585\tMAE: 0.0512 / 0.1703\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.3417 / 0.3959\tMAE: 0.0604 / 0.1918\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.3421 / 0.3498\tMAE: 0.0554 / 0.1649\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.3445 / 0.3239\tMAE: 0.0560 / 0.1470\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.3291 / 0.3559\tMAE: 0.0495 / 0.1647\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.3367 / 0.3599\tMAE: 0.0525 / 0.1700\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.3384 / 0.3277\tMAE: 0.0539 / 0.1484\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.3323 / 0.3716\tMAE: 0.0481 / 0.1837\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.3510 / 0.3206\tMAE: 0.0633 / 0.1432\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.3414 / 0.3288\tMAE: 0.0565 / 0.1499\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.3259 / 0.3682\tMAE: 0.0463 / 0.1713\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.3497 / 0.3183\tMAE: 0.0621 / 0.1448\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.3299 / 0.3341\tMAE: 0.0441 / 0.1540\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.3612 / 0.3235\tMAE: 0.0700 / 0.1451\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.3361 / 0.3227\tMAE: 0.0476 / 0.1490\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.3316 / 0.3255\tMAE: 0.0479 / 0.1433\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.3339 / 0.3384\tMAE: 0.0496 / 0.1432\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.3410 / 0.3347\tMAE: 0.0561 / 0.1432\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.3243 / 0.3223\tMAE: 0.0465 / 0.1430\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.3289 / 0.3249\tMAE: 0.0477 / 0.1414\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.3260 / 0.3286\tMAE: 0.0458 / 0.1439\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.3301 / 0.3767\tMAE: 0.0481 / 0.1790\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.3380 / 0.3178\tMAE: 0.0520 / 0.1417\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.3242 / 0.3418\tMAE: 0.0432 / 0.1461\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.3654 / 0.4054\tMAE: 0.0686 / 0.2103\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.3713 / 0.3891\tMAE: 0.0772 / 0.1925\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.3635 / 0.3447\tMAE: 0.0701 / 0.1589\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.4908 / 0.3849\tMAE: 0.1207 / 0.1711\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.4300 / 0.3240\tMAE: 0.0959 / 0.1468\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.4178 / 0.3634\tMAE: 0.0880 / 0.1612\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.3906 / 0.4930\tMAE: 0.0776 / 0.1967\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.4429 / 0.4381\tMAE: 0.1025 / 0.2165\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.4254 / 0.3676\tMAE: 0.1023 / 0.1613\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.3796 / 0.3401\tMAE: 0.0770 / 0.1563\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.3669 / 0.3328\tMAE: 0.0681 / 0.1466\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.3481 / 0.3409\tMAE: 0.0581 / 0.1472\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.3523 / 0.3478\tMAE: 0.0602 / 0.1484\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.3471 / 0.3249\tMAE: 0.0585 / 0.1439\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.3393 / 0.3423\tMAE: 0.0505 / 0.1491\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.3437 / 0.3491\tMAE: 0.0565 / 0.1588\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.3541 / 0.3643\tMAE: 0.0606 / 0.1735\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.3694 / 0.3366\tMAE: 0.0730 / 0.1481\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.3426 / 0.3530\tMAE: 0.0527 / 0.1624\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.3793 / 0.3237\tMAE: 0.0807 / 0.1450\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.3500 / 0.3735\tMAE: 0.0544 / 0.1814\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.3405 / 0.3342\tMAE: 0.0552 / 0.1475\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.3272 / 0.3404\tMAE: 0.0452 / 0.1451\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.3223 / 0.3447\tMAE: 0.0407 / 0.1453\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.3249 / 0.3301\tMAE: 0.0450 / 0.1442\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.3420 / 0.3356\tMAE: 0.0507 / 0.1503\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.3316 / 0.3282\tMAE: 0.0499 / 0.1458\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.3524 / 0.3352\tMAE: 0.0623 / 0.1485\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.3416 / 0.3470\tMAE: 0.0549 / 0.1470\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.3361 / 0.3491\tMAE: 0.0506 / 0.1555\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.3791 / 0.3350\tMAE: 0.0803 / 0.1484\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.3391 / 0.3357\tMAE: 0.0566 / 0.1480\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.3504 / 0.3280\tMAE: 0.0613 / 0.1460\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.3225 / 0.3316\tMAE: 0.0430 / 0.1440\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.3381 / 0.3509\tMAE: 0.0544 / 0.1539\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.3280 / 0.3289\tMAE: 0.0475 / 0.1469\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.3234 / 0.3640\tMAE: 0.0435 / 0.1721\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.3261 / 0.3662\tMAE: 0.0488 / 0.1723\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.3330 / 0.3346\tMAE: 0.0517 / 0.1465\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.3276 / 0.3304\tMAE: 0.0476 / 0.1450\n",
      "60% of metal data used - 4983 metal with 10387 insulator (32%)\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TTT_08_M2_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 4.2052 / 3.2897\tMAE: 1.6509 / 1.2985\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.5703 / 1.7312\tMAE: 0.9159 / 0.7990\n",
      "Epoch [3/300]\tTrain/Valid Loss: 1.6272 / 1.7508\tMAE: 0.6004 / 0.8564\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.2423 / 0.7380\tMAE: 0.3790 / 0.2939\n",
      "Epoch [5/300]\tTrain/Valid Loss: 1.0162 / 0.6639\tMAE: 0.2656 / 0.2971\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.9467 / 0.5753\tMAE: 0.2656 / 0.2462\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.8860 / 0.5273\tMAE: 0.2311 / 0.2258\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.8598 / 0.5041\tMAE: 0.2264 / 0.2357\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.8382 / 0.6196\tMAE: 0.2229 / 0.2207\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.8147 / 0.4513\tMAE: 0.2012 / 0.1950\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.7580 / 0.4073\tMAE: 0.1907 / 0.1881\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.7649 / 0.4953\tMAE: 0.1913 / 0.2003\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.8203 / 0.3647\tMAE: 0.2080 / 0.1745\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.7154 / 0.4236\tMAE: 0.1737 / 0.2145\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.7420 / 0.3645\tMAE: 0.1704 / 0.1759\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.7740 / 0.4356\tMAE: 0.2055 / 0.1978\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.7085 / 0.3359\tMAE: 0.1698 / 0.1548\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.6788 / 0.3686\tMAE: 0.1607 / 0.1504\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.6829 / 0.4357\tMAE: 0.1576 / 0.1767\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.6929 / 0.3504\tMAE: 0.1580 / 0.1600\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.6713 / 0.3274\tMAE: 0.1522 / 0.1552\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.6987 / 0.4147\tMAE: 0.1702 / 0.2138\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.6820 / 0.3709\tMAE: 0.1635 / 0.1490\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.6655 / 0.3228\tMAE: 0.1499 / 0.1493\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.6825 / 0.3765\tMAE: 0.1525 / 0.1661\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.6436 / 0.3343\tMAE: 0.1445 / 0.1542\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.6582 / 0.3642\tMAE: 0.1488 / 0.1741\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.6666 / 0.4097\tMAE: 0.1552 / 0.2160\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.6391 / 0.4262\tMAE: 0.1517 / 0.1458\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.6489 / 0.3345\tMAE: 0.1413 / 0.1630\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.6372 / 0.3089\tMAE: 0.1410 / 0.1428\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.6133 / 0.4183\tMAE: 0.1374 / 0.1807\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.6169 / 0.3188\tMAE: 0.1480 / 0.1551\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.6204 / 0.2950\tMAE: 0.1450 / 0.1450\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.6061 / 0.3662\tMAE: 0.1326 / 0.1567\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.6271 / 0.3287\tMAE: 0.1385 / 0.1464\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.6127 / 0.5259\tMAE: 0.1446 / 0.1921\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.6127 / 0.3012\tMAE: 0.1393 / 0.1390\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.5856 / 2.7772\tMAE: 0.1229 / 0.9163\n",
      "Epoch [40/300]\tTrain/Valid Loss: 2.4442 / 2.2421\tMAE: 0.8280 / 0.9082\n",
      "Epoch [41/300]\tTrain/Valid Loss: 1.6324 / 0.8991\tMAE: 0.5079 / 0.3942\n",
      "Epoch [42/300]\tTrain/Valid Loss: 1.2576 / 1.5204\tMAE: 0.3513 / 0.3903\n",
      "Epoch [43/300]\tTrain/Valid Loss: 1.1459 / 0.7125\tMAE: 0.3411 / 0.3641\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.8946 / 0.4929\tMAE: 0.2612 / 0.2432\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.8921 / 0.5708\tMAE: 0.2696 / 0.2719\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.9148 / 0.6250\tMAE: 0.2595 / 0.3439\n",
      "Epoch [47/300]\tTrain/Valid Loss: 1.0262 / 0.4875\tMAE: 0.3032 / 0.2431\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.8161 / 0.4774\tMAE: 0.2254 / 0.2252\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.8392 / 0.3581\tMAE: 0.2231 / 0.1844\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.7970 / 0.4262\tMAE: 0.2098 / 0.2359\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.8020 / 0.4021\tMAE: 0.2300 / 0.2111\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.7744 / 0.4104\tMAE: 0.2021 / 0.2230\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.7521 / 0.3615\tMAE: 0.2028 / 0.1806\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.7295 / 0.5495\tMAE: 0.1835 / 0.2957\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.8103 / 0.3509\tMAE: 0.2120 / 0.1724\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.7565 / 0.3810\tMAE: 0.1876 / 0.2037\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.7284 / 0.3781\tMAE: 0.1792 / 0.1841\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.7594 / 0.3330\tMAE: 0.1864 / 0.1698\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.7295 / 0.4146\tMAE: 0.1714 / 0.2057\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.7358 / 0.3547\tMAE: 0.1892 / 0.1778\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.7213 / 0.3788\tMAE: 0.1674 / 0.1802\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.7125 / 0.3537\tMAE: 0.1642 / 0.1884\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.6861 / 0.4389\tMAE: 0.1662 / 0.2329\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.7159 / 0.3595\tMAE: 0.1786 / 0.1608\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.6927 / 0.3319\tMAE: 0.1653 / 0.1599\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.6648 / 0.3361\tMAE: 0.1520 / 0.1639\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.6629 / 0.3728\tMAE: 0.1463 / 0.1965\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.6950 / 0.3287\tMAE: 0.1600 / 0.1687\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.6597 / 0.3306\tMAE: 0.1548 / 0.1725\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.6579 / 0.3294\tMAE: 0.1533 / 0.1713\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.6465 / 0.3206\tMAE: 0.1475 / 0.1510\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.6825 / 0.3018\tMAE: 0.1560 / 0.1510\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.6311 / 0.2823\tMAE: 0.1391 / 0.1373\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.6460 / 0.2950\tMAE: 0.1421 / 0.1500\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.6392 / 0.4263\tMAE: 0.1403 / 0.1562\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.6614 / 0.2910\tMAE: 0.1518 / 0.1448\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.6452 / 0.3075\tMAE: 0.1397 / 0.1542\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.6411 / 0.2929\tMAE: 0.1435 / 0.1403\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.6161 / 0.3033\tMAE: 0.1398 / 0.1382\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.6483 / 0.3356\tMAE: 0.1465 / 0.1700\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.6471 / 0.3276\tMAE: 0.1414 / 0.1508\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.6533 / 0.3200\tMAE: 0.1445 / 0.1519\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.6213 / 0.3034\tMAE: 0.1351 / 0.1468\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.6175 / 0.2931\tMAE: 0.1281 / 0.1395\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.6235 / 0.3855\tMAE: 0.1385 / 0.2074\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.6442 / 0.2936\tMAE: 0.1390 / 0.1367\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.6112 / 0.2949\tMAE: 0.1275 / 0.1450\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.6035 / 0.3441\tMAE: 0.1312 / 0.1779\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.7216 / 0.2980\tMAE: 0.1674 / 0.1498\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.6358 / 0.3394\tMAE: 0.1364 / 0.1626\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.7090 / 0.4278\tMAE: 0.1620 / 0.1995\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.6621 / 0.3116\tMAE: 0.1421 / 0.1531\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.6220 / 0.3117\tMAE: 0.1443 / 0.1456\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.6277 / 0.3016\tMAE: 0.1440 / 0.1529\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.5917 / 0.3283\tMAE: 0.1290 / 0.1397\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.5997 / 0.3049\tMAE: 0.1246 / 0.1362\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.5852 / 0.3390\tMAE: 0.1226 / 0.1465\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.5949 / 0.3089\tMAE: 0.1231 / 0.1572\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.5885 / 0.3092\tMAE: 0.1296 / 0.1430\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.6241 / 0.3181\tMAE: 0.1361 / 0.1400\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.6109 / 0.4025\tMAE: 0.1311 / 0.1779\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.8818 / 0.5469\tMAE: 0.2037 / 0.2054\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.6740 / 0.3390\tMAE: 0.1591 / 0.1491\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.6355 / 0.2935\tMAE: 0.1418 / 0.1439\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.6138 / 0.3315\tMAE: 0.1340 / 0.1595\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.6124 / 0.3541\tMAE: 0.1367 / 0.1905\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.6129 / 0.2845\tMAE: 0.1273 / 0.1319\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.6106 / 0.2916\tMAE: 0.1307 / 0.1439\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.6138 / 0.2785\tMAE: 0.1322 / 0.1330\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.6058 / 0.2749\tMAE: 0.1310 / 0.1327\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.5757 / 0.3563\tMAE: 0.1203 / 0.1706\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.6920 / 0.5946\tMAE: 0.1625 / 0.2881\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.6742 / 0.4840\tMAE: 0.1455 / 0.2290\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.6595 / 0.3169\tMAE: 0.1512 / 0.1493\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.6503 / 0.3274\tMAE: 0.1358 / 0.1739\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.6003 / 0.3854\tMAE: 0.1252 / 0.2113\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.6191 / 0.3086\tMAE: 0.1509 / 0.1543\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.6089 / 0.2942\tMAE: 0.1248 / 0.1470\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.6206 / 0.3036\tMAE: 0.1326 / 0.1457\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.6306 / 0.2822\tMAE: 0.1484 / 0.1289\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.5884 / 0.2700\tMAE: 0.1193 / 0.1284\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.5757 / 0.2995\tMAE: 0.1204 / 0.1483\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.6077 / 0.2866\tMAE: 0.1310 / 0.1430\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.5957 / 0.3375\tMAE: 0.1371 / 0.1586\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.6028 / 0.2884\tMAE: 0.1171 / 0.1445\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.5556 / 0.2760\tMAE: 0.1123 / 0.1337\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.5545 / 0.3373\tMAE: 0.1132 / 0.1820\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.5638 / 0.2782\tMAE: 0.1266 / 0.1362\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.5723 / 0.2659\tMAE: 0.1293 / 0.1301\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.5857 / 0.3234\tMAE: 0.1217 / 0.1405\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.5865 / 0.2708\tMAE: 0.1246 / 0.1346\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.5511 / 0.3092\tMAE: 0.1102 / 0.1580\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.5583 / 0.3114\tMAE: 0.1098 / 0.1596\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.5680 / 0.2766\tMAE: 0.1203 / 0.1328\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.5454 / 0.3143\tMAE: 0.1055 / 0.1505\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.5516 / 0.2726\tMAE: 0.1101 / 0.1293\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.5467 / 0.2845\tMAE: 0.1095 / 0.1259\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.5471 / 0.2719\tMAE: 0.1062 / 0.1339\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.5557 / 0.2716\tMAE: 0.1182 / 0.1296\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.5496 / 0.3030\tMAE: 0.1044 / 0.1416\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.5430 / 0.2977\tMAE: 0.1066 / 0.1431\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.5265 / 0.2781\tMAE: 0.1008 / 0.1335\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.5536 / 0.2832\tMAE: 0.1103 / 0.1357\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.5236 / 0.2902\tMAE: 0.0983 / 0.1291\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.5214 / 0.3594\tMAE: 0.0993 / 0.1618\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.5484 / 0.2709\tMAE: 0.1086 / 0.1263\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.5240 / 0.3811\tMAE: 0.0949 / 0.1579\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.5539 / 0.2902\tMAE: 0.1068 / 0.1335\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.5404 / 0.3330\tMAE: 0.1041 / 0.1782\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.5355 / 0.3212\tMAE: 0.1045 / 0.1640\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.5869 / 0.3168\tMAE: 0.1175 / 0.1530\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.5709 / 0.3170\tMAE: 0.1159 / 0.1343\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.5797 / 0.3249\tMAE: 0.1105 / 0.1732\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.5807 / 0.3434\tMAE: 0.1264 / 0.1817\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.5790 / 0.3028\tMAE: 0.1242 / 0.1516\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.5525 / 0.2708\tMAE: 0.1036 / 0.1256\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.5304 / 0.3268\tMAE: 0.1033 / 0.1743\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.5573 / 0.2858\tMAE: 0.1269 / 0.1296\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.5718 / 0.3848\tMAE: 0.1094 / 0.2093\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.5401 / 0.2851\tMAE: 0.1026 / 0.1391\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.6472 / 0.2863\tMAE: 0.1317 / 0.1385\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.6464 / 0.2844\tMAE: 0.1399 / 0.1354\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.5975 / 0.2692\tMAE: 0.1202 / 0.1292\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.5544 / 0.4855\tMAE: 0.1082 / 0.2679\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.5850 / 0.3163\tMAE: 0.1373 / 0.1628\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.5487 / 0.3433\tMAE: 0.1067 / 0.1392\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.5690 / 0.3708\tMAE: 0.1218 / 0.1693\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.5384 / 0.2991\tMAE: 0.1050 / 0.1524\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.6199 / 0.3305\tMAE: 0.1237 / 0.1617\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.5624 / 0.2778\tMAE: 0.1129 / 0.1289\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.5480 / 0.2659\tMAE: 0.1056 / 0.1230\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.5449 / 0.2913\tMAE: 0.1068 / 0.1422\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.5325 / 0.4015\tMAE: 0.1061 / 0.2213\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.5599 / 0.2860\tMAE: 0.1160 / 0.1382\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.5317 / 0.2699\tMAE: 0.0978 / 0.1265\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.5454 / 0.2878\tMAE: 0.1001 / 0.1407\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.5432 / 0.2535\tMAE: 0.0991 / 0.1199\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.5197 / 0.3258\tMAE: 0.0944 / 0.1764\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.5144 / 0.2612\tMAE: 0.0954 / 0.1210\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.5126 / 0.2952\tMAE: 0.0911 / 0.1472\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.5325 / 0.2810\tMAE: 0.0974 / 0.1430\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.5150 / 0.2806\tMAE: 0.0912 / 0.1388\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.5819 / 0.3062\tMAE: 0.1210 / 0.1443\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.5524 / 0.2651\tMAE: 0.1064 / 0.1253\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.5442 / 0.2674\tMAE: 0.1038 / 0.1260\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.5353 / 0.2783\tMAE: 0.1012 / 0.1316\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.5216 / 0.2578\tMAE: 0.0945 / 0.1228\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.5327 / 0.4585\tMAE: 0.0979 / 0.1852\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.5678 / 0.4985\tMAE: 0.1138 / 0.2197\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.6938 / 0.3237\tMAE: 0.1559 / 0.1641\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.5901 / 0.3716\tMAE: 0.1265 / 0.1393\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.7535 / 0.6766\tMAE: 0.1633 / 0.2681\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.7412 / 0.3762\tMAE: 0.1645 / 0.1695\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.6305 / 0.3588\tMAE: 0.1288 / 0.1751\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.5948 / 0.3031\tMAE: 0.1171 / 0.1321\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.5653 / 0.3446\tMAE: 0.1071 / 0.1771\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.5750 / 0.2798\tMAE: 0.1167 / 0.1264\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.5783 / 0.2758\tMAE: 0.1072 / 0.1285\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.5487 / 0.2883\tMAE: 0.1007 / 0.1305\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.5629 / 0.3020\tMAE: 0.1062 / 0.1356\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.5577 / 0.2836\tMAE: 0.1044 / 0.1379\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.5771 / 0.3321\tMAE: 0.1113 / 0.1652\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.5508 / 0.3041\tMAE: 0.1077 / 0.1374\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.5359 / 0.2800\tMAE: 0.0986 / 0.1340\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.5334 / 0.2730\tMAE: 0.0942 / 0.1257\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.5375 / 0.2642\tMAE: 0.0924 / 0.1270\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.5327 / 0.3124\tMAE: 0.0912 / 0.1627\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.5770 / 0.3123\tMAE: 0.1104 / 0.1491\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.5446 / 0.2839\tMAE: 0.1033 / 0.1373\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.5466 / 0.2682\tMAE: 0.1091 / 0.1245\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.5102 / 0.2593\tMAE: 0.0899 / 0.1226\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.5181 / 0.2610\tMAE: 0.0952 / 0.1243\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.5491 / 0.2907\tMAE: 0.1041 / 0.1459\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.5128 / 0.2857\tMAE: 0.0942 / 0.1452\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.5098 / 0.2609\tMAE: 0.0905 / 0.1257\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.5298 / 0.2648\tMAE: 0.0919 / 0.1255\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.5015 / 0.3344\tMAE: 0.0896 / 0.1508\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.5231 / 0.2660\tMAE: 0.0946 / 0.1269\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.5313 / 0.3055\tMAE: 0.0989 / 0.1303\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.5398 / 0.2666\tMAE: 0.0961 / 0.1250\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.5159 / 0.2843\tMAE: 0.0953 / 0.1430\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.5073 / 0.3634\tMAE: 0.0920 / 0.2028\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.5257 / 0.2611\tMAE: 0.1017 / 0.1244\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.5822 / 0.3206\tMAE: 0.1191 / 0.1525\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.5606 / 0.3093\tMAE: 0.1016 / 0.1543\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.5353 / 0.2745\tMAE: 0.0964 / 0.1273\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.5157 / 0.3837\tMAE: 0.0866 / 0.1722\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.5326 / 0.2813\tMAE: 0.0924 / 0.1326\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.4994 / 0.3012\tMAE: 0.0909 / 0.1424\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.5082 / 0.2795\tMAE: 0.0879 / 0.1376\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.5032 / 0.3216\tMAE: 0.0871 / 0.1678\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.5127 / 0.2763\tMAE: 0.0889 / 0.1283\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.5005 / 0.2998\tMAE: 0.0806 / 0.1444\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.5108 / 0.2834\tMAE: 0.0943 / 0.1425\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.5016 / 0.2483\tMAE: 0.0898 / 0.1188\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.4819 / 0.2705\tMAE: 0.0786 / 0.1303\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.4905 / 0.3131\tMAE: 0.0828 / 0.1645\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.5118 / 0.2824\tMAE: 0.0949 / 0.1415\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.4886 / 0.2664\tMAE: 0.0813 / 0.1270\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.4931 / 0.2638\tMAE: 0.0739 / 0.1294\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.4833 / 0.2776\tMAE: 0.0741 / 0.1272\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.5398 / 0.2972\tMAE: 0.0951 / 0.1473\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.5044 / 0.2630\tMAE: 0.0824 / 0.1237\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.4972 / 0.2820\tMAE: 0.0764 / 0.1408\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.4912 / 0.3325\tMAE: 0.0791 / 0.1802\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.5138 / 0.2609\tMAE: 0.1005 / 0.1229\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.4815 / 0.3000\tMAE: 0.0781 / 0.1532\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.5015 / 0.2905\tMAE: 0.0877 / 0.1394\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.5184 / 0.3268\tMAE: 0.0897 / 0.1714\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.4882 / 0.3114\tMAE: 0.0837 / 0.1654\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.4894 / 0.2591\tMAE: 0.0859 / 0.1230\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.5051 / 0.3094\tMAE: 0.0755 / 0.1519\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.5030 / 0.3065\tMAE: 0.0877 / 0.1616\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.4962 / 0.2980\tMAE: 0.0847 / 0.1342\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.4968 / 0.3432\tMAE: 0.0870 / 0.1874\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.5025 / 0.3061\tMAE: 0.0896 / 0.1391\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.4750 / 0.2907\tMAE: 0.0740 / 0.1475\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.5242 / 0.2851\tMAE: 0.1092 / 0.1426\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.5211 / 0.2773\tMAE: 0.0964 / 0.1403\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.5563 / 0.2796\tMAE: 0.0982 / 0.1397\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.5125 / 0.2951\tMAE: 0.0895 / 0.1495\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.4934 / 0.2650\tMAE: 0.0854 / 0.1240\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.4838 / 0.2746\tMAE: 0.0767 / 0.1328\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.4995 / 0.2780\tMAE: 0.0842 / 0.1332\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.5233 / 0.3167\tMAE: 0.0897 / 0.1594\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.5459 / 0.2934\tMAE: 0.1021 / 0.1402\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.5410 / 0.3112\tMAE: 0.1001 / 0.1530\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.5200 / 0.3070\tMAE: 0.0974 / 0.1455\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.5130 / 0.2639\tMAE: 0.0909 / 0.1243\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.5071 / 0.2681\tMAE: 0.0875 / 0.1305\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.5125 / 0.2819\tMAE: 0.0923 / 0.1320\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.4857 / 0.2859\tMAE: 0.0768 / 0.1322\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.4859 / 0.2910\tMAE: 0.0793 / 0.1276\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.4939 / 0.3222\tMAE: 0.0785 / 0.1650\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.5733 / 0.7919\tMAE: 0.1176 / 0.2025\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.7390 / 0.3440\tMAE: 0.1844 / 0.1575\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.6517 / 0.5689\tMAE: 0.1449 / 0.1909\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.6819 / 0.3071\tMAE: 0.1476 / 0.1456\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.5879 / 0.3336\tMAE: 0.1205 / 0.1731\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.5602 / 0.2898\tMAE: 0.1167 / 0.1300\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.5533 / 0.3300\tMAE: 0.1112 / 0.1364\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.5964 / 0.3501\tMAE: 0.1189 / 0.1616\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.6196 / 0.3184\tMAE: 0.1260 / 0.1555\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.6655 / 0.3378\tMAE: 0.1368 / 0.1496\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.6108 / 0.3299\tMAE: 0.1228 / 0.1664\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.5778 / 0.3163\tMAE: 0.1178 / 0.1368\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.5609 / 0.2757\tMAE: 0.1073 / 0.1270\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.5750 / 0.2826\tMAE: 0.1080 / 0.1280\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.5427 / 0.2645\tMAE: 0.1012 / 0.1220\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.5349 / 0.2658\tMAE: 0.1032 / 0.1237\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.5283 / 0.3001\tMAE: 0.1013 / 0.1404\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.5581 / 0.3049\tMAE: 0.1111 / 0.1482\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.5283 / 0.3229\tMAE: 0.1042 / 0.1699\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.5315 / 0.2715\tMAE: 0.0985 / 0.1264\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.5128 / 0.3023\tMAE: 0.0904 / 0.1452\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.5310 / 0.2684\tMAE: 0.0966 / 0.1275\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.5064 / 0.2697\tMAE: 0.0859 / 0.1267\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.5122 / 0.2768\tMAE: 0.0897 / 0.1328\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.4924 / 0.2772\tMAE: 0.0820 / 0.1320\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.4922 / 0.3144\tMAE: 0.0802 / 0.1590\n",
      "100% of metal data used - 8218 metal with 10387 insulator (44%)\n",
      "d:/MODELS/202204/nmm\\M02R\\metal_TTT_09_M2_L1_logL1\n",
      "Epoch [1/300]\tTrain/Valid Loss: 4.5208 / 3.6506\tMAE: 1.5208 / 1.3844\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.6033 / 1.5157\tMAE: 0.9327 / 0.6950\n",
      "Epoch [3/300]\tTrain/Valid Loss: 1.7266 / 1.0946\tMAE: 0.5536 / 0.4452\n",
      "Epoch [4/300]\tTrain/Valid Loss: 1.2830 / 0.6844\tMAE: 0.3205 / 0.2972\n",
      "Epoch [5/300]\tTrain/Valid Loss: 0.9624 / 0.4217\tMAE: 0.2124 / 0.2031\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.9280 / 0.4726\tMAE: 0.2069 / 0.1903\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.8553 / 0.4763\tMAE: 0.1850 / 0.1789\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.8608 / 0.3954\tMAE: 0.1760 / 0.1803\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.8123 / 0.3778\tMAE: 0.1653 / 0.1593\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.7932 / 0.3436\tMAE: 0.1566 / 0.1475\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.7885 / 0.3261\tMAE: 0.1521 / 0.1530\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.7845 / 0.3100\tMAE: 0.1477 / 0.1421\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.7804 / 0.2936\tMAE: 0.1447 / 0.1374\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.7758 / 0.3268\tMAE: 0.1398 / 0.1554\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.7648 / 0.2844\tMAE: 0.1374 / 0.1317\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.7390 / 0.2793\tMAE: 0.1306 / 0.1284\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.7396 / 0.2897\tMAE: 0.1288 / 0.1303\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.8095 / 0.2925\tMAE: 0.1500 / 0.1290\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.7351 / 0.3072\tMAE: 0.1276 / 0.1351\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.7604 / 0.3310\tMAE: 0.1340 / 0.1463\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.7394 / 0.2693\tMAE: 0.1264 / 0.1238\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.7574 / 0.2887\tMAE: 0.1365 / 0.1362\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.7351 / 0.3075\tMAE: 0.1241 / 0.1395\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.7090 / 0.2895\tMAE: 0.1194 / 0.1250\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.7142 / 0.2910\tMAE: 0.1193 / 0.1198\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.7247 / 0.3167\tMAE: 0.1216 / 0.1503\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.7036 / 0.2563\tMAE: 0.1147 / 0.1211\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.7110 / 0.2874\tMAE: 0.1162 / 0.1274\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.7051 / 0.2602\tMAE: 0.1131 / 0.1180\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.6919 / 0.2613\tMAE: 0.1094 / 0.1180\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.7222 / 0.2623\tMAE: 0.1128 / 0.1228\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.7383 / 0.2634\tMAE: 0.1177 / 0.1235\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.6864 / 0.2630\tMAE: 0.1112 / 0.1207\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.6959 / 0.2843\tMAE: 0.1075 / 0.1376\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.7009 / 0.2495\tMAE: 0.1117 / 0.1137\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.6842 / 0.2771\tMAE: 0.1092 / 0.1260\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.6852 / 0.2539\tMAE: 0.1071 / 0.1129\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.6713 / 0.2506\tMAE: 0.1036 / 0.1152\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.6822 / 0.2750\tMAE: 0.1106 / 0.1186\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.6680 / 0.2512\tMAE: 0.1050 / 0.1136\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.6637 / 0.2621\tMAE: 0.1007 / 0.1145\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.7039 / 0.2558\tMAE: 0.1101 / 0.1153\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.6845 / 0.2441\tMAE: 0.1093 / 0.1100\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.6733 / 0.2565\tMAE: 0.1069 / 0.1121\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.6647 / 0.3100\tMAE: 0.1028 / 0.1144\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.6581 / 0.2651\tMAE: 0.1019 / 0.1183\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.6440 / 0.2419\tMAE: 0.0934 / 0.1098\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.6461 / 0.2920\tMAE: 0.0933 / 0.1427\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.6522 / 0.2426\tMAE: 0.0946 / 0.1101\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.6370 / 0.2655\tMAE: 0.0910 / 0.1180\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.6399 / 0.2570\tMAE: 0.0902 / 0.1173\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.6420 / 0.3014\tMAE: 0.0917 / 0.1176\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.6633 / 0.3103\tMAE: 0.0953 / 0.1124\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.6708 / 0.2460\tMAE: 0.1049 / 0.1113\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.7313 / 0.2791\tMAE: 0.1177 / 0.1253\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.7716 / 0.2650\tMAE: 0.1316 / 0.1213\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.7035 / 0.3565\tMAE: 0.1117 / 0.1389\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.7513 / 0.2766\tMAE: 0.1140 / 0.1301\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.6974 / 0.2641\tMAE: 0.1121 / 0.1264\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.6674 / 0.2564\tMAE: 0.0983 / 0.1098\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.6617 / 0.2564\tMAE: 0.0950 / 0.1178\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.6516 / 0.2603\tMAE: 0.0949 / 0.1121\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.6371 / 0.2434\tMAE: 0.0923 / 0.1050\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.6452 / 0.2621\tMAE: 0.0943 / 0.1220\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.6534 / 0.2568\tMAE: 0.0967 / 0.1109\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.6464 / 0.2460\tMAE: 0.0940 / 0.1103\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.6681 / 0.3186\tMAE: 0.0950 / 0.1403\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.7137 / 0.3145\tMAE: 0.1033 / 0.1299\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.7055 / 0.2633\tMAE: 0.1048 / 0.1152\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.6637 / 0.2586\tMAE: 0.0961 / 0.1150\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.6445 / 0.2415\tMAE: 0.0917 / 0.1067\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.6411 / 0.2546\tMAE: 0.0900 / 0.1142\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.6535 / 0.2594\tMAE: 0.0961 / 0.1175\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.6163 / 0.2410\tMAE: 0.0815 / 0.1063\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.6187 / 0.3027\tMAE: 0.0826 / 0.1222\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.6316 / 0.2372\tMAE: 0.0847 / 0.1050\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.6252 / 0.2404\tMAE: 0.0812 / 0.1053\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.6260 / 0.2564\tMAE: 0.0880 / 0.1169\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.6827 / 0.4424\tMAE: 0.1021 / 0.1859\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.7538 / 0.2512\tMAE: 0.1341 / 0.1145\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.6674 / 0.2652\tMAE: 0.0996 / 0.1242\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.6711 / 0.2578\tMAE: 0.1029 / 0.1129\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.6372 / 0.2490\tMAE: 0.0901 / 0.1130\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.6588 / 0.2777\tMAE: 0.0965 / 0.1133\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.6529 / 0.2419\tMAE: 0.0924 / 0.1062\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.6158 / 0.2392\tMAE: 0.0815 / 0.1055\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.6229 / 0.2398\tMAE: 0.0803 / 0.1071\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.6659 / 0.2694\tMAE: 0.0917 / 0.1144\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.6466 / 0.2574\tMAE: 0.0922 / 0.1162\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.6228 / 0.2666\tMAE: 0.0835 / 0.1187\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.6454 / 0.2559\tMAE: 0.0911 / 0.1122\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.6364 / 0.2495\tMAE: 0.0872 / 0.1109\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.6330 / 0.2445\tMAE: 0.0882 / 0.1075\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.6426 / 0.2504\tMAE: 0.0891 / 0.1095\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.6223 / 0.2577\tMAE: 0.0812 / 0.1227\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.6053 / 0.2523\tMAE: 0.0757 / 0.1093\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.6081 / 0.2371\tMAE: 0.0760 / 0.1086\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.6017 / 0.2388\tMAE: 0.0734 / 0.1093\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.6084 / 0.2564\tMAE: 0.0726 / 0.1217\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.6094 / 0.2427\tMAE: 0.0786 / 0.1082\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.6010 / 0.2537\tMAE: 0.0713 / 0.1127\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.6062 / 0.2379\tMAE: 0.0698 / 0.1065\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.6134 / 0.2685\tMAE: 0.0731 / 0.1204\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.6314 / 0.3447\tMAE: 0.0821 / 0.1470\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.6971 / 0.3006\tMAE: 0.1057 / 0.1306\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.6493 / 0.2547\tMAE: 0.0885 / 0.1101\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.6312 / 0.2539\tMAE: 0.0808 / 0.1122\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.6130 / 0.2479\tMAE: 0.0756 / 0.1085\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.6319 / 0.2610\tMAE: 0.0827 / 0.1142\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.6338 / 0.2650\tMAE: 0.0862 / 0.1222\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.6473 / 0.2676\tMAE: 0.0931 / 0.1115\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.6942 / 0.3219\tMAE: 0.1058 / 0.1405\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.6703 / 0.3122\tMAE: 0.1056 / 0.1377\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.6604 / 0.2455\tMAE: 0.0925 / 0.1086\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.6359 / 0.2958\tMAE: 0.0861 / 0.1130\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.6371 / 0.2637\tMAE: 0.0927 / 0.1137\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.6182 / 0.2739\tMAE: 0.0762 / 0.1080\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.6291 / 0.2451\tMAE: 0.0815 / 0.1105\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.6218 / 0.2612\tMAE: 0.0823 / 0.1182\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.6337 / 0.3026\tMAE: 0.0870 / 0.1421\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.6228 / 0.2302\tMAE: 0.0807 / 0.1051\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.6027 / 0.2480\tMAE: 0.0751 / 0.1073\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.5986 / 0.2430\tMAE: 0.0705 / 0.1096\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.6102 / 0.2489\tMAE: 0.0762 / 0.1074\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.6071 / 0.2812\tMAE: 0.0753 / 0.1292\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.6168 / 0.2462\tMAE: 0.0764 / 0.1093\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.6066 / 0.2654\tMAE: 0.0728 / 0.1212\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.6687 / 0.2926\tMAE: 0.0958 / 0.1191\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.6344 / 0.2615\tMAE: 0.0828 / 0.1131\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.6212 / 0.2599\tMAE: 0.0849 / 0.1138\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.6024 / 0.2459\tMAE: 0.0730 / 0.1088\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.6305 / 0.2534\tMAE: 0.0824 / 0.1131\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.6054 / 0.2482\tMAE: 0.0724 / 0.1104\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.5933 / 0.2649\tMAE: 0.0684 / 0.1077\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.5963 / 0.2342\tMAE: 0.0733 / 0.1066\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.5882 / 0.2692\tMAE: 0.0688 / 0.1284\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.6046 / 0.2390\tMAE: 0.0822 / 0.1064\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.5846 / 0.2582\tMAE: 0.0683 / 0.1180\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.5987 / 0.2403\tMAE: 0.0723 / 0.1066\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.5771 / 0.2437\tMAE: 0.0623 / 0.1089\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.5821 / 0.2838\tMAE: 0.0650 / 0.1112\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.6001 / 0.2475\tMAE: 0.0721 / 0.1075\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.5934 / 0.2922\tMAE: 0.0700 / 0.1173\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.6039 / 0.2511\tMAE: 0.0745 / 0.1071\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.6002 / 0.2388\tMAE: 0.0722 / 0.1081\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.5837 / 0.2479\tMAE: 0.0690 / 0.1092\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.5776 / 0.2789\tMAE: 0.0627 / 0.1102\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.5981 / 0.2574\tMAE: 0.0714 / 0.1146\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.5815 / 0.2359\tMAE: 0.0638 / 0.1069\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.5708 / 0.2729\tMAE: 0.0636 / 0.1221\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.5727 / 0.2674\tMAE: 0.0640 / 0.1182\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.5698 / 0.2540\tMAE: 0.0590 / 0.1140\n",
      "Epoch [153/300]\tTrain/Valid Loss: inf / 0.3389\tMAE: 0.0986 / 0.1530\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.7211 / 0.8528\tMAE: 0.1089 / 0.1755\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.7288 / 0.2659\tMAE: 0.1103 / 0.1186\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.6619 / 0.2541\tMAE: 0.0980 / 0.1119\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.6366 / 0.2976\tMAE: 0.0871 / 0.1318\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.6385 / 0.2473\tMAE: 0.0889 / 0.1099\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.6206 / 0.3034\tMAE: 0.0812 / 0.1182\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.6381 / 0.2388\tMAE: 0.0856 / 0.1104\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.6101 / 0.2717\tMAE: 0.0794 / 0.1257\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.6047 / 0.2380\tMAE: 0.0749 / 0.1072\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.5855 / 0.2579\tMAE: 0.0658 / 0.1101\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.6079 / 0.2625\tMAE: 0.0762 / 0.1215\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.5933 / 0.2363\tMAE: 0.0693 / 0.1052\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.5776 / 0.2381\tMAE: 0.0630 / 0.1083\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.5824 / 0.2372\tMAE: 0.0636 / 0.1069\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.5755 / 0.2457\tMAE: 0.0611 / 0.1064\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.5856 / 0.2572\tMAE: 0.0649 / 0.1146\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.6067 / 0.2693\tMAE: 0.0734 / 0.1156\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.6032 / 0.2467\tMAE: 0.0762 / 0.1088\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.5926 / 0.2664\tMAE: 0.0683 / 0.1091\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.5775 / 0.2438\tMAE: 0.0639 / 0.1102\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.5760 / 0.2748\tMAE: 0.0644 / 0.1229\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.5881 / 0.2501\tMAE: 0.0698 / 0.1074\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.5681 / 0.2386\tMAE: 0.0588 / 0.1063\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.6084 / 0.2479\tMAE: 0.0766 / 0.1097\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.5753 / 0.2526\tMAE: 0.0616 / 0.1118\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.5697 / 0.2408\tMAE: 0.0582 / 0.1070\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.5818 / 0.2984\tMAE: 0.0618 / 0.1208\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.5959 / 0.2655\tMAE: 0.0726 / 0.1290\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.5786 / 0.2360\tMAE: 0.0649 / 0.1081\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.5870 / 0.2545\tMAE: 0.0654 / 0.1151\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.5727 / 0.2401\tMAE: 0.0626 / 0.1071\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.5736 / 0.2414\tMAE: 0.0596 / 0.1080\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.5578 / 0.2516\tMAE: 0.0555 / 0.1180\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.5991 / 0.2761\tMAE: 0.0722 / 0.1193\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.5820 / 0.2414\tMAE: 0.0656 / 0.1066\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.6244 / 0.2955\tMAE: 0.0807 / 0.1244\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.6260 / 0.3106\tMAE: 0.0800 / 0.1398\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.6226 / 0.2449\tMAE: 0.0805 / 0.1089\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.5769 / 0.2444\tMAE: 0.0610 / 0.1068\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.5775 / 0.2384\tMAE: 0.0644 / 0.1069\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.5697 / 0.2371\tMAE: 0.0604 / 0.1084\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.5596 / 0.2408\tMAE: 0.0533 / 0.1085\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.5553 / 0.2554\tMAE: 0.0544 / 0.1220\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.5599 / 0.2469\tMAE: 0.0530 / 0.1071\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.5591 / 0.2468\tMAE: 0.0526 / 0.1133\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.5625 / 0.2392\tMAE: 0.0571 / 0.1081\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.5465 / 0.2387\tMAE: 0.0472 / 0.1052\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.5641 / 0.2472\tMAE: 0.0545 / 0.1095\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.5677 / 0.2730\tMAE: 0.0594 / 0.1266\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.5632 / 0.2675\tMAE: 0.0581 / 0.1063\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.5665 / 0.2547\tMAE: 0.0575 / 0.1164\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.5554 / 0.2561\tMAE: 0.0538 / 0.1194\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.5470 / 0.2451\tMAE: 0.0477 / 0.1084\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.5507 / 0.2402\tMAE: 0.0551 / 0.1079\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.5401 / 0.2455\tMAE: 0.0505 / 0.1094\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.5510 / 0.2427\tMAE: 0.0501 / 0.1093\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.5473 / 0.2686\tMAE: 0.0474 / 0.1300\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.5523 / 0.2372\tMAE: 0.0563 / 0.1087\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.5406 / 0.2332\tMAE: 0.0447 / 0.1056\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.5503 / 0.2596\tMAE: 0.0501 / 0.1248\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.5501 / 0.2500\tMAE: 0.0543 / 0.1178\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.5384 / 0.2479\tMAE: 0.0474 / 0.1139\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.5453 / 0.2330\tMAE: 0.0476 / 0.1053\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.5413 / 0.2331\tMAE: 0.0434 / 0.1041\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.5405 / 0.2439\tMAE: 0.0471 / 0.1097\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.6431 / 0.2982\tMAE: 0.0797 / 0.1306\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.6185 / 0.2719\tMAE: 0.0786 / 0.1167\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.5846 / 0.2841\tMAE: 0.0697 / 0.1266\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.5817 / 0.2568\tMAE: 0.0719 / 0.1212\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.5687 / 0.2426\tMAE: 0.0598 / 0.1123\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.5781 / 0.2569\tMAE: 0.0617 / 0.1121\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.6246 / 0.2483\tMAE: 0.0785 / 0.1099\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.5888 / 0.2548\tMAE: 0.0728 / 0.1164\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.5879 / 0.2635\tMAE: 0.0653 / 0.1162\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.5744 / 0.2608\tMAE: 0.0592 / 0.1209\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.5657 / 0.2510\tMAE: 0.0560 / 0.1124\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.5621 / 0.2497\tMAE: 0.0563 / 0.1128\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.5482 / 0.2377\tMAE: 0.0511 / 0.1085\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.5482 / 0.2448\tMAE: 0.0495 / 0.1108\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.5625 / 0.2445\tMAE: 0.0512 / 0.1103\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.5558 / 0.2618\tMAE: 0.0509 / 0.1116\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.5994 / 0.2716\tMAE: 0.0719 / 0.1216\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.5751 / 0.2466\tMAE: 0.0675 / 0.1092\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.5624 / 0.2464\tMAE: 0.0569 / 0.1118\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.5558 / 0.2459\tMAE: 0.0526 / 0.1093\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.5373 / 0.2507\tMAE: 0.0482 / 0.1162\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.5474 / 0.2292\tMAE: 0.0469 / 0.1040\n",
      "Epoch [241/300]\tTrain/Valid Loss: inf / 0.2295\tMAE: 0.0443 / 0.1049\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.5374 / 0.2364\tMAE: 0.0435 / 0.1073\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.5342 / 0.2407\tMAE: 0.0418 / 0.1075\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.5438 / 0.2317\tMAE: 0.0449 / 0.1049\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.5429 / 0.2467\tMAE: 0.0455 / 0.1142\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.5392 / 0.2460\tMAE: 0.0489 / 0.1098\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.5384 / 0.2344\tMAE: 0.0468 / 0.1062\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.5357 / 0.2390\tMAE: 0.0415 / 0.1113\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.5306 / 0.2350\tMAE: 0.0412 / 0.1097\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.5334 / 0.2596\tMAE: 0.0417 / 0.1286\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.5331 / 0.2311\tMAE: 0.0422 / 0.1080\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.5295 / 0.2312\tMAE: 0.0448 / 0.1081\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.5222 / 0.2329\tMAE: 0.0394 / 0.1063\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.5244 / 0.2351\tMAE: 0.0369 / 0.1071\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.5340 / 0.2329\tMAE: 0.0407 / 0.1088\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.5367 / 0.2356\tMAE: 0.0427 / 0.1101\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.5326 / 0.2377\tMAE: 0.0439 / 0.1096\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.5372 / 0.2400\tMAE: 0.0427 / 0.1117\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.5302 / 0.2339\tMAE: 0.0399 / 0.1081\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.5330 / 0.2444\tMAE: 0.0406 / 0.1164\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.5326 / 0.2460\tMAE: 0.0429 / 0.1166\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.5329 / 0.2365\tMAE: 0.0382 / 0.1111\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.5393 / 0.2276\tMAE: 0.0402 / 0.1060\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.5259 / 0.2320\tMAE: 0.0368 / 0.1061\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.5255 / 0.2383\tMAE: 0.0379 / 0.1123\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.5461 / 0.2343\tMAE: 0.0486 / 0.1066\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.5283 / 0.2358\tMAE: 0.0387 / 0.1065\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.5473 / 0.2304\tMAE: 0.0496 / 0.1058\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.5291 / 0.2325\tMAE: 0.0381 / 0.1064\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.5211 / 0.2302\tMAE: 0.0336 / 0.1062\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.5308 / 0.2331\tMAE: 0.0378 / 0.1071\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.5361 / 0.2397\tMAE: 0.0464 / 0.1083\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.5261 / 0.2405\tMAE: 0.0364 / 0.1107\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.5445 / 0.2420\tMAE: 0.0522 / 0.1135\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.5463 / 0.2366\tMAE: 0.0466 / 0.1096\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.5305 / 0.2495\tMAE: 0.0415 / 0.1206\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.5375 / 0.2665\tMAE: 0.0476 / 0.1338\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.5222 / 0.2337\tMAE: 0.0398 / 0.1067\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.5242 / 0.2341\tMAE: 0.0382 / 0.1076\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.5193 / 0.2381\tMAE: 0.0339 / 0.1100\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.5498 / 0.2865\tMAE: 0.0435 / 0.1228\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.5563 / 0.2622\tMAE: 0.0501 / 0.1169\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.5371 / 0.2465\tMAE: 0.0414 / 0.1079\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.5375 / 0.2347\tMAE: 0.0433 / 0.1081\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.5211 / 0.2417\tMAE: 0.0351 / 0.1086\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.5258 / 0.2481\tMAE: 0.0360 / 0.1168\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.5361 / 0.2445\tMAE: 0.0435 / 0.1125\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.5289 / 0.2374\tMAE: 0.0444 / 0.1068\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.5334 / 0.2379\tMAE: 0.0374 / 0.1072\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.5272 / 0.2498\tMAE: 0.0414 / 0.1166\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.5295 / 0.2417\tMAE: 0.0374 / 0.1106\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.5216 / 0.2393\tMAE: 0.0368 / 0.1071\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.5215 / 0.2377\tMAE: 0.0387 / 0.1086\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.5206 / 0.2346\tMAE: 0.0349 / 0.1056\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.5175 / 0.2477\tMAE: 0.0343 / 0.1158\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.5494 / 0.2366\tMAE: 0.0502 / 0.1074\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.5251 / 0.2467\tMAE: 0.0405 / 0.1136\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.5412 / 0.2382\tMAE: 0.0434 / 0.1078\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.5257 / 0.2379\tMAE: 0.0355 / 0.1090\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.5177 / 0.2389\tMAE: 0.0338 / 0.1081\n"
     ]
    }
   ],
   "source": [
    "from model.model_02r import DistNN\n",
    "from util import trainer_mix2 as tr\n",
    "dataset = Dataset()\n",
    "\n",
    "for scale in ['metal_TTT']:\n",
    "    dataset.load_dataset(f'c:/WORKSPACE_KRICT/DATA/data_snu/inputdata_{scale}.pickle', True)\n",
    "    for mr in [0.2, 0.4, 0.6, 1]:\n",
    "        exec_model(scale=scale, model_type='M02R', dataset=dataset, \n",
    "                   comment='M2_L1_logL1', metal_ratio=mr, batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ex01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f92c05c172f2cb43def306c9114b025ca30c850dcb72389d99cb257a52585359"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
