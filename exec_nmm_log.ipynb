{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os, gc, json\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from util.input_data import Dataset\n",
    "from util.AdaBound import AdaBound\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "def exec_model(\n",
    "    scale,\n",
    "    model_type,\n",
    "    comment='',\n",
    "    lr = 1e-5,\n",
    "    wd = 1e-7,\n",
    "    tries = 1,\n",
    "    root_model = 'd:/MODELS/202204/nmm',\n",
    "    root_data  = 'c:/WORKSPACE_KRICT/DATA/data_snu',\n",
    "    num_epochs = 300,\n",
    "    batch_size = 128,\n",
    "    train_ratio = 0.7,\n",
    "    valid_ratio = 0.2,\n",
    "):\n",
    "    gc.collect()\n",
    "\n",
    "    dataset = Dataset()\n",
    "    dataset.load_dataset(os.path.join(root_data, f'inputdata_{scale}.pickle'), silent=True)\n",
    "\n",
    "    for n in range(0, tries):\n",
    "        rseed  = 35 + n\n",
    "        train_data, valid_data, test_data = dataset.train_test_split(train_ratio=train_ratio, \n",
    "                                                                     valid_ratio=valid_ratio,\n",
    "                                                                     rseed=rseed)\n",
    "        train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, \n",
    "                                    collate_fn=tr.collate_fn)\n",
    "        val_data_loader = DataLoader(valid_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "        test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate_fn)\n",
    "\n",
    "        model = DistNN(dataset.n_atom_feats, dataset.n_rdf_feature, dataset.n_bdf_feature).cuda()\n",
    "        optimizer = AdaBound(model.parameters(), lr=lr, weight_decay=wd)\n",
    "        criterion = torch.nn.L1Loss()\n",
    "\n",
    "        for i in range(99):\n",
    "            root = os.path.join(root_model, model_type)\n",
    "            if not os.path.isdir(root):\n",
    "                os.makedirs(root)\n",
    "            if '{}_{:02d}'.format(scale, i) not in ' '.join(os.listdir(root)):\n",
    "                output_root = os.path.join(root, '{}_{:02d}'.format(scale, i))\n",
    "                if len(comment) > 0: output_root += f'_{comment}'\n",
    "                os.makedirs(output_root)\n",
    "                break\n",
    "        print(output_root)\n",
    "        with open(os.path.join(output_root, 'params.json'),'w') as f:\n",
    "            json.dump(dict(random_seed=rseed, learning_rate=lr, weight_decay=wd, \n",
    "                train_ratio=train_ratio, valid_ratio=valid_ratio, batch_size=batch_size), \n",
    "                f, indent=4)\n",
    "        writer = SummaryWriter(output_root)\n",
    "        #with torch.no_grad():\n",
    "        #    dummy = iter(test_data_loader).next()\n",
    "        #    writer.add_graph(model, dummy[:7])\n",
    "\n",
    "        for epoch in range(1, num_epochs+1):\n",
    "            train_loss, train_mae = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "            valid_loss, valid_mae, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "            print('Epoch [{}/{}]\\tTrain/Valid Loss: {:.4f} / {:.4f}\\tMAE: {:.4f} / {:.4f}'\n",
    "                    .format(epoch, num_epochs, train_loss, valid_loss, train_mae, valid_mae))\n",
    "\n",
    "            writer.add_scalar('train/loss', train_loss, epoch)\n",
    "            writer.add_scalar('train/MAE', train_mae, epoch)\n",
    "            writer.add_scalar('valid/loss', valid_loss, epoch)\n",
    "            writer.add_scalar('valid/MAE', valid_mae, epoch)\n",
    "\n",
    "            if epoch%20 == 0:\n",
    "                torch.save(model.state_dict(), \n",
    "                           os.path.join(output_root, 'model.{:05d}.pt'.format(epoch)))\n",
    "                _, _, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "                np.savetxt(os.path.join(output_root, 'pred.{:05d}.txt'.format(epoch)), \n",
    "                           np.hstack([idxs, targets, preds]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:/MODELS/202204/nmm\\M03R\\metal_FFF_00_log\n",
      "Epoch [1/300]\tTrain/Valid Loss: 2.4689 / 1.8823\tMAE: 1.2529 / 1.1900\n",
      "Epoch [2/300]\tTrain/Valid Loss: 1.5279 / 1.5210\tMAE: 0.8525 / 1.1532\n",
      "Epoch [3/300]\tTrain/Valid Loss: 0.7394 / 0.2886\tMAE: 0.4728 / 0.2046\n",
      "Epoch [4/300]\tTrain/Valid Loss: 0.2910 / 0.2549\tMAE: 0.2431 / 0.1900\n",
      "Epoch [5/300]\tTrain/Valid Loss: 0.2758 / 0.3123\tMAE: 0.2350 / 0.1801\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.2588 / 0.2366\tMAE: 0.2104 / 0.1683\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.2343 / 0.2405\tMAE: 0.1706 / 0.1603\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.2577 / 0.2418\tMAE: 0.1972 / 0.1747\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.2663 / 0.2814\tMAE: 0.1824 / 0.1614\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.2585 / 0.2929\tMAE: 0.1852 / 0.2107\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.3372 / 0.2259\tMAE: 0.2368 / 0.1897\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.3069 / 0.2901\tMAE: 0.2015 / 0.1885\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.2700 / 0.2175\tMAE: 0.1805 / 0.1619\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.2816 / 0.3345\tMAE: 0.1889 / 0.2423\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.3197 / 0.2170\tMAE: 0.1986 / 0.1761\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.2794 / 0.2986\tMAE: 0.1758 / 0.2107\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.2555 / 0.2023\tMAE: 0.1653 / 0.1613\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.2831 / 0.2226\tMAE: 0.1676 / 0.1591\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.2736 / 0.2670\tMAE: 0.1697 / 0.1907\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.2562 / 0.2007\tMAE: 0.1686 / 0.1566\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.2877 / 0.2109\tMAE: 0.1697 / 0.1886\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.2948 / 0.4684\tMAE: 0.1645 / 0.2528\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.3017 / 0.3446\tMAE: 0.1879 / 0.1629\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.2931 / 0.4006\tMAE: 0.1775 / 0.1626\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.3265 / 0.2246\tMAE: 0.1662 / 0.1584\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.3057 / 0.3429\tMAE: 0.1682 / 0.1986\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.3128 / 0.3226\tMAE: 0.1695 / 0.1950\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.3622 / 0.3168\tMAE: 0.1848 / 0.2630\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.3276 / 0.2321\tMAE: 0.1799 / 0.1540\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.3390 / 0.4872\tMAE: 0.1826 / 0.4378\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.3113 / 0.2549\tMAE: 0.1839 / 0.1786\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.3628 / 0.4642\tMAE: 0.1794 / 0.1724\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.3268 / 0.2900\tMAE: 0.1588 / 0.1636\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.4078 / 0.2895\tMAE: 0.1849 / 0.1673\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.2601 / 0.2226\tMAE: 0.1542 / 0.1481\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.2937 / 0.2543\tMAE: 0.1522 / 0.1473\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.3484 / 0.2203\tMAE: 0.1611 / 0.1652\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.2933 / 0.2645\tMAE: 0.1621 / 0.1940\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.4204 / 0.4451\tMAE: 0.1721 / 0.1623\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.3293 / 0.3504\tMAE: 0.1575 / 0.1663\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.3559 / 0.3830\tMAE: 0.1581 / 0.1474\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.3013 / 0.2014\tMAE: 0.1445 / 0.1474\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.2869 / 0.3487\tMAE: 0.1423 / 0.1766\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.2613 / 0.2575\tMAE: 0.1366 / 0.1785\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.3849 / 0.2748\tMAE: 0.1642 / 0.1774\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.3005 / 0.3128\tMAE: 0.1568 / 0.1341\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.2597 / 0.2221\tMAE: 0.1418 / 0.1744\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.3329 / 0.2122\tMAE: 0.1415 / 0.1431\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.4272 / 0.2424\tMAE: 0.1711 / 0.1575\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.2957 / 0.6090\tMAE: 0.1453 / 0.1522\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.3413 / 0.2331\tMAE: 0.1555 / 0.1445\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.2672 / 0.3338\tMAE: 0.1279 / 0.1340\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.2878 / 0.2606\tMAE: 0.1377 / 0.1393\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.3223 / 0.6524\tMAE: 0.1376 / 0.2606\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.3391 / 0.2309\tMAE: 0.1552 / 0.1756\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.3749 / 0.3998\tMAE: 0.1393 / 0.1716\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.2956 / 0.3955\tMAE: 0.1323 / 0.1274\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.3124 / 0.2029\tMAE: 0.1302 / 0.1386\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.2858 / 0.2228\tMAE: 0.1340 / 0.1280\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.3254 / 0.3700\tMAE: 0.1326 / 0.1426\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.3141 / 0.2853\tMAE: 0.1454 / 0.1250\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.2671 / 0.3083\tMAE: 0.1204 / 0.1272\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.3809 / 0.3811\tMAE: 0.1487 / 0.1606\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.2827 / 0.2702\tMAE: 0.1373 / 0.1349\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.3207 / 0.2479\tMAE: 0.1449 / 0.1321\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.2597 / 0.3241\tMAE: 0.1232 / 0.1249\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.3213 / 0.2868\tMAE: 0.1348 / 0.1383\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.2930 / 0.2132\tMAE: 0.1250 / 0.1247\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.3075 / 0.2312\tMAE: 0.1317 / 0.1208\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.2620 / 0.2476\tMAE: 0.1271 / 0.1250\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.2508 / 0.2397\tMAE: 0.1192 / 0.1226\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.2898 / 0.3569\tMAE: 0.1298 / 0.1202\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.2685 / 0.3803\tMAE: 0.1161 / 0.1516\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.2735 / 0.3763\tMAE: 0.1268 / 0.1471\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.3024 / 0.4276\tMAE: 0.1202 / 0.1320\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.3627 / 0.3281\tMAE: 0.1657 / 0.1225\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.3522 / 0.3510\tMAE: 0.1356 / 0.1316\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.3299 / 0.2092\tMAE: 0.1373 / 0.1189\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.3222 / 0.2959\tMAE: 0.1301 / 0.2631\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.2654 / 0.2989\tMAE: 0.1228 / 0.1382\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.2875 / 0.2393\tMAE: 0.1234 / 0.1225\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.3809 / 0.2699\tMAE: 0.1603 / 0.1331\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.2390 / 0.2505\tMAE: 0.1185 / 0.1223\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.3989 / 0.4442\tMAE: 0.1456 / 0.1800\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.3417 / 0.2173\tMAE: 0.1523 / 0.1919\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.2535 / 0.2017\tMAE: 0.1181 / 0.1261\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.2827 / 0.2968\tMAE: 0.1240 / 0.1411\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.2370 / 0.2634\tMAE: 0.1235 / 0.1350\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.2610 / 0.4709\tMAE: 0.1209 / 0.2010\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.3077 / 0.2629\tMAE: 0.1375 / 0.1193\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.2566 / 0.1942\tMAE: 0.1123 / 0.1388\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.3097 / 0.2323\tMAE: 0.1256 / 0.1371\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.3454 / 0.2794\tMAE: 0.1321 / 0.1342\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.3101 / 0.4062\tMAE: 0.1373 / 0.1958\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.2985 / 0.2411\tMAE: 0.1362 / 0.1200\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.2780 / 0.4156\tMAE: 0.1159 / 0.1388\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.3762 / 0.2167\tMAE: 0.1467 / 0.1427\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.2646 / 0.4179\tMAE: 0.1255 / 0.1519\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.2576 / 0.3826\tMAE: 0.1178 / 0.1422\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.3676 / 0.3677\tMAE: 0.1389 / 0.1527\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.2853 / 0.3103\tMAE: 0.1277 / 0.2028\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.2968 / 0.2317\tMAE: 0.1174 / 0.1138\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.2276 / 0.2171\tMAE: 0.1088 / 0.1234\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.2455 / 0.2843\tMAE: 0.1144 / 0.1230\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.2417 / 0.3414\tMAE: 0.1181 / 0.1522\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.2463 / 0.2309\tMAE: 0.1125 / 0.1139\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.3021 / 0.2127\tMAE: 0.1363 / 0.1623\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.3333 / 0.3629\tMAE: 0.1253 / 0.2615\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.2603 / 0.1966\tMAE: 0.1276 / 0.1184\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.2794 / 0.1955\tMAE: 0.1225 / 0.1236\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.3053 / 0.4213\tMAE: 0.1308 / 0.3903\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.2702 / 0.3371\tMAE: 0.1493 / 0.1138\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.2537 / 0.2232\tMAE: 0.1191 / 0.1185\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.2737 / 0.2674\tMAE: 0.1160 / 0.1699\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.2386 / 0.2503\tMAE: 0.1065 / 0.1130\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.3697 / 0.5780\tMAE: 0.1401 / 0.1260\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.3440 / 0.3881\tMAE: 0.1512 / 0.1316\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2530 / 0.3151\tMAE: 0.1151 / 0.1475\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.3018 / 0.3149\tMAE: 0.1378 / 0.1227\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.3309 / 0.4611\tMAE: 0.1328 / 0.2699\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.3106 / 0.3292\tMAE: 0.1286 / 0.1171\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.2542 / 0.3840\tMAE: 0.1077 / 0.1247\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.3866 / 0.2800\tMAE: 0.1653 / 0.1230\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2923 / 0.2493\tMAE: 0.1303 / 0.1221\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.3120 / 0.1957\tMAE: 0.1271 / 0.1122\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.3135 / 0.3978\tMAE: 0.1284 / 0.2847\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.3204 / 0.2044\tMAE: 0.1386 / 0.1206\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.2807 / 0.4884\tMAE: 0.1192 / 0.1276\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.3409 / 0.2464\tMAE: 0.1438 / 0.1552\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.2568 / 0.2698\tMAE: 0.1157 / 0.1234\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.3135 / 0.2462\tMAE: 0.1257 / 0.2211\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2389 / 0.2181\tMAE: 0.1195 / 0.1400\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.3011 / 0.2233\tMAE: 0.1422 / 0.1754\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.3054 / 0.2156\tMAE: 0.1176 / 0.1409\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.2828 / 0.3357\tMAE: 0.1217 / 0.1472\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.2268 / 0.2173\tMAE: 0.1034 / 0.1128\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.2459 / 0.2365\tMAE: 0.1143 / 0.1140\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.2159 / 0.2467\tMAE: 0.1030 / 0.1380\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.2320 / 0.4030\tMAE: 0.1258 / 0.1659\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.2795 / 0.2157\tMAE: 0.1248 / 0.1138\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.2330 / 0.3390\tMAE: 0.1086 / 0.1839\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.3533 / 0.3853\tMAE: 0.1319 / 0.1383\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.2874 / 0.3255\tMAE: 0.1279 / 0.1332\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.2919 / 0.1995\tMAE: 0.1103 / 0.1340\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.2329 / 0.1881\tMAE: 0.1226 / 0.1168\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.2851 / 0.3843\tMAE: 0.1183 / 0.1194\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.2892 / 0.2072\tMAE: 0.1198 / 0.1344\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.2207 / 0.3288\tMAE: 0.1092 / 0.1422\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.2664 / 0.2937\tMAE: 0.1272 / 0.1146\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.2528 / 0.3076\tMAE: 0.1114 / 0.1198\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.2701 / 0.1798\tMAE: 0.1233 / 0.1256\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.2665 / 0.2687\tMAE: 0.1208 / 0.2883\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.3147 / 0.2743\tMAE: 0.1866 / 0.1348\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.2308 / 0.2443\tMAE: 0.1035 / 0.1377\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.2334 / 0.4212\tMAE: 0.1147 / 0.1488\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.3079 / 0.3222\tMAE: 0.1557 / 0.1201\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.2398 / 0.2856\tMAE: 0.1281 / 0.1304\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.2665 / 0.2283\tMAE: 0.1115 / 0.1378\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.2668 / 0.2178\tMAE: 0.1115 / 0.1339\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.2466 / 0.2299\tMAE: 0.1366 / 0.2336\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.2449 / 0.1890\tMAE: 0.1461 / 0.1292\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.2314 / 0.2389\tMAE: 0.1190 / 0.1250\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.2497 / 0.2877\tMAE: 0.1272 / 0.1345\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.2916 / 0.1973\tMAE: 0.1297 / 0.1333\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.2219 / 0.2298\tMAE: 0.1256 / 0.1776\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.2106 / 0.3039\tMAE: 0.1065 / 0.1955\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.2570 / 0.3057\tMAE: 0.1304 / 0.1304\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.2282 / 0.3452\tMAE: 0.1181 / 0.1875\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.3406 / 0.2361\tMAE: 0.1276 / 0.1291\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.2282 / 0.2684\tMAE: 0.1383 / 0.1292\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.2347 / 0.2804\tMAE: 0.1051 / 0.1868\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.2516 / 0.2690\tMAE: 0.1664 / 0.1321\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.3389 / 0.3391\tMAE: 0.1228 / 0.1494\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.2136 / 0.1935\tMAE: 0.1335 / 0.1135\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.2449 / 0.2095\tMAE: 0.1332 / 0.1168\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.2820 / 0.1953\tMAE: 0.1312 / 0.1355\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.2101 / 0.3055\tMAE: 0.1110 / 0.1307\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.2734 / 0.2116\tMAE: 0.1174 / 0.1249\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.2273 / 0.1999\tMAE: 0.1249 / 0.1234\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.2227 / 0.3101\tMAE: 0.1319 / 0.1811\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.2298 / 0.2757\tMAE: 0.1163 / 0.1107\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.2261 / 0.3482\tMAE: 0.1194 / 0.1290\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.2446 / 0.3258\tMAE: 0.1107 / 0.1243\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.2685 / 0.2491\tMAE: 0.1161 / 0.1361\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.2112 / 0.1761\tMAE: 0.1273 / 0.1200\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.2034 / 0.3210\tMAE: 0.1120 / 0.1226\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.2492 / 0.2459\tMAE: 0.1078 / 0.1464\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.2144 / 0.3264\tMAE: 0.1132 / 0.1518\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.2141 / 0.3535\tMAE: 0.1342 / 0.1367\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.2420 / 0.2512\tMAE: 0.1232 / 0.1498\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.2380 / 0.3817\tMAE: 0.1521 / 0.1946\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.2067 / 0.1959\tMAE: 0.1413 / 0.1494\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.2307 / 0.4217\tMAE: 0.1050 / 0.1635\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.2244 / 0.2252\tMAE: 0.1458 / 0.1182\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.1912 / 0.2268\tMAE: 0.1362 / 0.1928\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.2114 / 0.3974\tMAE: 0.1317 / 0.2338\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.2758 / 0.2199\tMAE: 0.1370 / 0.1261\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.2145 / 0.4319\tMAE: 0.1296 / 0.1320\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.2235 / 0.2487\tMAE: 0.1094 / 0.2041\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.2168 / 0.2548\tMAE: 0.1268 / 0.1264\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.1986 / 0.1764\tMAE: 0.1232 / 0.1279\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.1823 / 0.2947\tMAE: 0.1173 / 0.1187\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.1901 / 0.2545\tMAE: 0.1404 / 0.3084\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.2061 / 0.4155\tMAE: 0.1205 / 0.2085\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.2512 / 0.2393\tMAE: 0.1651 / 0.1689\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.1815 / 0.1869\tMAE: 0.1212 / 0.1211\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.2287 / 0.1964\tMAE: 0.1401 / 0.1689\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.2053 / 0.3112\tMAE: 0.1228 / 0.1271\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.2422 / 0.3055\tMAE: 0.1312 / 0.1320\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.1856 / 0.2232\tMAE: 0.1122 / 0.1283\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.1910 / 0.1994\tMAE: 0.1137 / 0.1251\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.2213 / 0.2822\tMAE: 0.1308 / 0.1414\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.1978 / 0.2340\tMAE: 0.1247 / 0.1354\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.2090 / 0.2222\tMAE: 0.1151 / 0.1581\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.2076 / 0.2287\tMAE: 0.1292 / 0.1525\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1796 / 0.2348\tMAE: 0.1520 / 0.1769\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.1950 / 0.2500\tMAE: 0.1266 / 0.1439\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.2186 / 0.2436\tMAE: 0.1186 / 0.1602\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.2345 / 0.2145\tMAE: 0.1670 / 0.1446\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.1657 / 0.2012\tMAE: 0.1137 / 0.1126\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.1628 / 0.3038\tMAE: 0.1246 / 0.1240\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.1796 / 0.1793\tMAE: 0.1297 / 0.1300\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.1889 / 0.2358\tMAE: 0.1147 / 0.1531\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.2043 / 0.2740\tMAE: 0.1178 / 0.1156\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.1960 / 0.2341\tMAE: 0.1225 / 0.1334\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.1863 / 0.1993\tMAE: 0.1854 / 0.1474\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.1655 / 0.2052\tMAE: 0.1263 / 0.1421\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.1649 / 0.3306\tMAE: 0.1196 / 0.1567\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.1997 / 0.2106\tMAE: 0.1105 / 0.1819\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.2082 / 0.2766\tMAE: 0.1492 / 0.1226\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.1944 / 0.3377\tMAE: 0.1332 / 0.1493\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.1884 / 0.2658\tMAE: 0.1530 / 0.1797\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.1818 / 0.2172\tMAE: 0.1327 / 0.2021\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.1863 / 0.3264\tMAE: 0.1829 / 0.4004\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.1797 / 0.2983\tMAE: 0.1488 / 0.1603\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.1627 / 0.2009\tMAE: 0.1404 / 0.1207\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.1381 / 0.2055\tMAE: 0.1104 / 0.1689\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.1698 / 0.2953\tMAE: 0.1370 / 0.2504\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.1877 / 0.1988\tMAE: 0.1541 / 0.1344\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.1569 / 0.2058\tMAE: 0.1278 / 0.1410\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1563 / 0.3147\tMAE: 0.1383 / 0.1909\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.1481 / 0.1790\tMAE: 0.1414 / 0.1229\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.1390 / 0.1967\tMAE: 0.1188 / 0.1286\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.1507 / 0.2035\tMAE: 0.1380 / 0.1211\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.1393 / 0.2205\tMAE: 0.1100 / 0.1704\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.1673 / 0.2435\tMAE: 0.1676 / 0.1226\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.1666 / 0.2556\tMAE: 0.1305 / 0.1872\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.1572 / 0.1955\tMAE: 0.1450 / 0.1409\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.1488 / 0.2075\tMAE: 0.1523 / 0.1525\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.1292 / 0.2691\tMAE: 0.1220 / 0.2259\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.1497 / 0.2141\tMAE: 0.1126 / 0.1533\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.1473 / 0.2514\tMAE: 0.1279 / 0.1493\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.1281 / 0.1976\tMAE: 0.1132 / 0.1578\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.1410 / 0.2100\tMAE: 0.1610 / 0.1921\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.1464 / 0.2189\tMAE: 0.1499 / 0.2194\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.1464 / 0.2139\tMAE: 0.1392 / 0.1356\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.1212 / 0.2146\tMAE: 0.1109 / 0.1702\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.1413 / 0.2331\tMAE: 0.1253 / 0.1200\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.1444 / 0.2056\tMAE: 0.1096 / 0.1866\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.1620 / 0.2165\tMAE: 0.1991 / 0.1931\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.1434 / 0.2201\tMAE: 0.1454 / 0.1639\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.1465 / 0.2365\tMAE: 0.1280 / 0.1729\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.1225 / 0.1745\tMAE: 0.1078 / 0.1281\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.1240 / 0.2182\tMAE: 0.1098 / 0.1363\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.1351 / 0.2649\tMAE: 0.1460 / 0.2449\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.1411 / 0.1889\tMAE: 0.1562 / 0.1310\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.1250 / 0.1777\tMAE: 0.1494 / 0.1260\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.1274 / 0.2581\tMAE: 0.1343 / 0.1592\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.1311 / 0.2208\tMAE: 0.1552 / 0.2247\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.1348 / 0.1967\tMAE: 0.1509 / 0.1190\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.1359 / 0.2235\tMAE: 0.1648 / 0.1580\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.1293 / 0.2680\tMAE: 0.1291 / 0.1712\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.1243 / 0.2097\tMAE: 0.1294 / 0.1356\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.1365 / 0.1998\tMAE: 0.1386 / 0.1771\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.1259 / 0.2337\tMAE: 0.1337 / 0.2167\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.1219 / 0.1854\tMAE: 0.1477 / 0.1682\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.1139 / 0.2052\tMAE: 0.1211 / 0.2082\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.1266 / 0.2027\tMAE: 0.1438 / 0.1210\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.1220 / 0.1883\tMAE: 0.1573 / 0.1554\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.1138 / 0.2096\tMAE: 0.1230 / 0.1762\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.1104 / 0.2068\tMAE: 0.1145 / 0.1988\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.1178 / 0.2125\tMAE: 0.1452 / 0.2147\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.1301 / 0.1870\tMAE: 0.1567 / 0.1441\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.1084 / 0.2136\tMAE: 0.1236 / 0.2007\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.1227 / 0.2054\tMAE: 0.1681 / 0.1681\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.1113 / 0.2351\tMAE: 0.1332 / 0.2951\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.1253 / 0.1929\tMAE: 0.1821 / 0.1642\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.1153 / 0.2137\tMAE: 0.1519 / 0.2145\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.1099 / 0.2041\tMAE: 0.1344 / 0.1497\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.1243 / 0.1887\tMAE: 0.1633 / 0.1310\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.1158 / 0.2096\tMAE: 0.1601 / 0.2625\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.1057 / 0.2043\tMAE: 0.1339 / 0.1984\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.1173 / 0.1768\tMAE: 0.1755 / 0.1427\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.1353 / 0.2588\tMAE: 0.2246 / 0.4599\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.1196 / 0.2045\tMAE: 0.1701 / 0.1503\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.1115 / 0.1923\tMAE: 0.1439 / 0.1725\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.1180 / 0.2435\tMAE: 0.1703 / 0.3941\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.1655 / 0.2577\tMAE: 0.3114 / 0.4256\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.1170 / 0.1889\tMAE: 0.1642 / 0.1255\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.0966 / 0.1793\tMAE: 0.1014 / 0.1128\n",
      "d:/MODELS/202204/nmm\\M03R\\metal_TFF_00_log\n",
      "Epoch [1/300]\tTrain/Valid Loss: 3.4566 / 3.2336\tMAE: 1.3729 / 1.1365\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.1298 / 1.0296\tMAE: 0.8307 / 0.9594\n",
      "Epoch [3/300]\tTrain/Valid Loss: 0.4716 / 0.2788\tMAE: 0.3879 / 0.2508\n",
      "Epoch [4/300]\tTrain/Valid Loss: 0.2667 / 0.2353\tMAE: 0.1949 / 0.1754\n",
      "Epoch [5/300]\tTrain/Valid Loss: 0.2400 / 0.2294\tMAE: 0.1699 / 0.1786\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.2862 / 0.2454\tMAE: 0.1904 / 0.1548\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.2508 / 0.2156\tMAE: 0.1621 / 0.1623\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.2496 / 0.3304\tMAE: 0.1628 / 0.1701\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.2765 / 0.2079\tMAE: 0.1834 / 0.1468\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.2488 / 0.2918\tMAE: 0.1618 / 0.2074\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.3169 / 0.4419\tMAE: 0.1960 / 0.2311\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.2875 / 0.2618\tMAE: 0.1667 / 0.2456\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.2788 / 0.3343\tMAE: 0.1683 / 0.1513\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.2797 / 0.3758\tMAE: 0.1666 / 0.3440\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.2733 / 0.2014\tMAE: 0.1678 / 0.1419\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.2718 / 0.2256\tMAE: 0.1625 / 0.1461\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.2712 / 0.3315\tMAE: 0.1500 / 0.1920\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.2686 / 0.2403\tMAE: 0.1544 / 0.1398\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.2271 / 0.2311\tMAE: 0.1369 / 0.1383\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.2875 / 0.2379\tMAE: 0.1717 / 0.1558\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.2606 / 0.3444\tMAE: 0.1615 / 0.1494\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.3219 / 0.3775\tMAE: 0.1784 / 0.2757\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.2735 / 0.2343\tMAE: 0.1520 / 0.1468\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.2362 / 0.2343\tMAE: 0.1350 / 0.1370\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.3192 / 0.4243\tMAE: 0.1564 / 0.1513\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.3014 / 0.2478\tMAE: 0.1787 / 0.1560\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.3026 / 0.5377\tMAE: 0.1524 / 0.1538\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.4056 / 0.2859\tMAE: 0.1916 / 0.2015\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.2463 / 0.2006\tMAE: 0.1401 / 0.1343\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.2734 / 0.2976\tMAE: 0.1336 / 0.1344\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.3014 / 0.2023\tMAE: 0.1473 / 0.1349\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.2386 / 0.2030\tMAE: 0.1320 / 0.1291\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.3420 / 0.3849\tMAE: 0.1503 / 0.2370\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.2949 / 0.2227\tMAE: 0.1542 / 0.1674\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.3392 / 0.2478\tMAE: 0.1473 / 0.2154\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.2879 / 0.2637\tMAE: 0.1307 / 0.1660\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.2835 / 0.3346\tMAE: 0.1426 / 0.1505\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.2816 / 0.3127\tMAE: 0.1354 / 0.1956\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.2504 / 0.2988\tMAE: 0.1274 / 0.1490\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.2874 / 0.2723\tMAE: 0.1374 / 0.1453\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.2437 / 0.2176\tMAE: 0.1263 / 0.1344\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.2458 / 0.2405\tMAE: 0.1273 / 0.1386\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.2471 / 0.4316\tMAE: 0.1191 / 0.1331\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.3460 / 0.3884\tMAE: 0.1497 / 0.2633\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.2759 / 0.2719\tMAE: 0.1471 / 0.1251\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.2597 / 0.3845\tMAE: 0.1213 / 0.1525\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.3530 / 0.2047\tMAE: 0.1471 / 0.1500\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.2808 / 0.2872\tMAE: 0.1231 / 0.1907\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.2900 / 0.2413\tMAE: 0.1395 / 0.1210\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.2606 / 0.2989\tMAE: 0.1283 / 0.1665\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.2370 / 0.3196\tMAE: 0.1226 / 0.1317\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.2592 / 0.5194\tMAE: 0.1170 / 0.1474\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.2977 / 0.2127\tMAE: 0.1341 / 0.1182\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.2499 / 0.2512\tMAE: 0.1162 / 0.1402\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.2846 / 0.3933\tMAE: 0.1259 / 0.1312\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.3007 / 0.2730\tMAE: 0.1247 / 0.1761\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.2385 / 0.2015\tMAE: 0.1166 / 0.1170\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.2594 / 0.3351\tMAE: 0.1102 / 0.1173\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.2811 / 0.3528\tMAE: 0.1300 / 0.1452\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.3291 / 0.2377\tMAE: 0.1364 / 0.1163\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.2909 / 0.1985\tMAE: 0.1273 / 0.1152\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.2296 / 0.2166\tMAE: 0.1181 / 0.1234\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.3987 / 0.5159\tMAE: 0.1630 / 0.2131\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.3103 / 0.3014\tMAE: 0.1234 / 0.1329\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.2774 / 0.2372\tMAE: 0.1189 / 0.1324\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.2382 / 0.2454\tMAE: 0.1077 / 0.1627\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.2946 / 0.3528\tMAE: 0.1471 / 0.1216\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.3164 / 0.2301\tMAE: 0.1252 / 0.1246\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.3117 / 0.2682\tMAE: 0.1382 / 0.1281\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.2468 / 0.2749\tMAE: 0.1105 / 0.1213\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.2846 / 0.3790\tMAE: 0.1144 / 0.1309\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.2403 / 0.2466\tMAE: 0.1183 / 0.1319\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.2204 / 0.2104\tMAE: 0.1133 / 0.1179\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.2569 / 0.2354\tMAE: 0.1222 / 0.1169\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.2603 / 0.2081\tMAE: 0.1126 / 0.1437\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.3238 / 0.2448\tMAE: 0.1464 / 0.1753\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.2079 / 0.2653\tMAE: 0.1122 / 0.1170\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.2286 / 0.2182\tMAE: 0.1092 / 0.1174\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.3443 / 0.2092\tMAE: 0.1546 / 0.1307\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.2971 / 0.2177\tMAE: 0.1294 / 0.1204\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.2324 / 0.3309\tMAE: 0.1136 / 0.1200\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.3316 / 0.3673\tMAE: 0.1322 / 0.2118\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.2790 / 0.2250\tMAE: 0.1305 / 0.1278\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.3636 / 0.4011\tMAE: 0.1434 / 0.1514\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.2701 / 0.2460\tMAE: 0.1258 / 0.1316\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.2311 / 0.2619\tMAE: 0.1088 / 0.1167\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.2188 / 0.2606\tMAE: 0.1110 / 0.1154\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.2133 / 0.1920\tMAE: 0.1068 / 0.1145\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.2783 / 0.2397\tMAE: 0.1214 / 0.1345\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.2493 / 0.4443\tMAE: 0.1078 / 0.1454\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.2397 / 0.2045\tMAE: 0.1143 / 0.1198\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.2423 / 0.4148\tMAE: 0.1197 / 0.1873\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.3561 / 0.3637\tMAE: 0.1378 / 0.4221\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.2403 / 0.3155\tMAE: 0.1522 / 0.1675\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.2300 / 0.2083\tMAE: 0.1132 / 0.1150\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.2114 / 0.3941\tMAE: 0.1116 / 0.1659\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.2712 / 0.2404\tMAE: 0.1228 / 0.2056\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.3023 / 0.2192\tMAE: 0.1271 / 0.1202\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.2178 / 0.2030\tMAE: 0.1123 / 0.1208\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.2325 / 0.3155\tMAE: 0.1173 / 0.1247\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.2379 / 0.2009\tMAE: 0.1109 / 0.1232\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.3431 / 0.4198\tMAE: 0.1634 / 0.2840\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.3205 / 0.2972\tMAE: 0.1429 / 0.1182\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.2560 / 0.1857\tMAE: 0.1145 / 0.1219\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.2771 / 0.6183\tMAE: 0.1331 / 0.1773\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.2849 / 0.2912\tMAE: 0.1357 / 0.1473\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.1935 / 0.1974\tMAE: 0.1106 / 0.1180\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.3038 / 0.4050\tMAE: 0.1236 / 0.1247\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.2479 / 0.2302\tMAE: 0.1310 / 0.1206\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.2137 / 0.2187\tMAE: 0.1157 / 0.1894\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.2526 / 0.2262\tMAE: 0.1304 / 0.1165\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.2176 / 0.2471\tMAE: 0.1137 / 0.1425\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.2080 / 0.2714\tMAE: 0.1242 / 0.1498\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.2619 / 0.3874\tMAE: 0.1322 / 0.1414\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.2318 / 0.2733\tMAE: 0.1268 / 0.1210\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.2674 / 0.4216\tMAE: 0.1227 / 0.2019\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.2909 / 0.3699\tMAE: 0.1240 / 0.2182\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2289 / 0.2300\tMAE: 0.1083 / 0.1142\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.2342 / 0.3349\tMAE: 0.1273 / 0.2191\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.2299 / 0.3169\tMAE: 0.1320 / 0.1690\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.1921 / 0.2625\tMAE: 0.1144 / 0.1222\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.2495 / 0.2401\tMAE: 0.1185 / 0.1152\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.1932 / 0.2530\tMAE: 0.1028 / 0.1268\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2692 / 0.2546\tMAE: 0.1181 / 0.1686\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.3339 / 0.4414\tMAE: 0.1165 / 0.1442\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.2804 / 0.2897\tMAE: 0.1180 / 0.1321\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.2113 / 0.5256\tMAE: 0.1160 / 0.1585\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.2051 / 0.1940\tMAE: 0.1135 / 0.1282\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.2536 / 0.2518\tMAE: 0.1068 / 0.1404\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.1813 / 0.2186\tMAE: 0.1133 / 0.1255\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.2160 / 0.6501\tMAE: 0.1098 / 0.1952\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2725 / 0.3316\tMAE: 0.1201 / 0.1476\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.2329 / 0.2584\tMAE: 0.1401 / 0.1433\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.1555 / 0.2422\tMAE: 0.0955 / 0.1424\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.1908 / 0.2121\tMAE: 0.1083 / 0.1305\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.2349 / 0.3940\tMAE: 0.1144 / 0.2513\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.2261 / 0.3287\tMAE: 0.1136 / 0.1284\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.2142 / 0.4660\tMAE: 0.1090 / 0.1709\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.2175 / 0.2674\tMAE: 0.1088 / 0.1318\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.2284 / 0.3055\tMAE: 0.1022 / 0.1149\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.2004 / 0.2500\tMAE: 0.1133 / 0.1367\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.1669 / 0.4434\tMAE: 0.0960 / 0.1307\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.2505 / 0.2833\tMAE: 0.1142 / 0.1229\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.2822 / 0.6796\tMAE: 0.1090 / 0.1623\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.1762 / 0.2675\tMAE: 0.0927 / 0.1155\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.2441 / 0.2823\tMAE: 0.1040 / 0.1196\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.2285 / 0.4756\tMAE: 0.1035 / 0.1234\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.2502 / 0.3337\tMAE: 0.1014 / 0.1240\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.2002 / 0.3533\tMAE: 0.1201 / 0.1326\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.2109 / 0.4893\tMAE: 0.1014 / 0.1769\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.3384 / 0.2344\tMAE: 0.1026 / 0.1339\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.2186 / 0.2132\tMAE: 0.1158 / 0.1742\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.1674 / 0.3071\tMAE: 0.1063 / 0.1149\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.1519 / 0.2123\tMAE: 0.0954 / 0.1181\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.1456 / 0.2284\tMAE: 0.0942 / 0.1249\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.1635 / 0.5684\tMAE: 0.0997 / 0.1644\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.2236 / 0.2276\tMAE: 0.1184 / 0.1167\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.2110 / 0.3230\tMAE: 0.1003 / 0.1166\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.1814 / 0.2572\tMAE: 0.1190 / 0.1545\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.2485 / 0.4365\tMAE: 0.1085 / 0.1219\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.2461 / 0.2627\tMAE: 0.0997 / 0.1183\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.1319 / 0.3949\tMAE: 0.0915 / 0.1235\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.1777 / 0.3138\tMAE: 0.1148 / 0.2180\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.1581 / 0.2901\tMAE: 0.1029 / 0.1586\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.1993 / 0.2458\tMAE: 0.1427 / 0.1433\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.2224 / 0.2046\tMAE: 0.1103 / 0.1144\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.1421 / 0.2172\tMAE: 0.0981 / 0.1305\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.1530 / 0.2446\tMAE: 0.0954 / 0.1807\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.1479 / 0.2035\tMAE: 0.1110 / 0.1240\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.1285 / 0.1937\tMAE: 0.0862 / 0.1308\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.1305 / 0.2791\tMAE: 0.0887 / 0.1261\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.1857 / 0.2588\tMAE: 0.0897 / 0.1229\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.1753 / 0.2309\tMAE: 0.1045 / 0.1191\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.1765 / 0.1819\tMAE: 0.1114 / 0.1317\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.2028 / 0.2064\tMAE: 0.0942 / 0.1301\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.1615 / 0.2079\tMAE: 0.0946 / 0.1353\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.2001 / 0.5488\tMAE: 0.0926 / 0.1476\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.2683 / 0.4035\tMAE: 0.0944 / 0.1156\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.1821 / 0.2019\tMAE: 0.0921 / 0.1159\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.1582 / 0.2517\tMAE: 0.0957 / 0.1228\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.1546 / 0.2051\tMAE: 0.0885 / 0.1213\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.1393 / 0.2442\tMAE: 0.0958 / 0.1176\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.1173 / 0.2112\tMAE: 0.0947 / 0.1329\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.1255 / 0.2064\tMAE: 0.1093 / 0.1461\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.2282 / 0.5798\tMAE: 0.1148 / 0.1253\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.1958 / 0.2286\tMAE: 0.0912 / 0.1139\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.1726 / 0.2625\tMAE: 0.0874 / 0.1187\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.1271 / 0.1927\tMAE: 0.0863 / 0.1390\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.1769 / 0.3625\tMAE: 0.1153 / 0.1533\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.1764 / 0.2254\tMAE: 0.0829 / 0.1196\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.1569 / 0.2008\tMAE: 0.0928 / 0.1239\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.1417 / 0.2366\tMAE: 0.1085 / 0.1832\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.1474 / 0.1760\tMAE: 0.1098 / 0.1161\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.1389 / 0.2077\tMAE: 0.1125 / 0.1360\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.1481 / 0.3558\tMAE: 0.0846 / 0.1205\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.1428 / 0.2595\tMAE: 0.0835 / 0.1313\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.1488 / 0.1711\tMAE: 0.0755 / 0.1196\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.0969 / 0.2389\tMAE: 0.0936 / 0.1288\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.1294 / 0.2519\tMAE: 0.1259 / 0.1356\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.1196 / 0.2094\tMAE: 0.1003 / 0.1428\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.1320 / 0.3017\tMAE: 0.0866 / 0.1411\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.1725 / 0.2204\tMAE: 0.0873 / 0.1874\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.1129 / 0.1996\tMAE: 0.1066 / 0.1508\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.1556 / 0.3182\tMAE: 0.1123 / 0.1227\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.1313 / 0.1823\tMAE: 0.0962 / 0.1306\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.1873 / 0.2167\tMAE: 0.0926 / 0.1340\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.1044 / 0.2029\tMAE: 0.0994 / 0.1176\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.1320 / 0.1826\tMAE: 0.0954 / 0.1175\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.1053 / 0.2966\tMAE: 0.0912 / 0.1436\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.1221 / 0.2226\tMAE: 0.0963 / 0.1299\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.1271 / 0.1888\tMAE: 0.0912 / 0.1257\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.1311 / 0.1810\tMAE: 0.0922 / 0.1350\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.1109 / 0.2446\tMAE: 0.0933 / 0.2030\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.1277 / 0.2670\tMAE: 0.0953 / 0.1203\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.1300 / 0.2369\tMAE: 0.0894 / 0.1246\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1203 / 0.2418\tMAE: 0.1112 / 0.1438\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.0971 / 0.1955\tMAE: 0.0830 / 0.1151\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.0818 / 0.2290\tMAE: 0.0926 / 0.1296\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.1454 / 0.3495\tMAE: 0.0915 / 0.1325\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.1676 / 0.2138\tMAE: 0.1256 / 0.1605\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.0956 / 0.2249\tMAE: 0.1167 / 0.1324\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.0997 / 0.1882\tMAE: 0.1089 / 0.1121\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.1121 / 0.2483\tMAE: 0.0818 / 0.1318\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.1184 / 0.2792\tMAE: 0.1253 / 0.2217\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.1092 / 0.1980\tMAE: 0.1092 / 0.1170\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.0878 / 0.2363\tMAE: 0.0960 / 0.1944\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.0989 / 0.2060\tMAE: 0.1036 / 0.1317\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.1178 / 0.2455\tMAE: 0.0797 / 0.1385\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.1389 / 0.2317\tMAE: 0.0901 / 0.1602\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.0967 / 0.1849\tMAE: 0.1131 / 0.1271\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.1008 / 0.1922\tMAE: 0.1230 / 0.1678\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.0855 / 0.2070\tMAE: 0.0823 / 0.1357\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.0924 / 0.2351\tMAE: 0.0833 / 0.1416\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.1010 / 0.2445\tMAE: 0.1021 / 0.1268\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.0847 / 0.2391\tMAE: 0.1065 / 0.1181\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.0801 / 0.1797\tMAE: 0.0888 / 0.1362\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.1052 / 0.2267\tMAE: 0.1564 / 0.1203\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.0924 / 0.1976\tMAE: 0.1424 / 0.1369\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.0795 / 0.2527\tMAE: 0.1011 / 0.2203\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.0948 / 0.1712\tMAE: 0.1097 / 0.1155\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1074 / 0.2190\tMAE: 0.1964 / 0.2573\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.1026 / 0.1919\tMAE: 0.1478 / 0.1348\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.0877 / 0.2060\tMAE: 0.1244 / 0.2056\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.0836 / 0.2376\tMAE: 0.1218 / 0.2141\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.1003 / 0.2139\tMAE: 0.1240 / 0.1354\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.0795 / 0.2005\tMAE: 0.0864 / 0.1244\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.0971 / 0.1840\tMAE: 0.1157 / 0.1227\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.0812 / 0.1981\tMAE: 0.1272 / 0.1795\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.0906 / 0.1809\tMAE: 0.1225 / 0.1342\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.0810 / 0.2224\tMAE: 0.0888 / 0.1178\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.0795 / 0.1730\tMAE: 0.1016 / 0.1173\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.0776 / 0.2097\tMAE: 0.1075 / 0.2101\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.0797 / 0.1860\tMAE: 0.1260 / 0.1206\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.0754 / 0.1958\tMAE: 0.1210 / 0.1347\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.0673 / 0.1796\tMAE: 0.0952 / 0.1249\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.0661 / 0.1893\tMAE: 0.0895 / 0.1233\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.0711 / 0.1774\tMAE: 0.0873 / 0.1135\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.0637 / 0.2025\tMAE: 0.0890 / 0.1933\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.0745 / 0.2130\tMAE: 0.1131 / 0.2275\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.0757 / 0.1913\tMAE: 0.1302 / 0.1506\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.0659 / 0.1918\tMAE: 0.1057 / 0.1601\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.0762 / 0.1846\tMAE: 0.1203 / 0.1374\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.0673 / 0.1941\tMAE: 0.0956 / 0.1447\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.0717 / 0.1741\tMAE: 0.1115 / 0.1248\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.0662 / 0.1895\tMAE: 0.1047 / 0.1476\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.0640 / 0.2044\tMAE: 0.1016 / 0.1761\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.0691 / 0.1910\tMAE: 0.1198 / 0.1614\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.0718 / 0.1887\tMAE: 0.1272 / 0.1313\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.0710 / 0.1777\tMAE: 0.1297 / 0.1265\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.0602 / 0.1963\tMAE: 0.0942 / 0.1805\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.0675 / 0.2093\tMAE: 0.1108 / 0.2259\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.0784 / 0.2204\tMAE: 0.1458 / 0.2445\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.0786 / 0.2102\tMAE: 0.1416 / 0.2529\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.0705 / 0.1843\tMAE: 0.1212 / 0.1359\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.0999 / 0.2254\tMAE: 0.2185 / 0.2659\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.0844 / 0.1917\tMAE: 0.1662 / 0.1449\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.0639 / 0.1910\tMAE: 0.1043 / 0.1330\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.0619 / 0.1903\tMAE: 0.0934 / 0.1257\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.0611 / 0.1873\tMAE: 0.0928 / 0.1409\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.0614 / 0.1789\tMAE: 0.0987 / 0.1199\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.0634 / 0.1840\tMAE: 0.1062 / 0.1232\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.0683 / 0.1849\tMAE: 0.1205 / 0.1260\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.0528 / 0.1788\tMAE: 0.0716 / 0.1248\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.0586 / 0.1906\tMAE: 0.0796 / 0.1416\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.0651 / 0.1995\tMAE: 0.1023 / 0.1417\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.0658 / 0.1829\tMAE: 0.1139 / 0.1413\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.0588 / 0.1884\tMAE: 0.0954 / 0.1271\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.0562 / 0.2042\tMAE: 0.0784 / 0.2487\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.0814 / 0.1919\tMAE: 0.1539 / 0.1499\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.0712 / 0.1945\tMAE: 0.1246 / 0.1754\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.0660 / 0.2370\tMAE: 0.1088 / 0.3086\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.0875 / 0.1835\tMAE: 0.1794 / 0.1332\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.0609 / 0.1911\tMAE: 0.0929 / 0.1448\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.0683 / 0.1859\tMAE: 0.1172 / 0.1312\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.0582 / 0.1919\tMAE: 0.0905 / 0.1486\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.0565 / 0.1959\tMAE: 0.0877 / 0.1677\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.0559 / 0.2066\tMAE: 0.0864 / 0.2026\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.0606 / 0.1890\tMAE: 0.0944 / 0.1346\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.0555 / 0.1851\tMAE: 0.0822 / 0.1211\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.0637 / 0.2056\tMAE: 0.1041 / 0.1729\n",
      "d:/MODELS/202204/nmm\\M03R\\metal_TTT_00_log\n",
      "Epoch [1/300]\tTrain/Valid Loss: 3.4589 / 3.3509\tMAE: 1.3813 / 1.2457\n",
      "Epoch [2/300]\tTrain/Valid Loss: 2.4550 / 1.0780\tMAE: 0.8917 / 0.8035\n",
      "Epoch [3/300]\tTrain/Valid Loss: 0.5749 / 0.3288\tMAE: 0.4431 / 0.2725\n",
      "Epoch [4/300]\tTrain/Valid Loss: 0.2679 / 0.2427\tMAE: 0.2381 / 0.2131\n",
      "Epoch [5/300]\tTrain/Valid Loss: 0.2322 / 0.2248\tMAE: 0.1912 / 0.1730\n",
      "Epoch [6/300]\tTrain/Valid Loss: 0.2486 / 0.2215\tMAE: 0.1738 / 0.1686\n",
      "Epoch [7/300]\tTrain/Valid Loss: 0.2688 / 0.2155\tMAE: 0.1855 / 0.1760\n",
      "Epoch [8/300]\tTrain/Valid Loss: 0.2487 / 0.3029\tMAE: 0.1664 / 0.1586\n",
      "Epoch [9/300]\tTrain/Valid Loss: 0.2616 / 0.2259\tMAE: 0.1746 / 0.1882\n",
      "Epoch [10/300]\tTrain/Valid Loss: 0.2288 / 0.2570\tMAE: 0.1573 / 0.1745\n",
      "Epoch [11/300]\tTrain/Valid Loss: 0.2305 / 0.2097\tMAE: 0.1506 / 0.1464\n",
      "Epoch [12/300]\tTrain/Valid Loss: 0.2682 / 0.3902\tMAE: 0.1695 / 0.1568\n",
      "Epoch [13/300]\tTrain/Valid Loss: 0.2806 / 0.3291\tMAE: 0.1897 / 0.1949\n",
      "Epoch [14/300]\tTrain/Valid Loss: 0.3417 / 0.2344\tMAE: 0.2127 / 0.1682\n",
      "Epoch [15/300]\tTrain/Valid Loss: 0.2454 / 0.2272\tMAE: 0.1539 / 0.1485\n",
      "Epoch [16/300]\tTrain/Valid Loss: 0.2829 / 0.3208\tMAE: 0.1631 / 0.1569\n",
      "Epoch [17/300]\tTrain/Valid Loss: 0.2556 / 0.2284\tMAE: 0.1540 / 0.1429\n",
      "Epoch [18/300]\tTrain/Valid Loss: 0.2506 / 0.2259\tMAE: 0.1580 / 0.1524\n",
      "Epoch [19/300]\tTrain/Valid Loss: 0.2385 / 0.2826\tMAE: 0.1412 / 0.1410\n",
      "Epoch [20/300]\tTrain/Valid Loss: 0.2752 / 0.2603\tMAE: 0.1580 / 0.1439\n",
      "Epoch [21/300]\tTrain/Valid Loss: 0.3029 / 0.2136\tMAE: 0.1738 / 0.1744\n",
      "Epoch [22/300]\tTrain/Valid Loss: 0.2561 / 0.2599\tMAE: 0.1375 / 0.1515\n",
      "Epoch [23/300]\tTrain/Valid Loss: 0.2529 / 0.2586\tMAE: 0.1444 / 0.1423\n",
      "Epoch [24/300]\tTrain/Valid Loss: 0.2979 / 0.2418\tMAE: 0.1684 / 0.1594\n",
      "Epoch [25/300]\tTrain/Valid Loss: 0.2715 / 0.2957\tMAE: 0.1489 / 0.1392\n",
      "Epoch [26/300]\tTrain/Valid Loss: 0.2437 / 0.2712\tMAE: 0.1357 / 0.1363\n",
      "Epoch [27/300]\tTrain/Valid Loss: 0.2579 / 0.2286\tMAE: 0.1452 / 0.1463\n",
      "Epoch [28/300]\tTrain/Valid Loss: 0.2618 / 0.3931\tMAE: 0.1453 / 0.1606\n",
      "Epoch [29/300]\tTrain/Valid Loss: 0.2638 / 0.2413\tMAE: 0.1479 / 0.1360\n",
      "Epoch [30/300]\tTrain/Valid Loss: 0.2945 / 0.3575\tMAE: 0.1534 / 0.1739\n",
      "Epoch [31/300]\tTrain/Valid Loss: 0.3283 / 0.3852\tMAE: 0.1758 / 0.1424\n",
      "Epoch [32/300]\tTrain/Valid Loss: 0.3098 / 0.2781\tMAE: 0.1646 / 0.1470\n",
      "Epoch [33/300]\tTrain/Valid Loss: 0.2509 / 0.2388\tMAE: 0.1397 / 0.1631\n",
      "Epoch [34/300]\tTrain/Valid Loss: 0.3010 / 0.4102\tMAE: 0.1474 / 0.1427\n",
      "Epoch [35/300]\tTrain/Valid Loss: 0.4139 / 0.3498\tMAE: 0.1999 / 0.1974\n",
      "Epoch [36/300]\tTrain/Valid Loss: 0.2980 / 0.3234\tMAE: 0.1439 / 0.1372\n",
      "Epoch [37/300]\tTrain/Valid Loss: 0.2475 / 0.1912\tMAE: 0.1298 / 0.1329\n",
      "Epoch [38/300]\tTrain/Valid Loss: 0.2842 / 0.2382\tMAE: 0.1464 / 0.1668\n",
      "Epoch [39/300]\tTrain/Valid Loss: 0.2907 / 0.2492\tMAE: 0.1402 / 0.1611\n",
      "Epoch [40/300]\tTrain/Valid Loss: 0.3482 / 0.3035\tMAE: 0.1869 / 0.1400\n",
      "Epoch [41/300]\tTrain/Valid Loss: 0.3294 / 0.4959\tMAE: 0.1374 / 0.2073\n",
      "Epoch [42/300]\tTrain/Valid Loss: 0.2641 / 0.2255\tMAE: 0.1578 / 0.1316\n",
      "Epoch [43/300]\tTrain/Valid Loss: 0.3018 / 0.2114\tMAE: 0.1551 / 0.1940\n",
      "Epoch [44/300]\tTrain/Valid Loss: 0.2826 / 0.2405\tMAE: 0.1399 / 0.1375\n",
      "Epoch [45/300]\tTrain/Valid Loss: 0.2467 / 0.3151\tMAE: 0.1315 / 0.1407\n",
      "Epoch [46/300]\tTrain/Valid Loss: 0.2522 / 0.3209\tMAE: 0.1232 / 0.1734\n",
      "Epoch [47/300]\tTrain/Valid Loss: 0.3981 / 0.3651\tMAE: 0.1827 / 0.1706\n",
      "Epoch [48/300]\tTrain/Valid Loss: 0.3119 / 0.5229\tMAE: 0.1392 / 0.3640\n",
      "Epoch [49/300]\tTrain/Valid Loss: 0.3744 / 0.2482\tMAE: 0.1796 / 0.1426\n",
      "Epoch [50/300]\tTrain/Valid Loss: 0.2540 / 0.6226\tMAE: 0.1321 / 0.1340\n",
      "Epoch [51/300]\tTrain/Valid Loss: 0.2841 / 0.2330\tMAE: 0.1431 / 0.1372\n",
      "Epoch [52/300]\tTrain/Valid Loss: 0.2553 / 0.2035\tMAE: 0.1283 / 0.1409\n",
      "Epoch [53/300]\tTrain/Valid Loss: 0.2987 / 0.3591\tMAE: 0.1538 / 0.1985\n",
      "Epoch [54/300]\tTrain/Valid Loss: 0.2857 / 0.2949\tMAE: 0.1303 / 0.1553\n",
      "Epoch [55/300]\tTrain/Valid Loss: 0.2587 / 0.4480\tMAE: 0.1237 / 0.1361\n",
      "Epoch [56/300]\tTrain/Valid Loss: 0.4215 / 0.4171\tMAE: 0.1808 / 0.2757\n",
      "Epoch [57/300]\tTrain/Valid Loss: 0.2746 / 0.2357\tMAE: 0.1470 / 0.1314\n",
      "Epoch [58/300]\tTrain/Valid Loss: 0.2570 / 0.3052\tMAE: 0.1319 / 0.1611\n",
      "Epoch [59/300]\tTrain/Valid Loss: 0.2576 / 0.2377\tMAE: 0.1308 / 0.1332\n",
      "Epoch [60/300]\tTrain/Valid Loss: 0.2465 / 0.2860\tMAE: 0.1240 / 0.1340\n",
      "Epoch [61/300]\tTrain/Valid Loss: 0.2636 / 0.3163\tMAE: 0.1273 / 0.1394\n",
      "Epoch [62/300]\tTrain/Valid Loss: 0.2517 / 0.2747\tMAE: 0.1181 / 0.1250\n",
      "Epoch [63/300]\tTrain/Valid Loss: 0.2744 / 0.2679\tMAE: 0.1288 / 0.2344\n",
      "Epoch [64/300]\tTrain/Valid Loss: 0.2046 / 0.2272\tMAE: 0.1232 / 0.1437\n",
      "Epoch [65/300]\tTrain/Valid Loss: 0.2740 / 0.4734\tMAE: 0.1363 / 0.1592\n",
      "Epoch [66/300]\tTrain/Valid Loss: 0.2816 / 0.5605\tMAE: 0.1371 / 0.1966\n",
      "Epoch [67/300]\tTrain/Valid Loss: 0.2667 / 0.3794\tMAE: 0.1366 / 0.1387\n",
      "Epoch [68/300]\tTrain/Valid Loss: 0.2218 / 0.2982\tMAE: 0.1095 / 0.1341\n",
      "Epoch [69/300]\tTrain/Valid Loss: 0.2539 / 0.3361\tMAE: 0.1163 / 0.1396\n",
      "Epoch [70/300]\tTrain/Valid Loss: 0.2395 / 0.2879\tMAE: 0.1148 / 0.1409\n",
      "Epoch [71/300]\tTrain/Valid Loss: 0.2562 / 0.2562\tMAE: 0.1198 / 0.1280\n",
      "Epoch [72/300]\tTrain/Valid Loss: 0.2600 / 0.2282\tMAE: 0.1226 / 0.1525\n",
      "Epoch [73/300]\tTrain/Valid Loss: 0.2034 / 0.2902\tMAE: 0.1172 / 0.1309\n",
      "Epoch [74/300]\tTrain/Valid Loss: 0.2405 / 0.2401\tMAE: 0.1181 / 0.1327\n",
      "Epoch [75/300]\tTrain/Valid Loss: 0.2237 / 0.2835\tMAE: 0.1066 / 0.1774\n",
      "Epoch [76/300]\tTrain/Valid Loss: 0.2428 / 0.2318\tMAE: 0.1244 / 0.1381\n",
      "Epoch [77/300]\tTrain/Valid Loss: 0.2200 / 0.3972\tMAE: 0.1129 / 0.1648\n",
      "Epoch [78/300]\tTrain/Valid Loss: 0.3170 / 0.4115\tMAE: 0.1488 / 0.2417\n",
      "Epoch [79/300]\tTrain/Valid Loss: 0.2594 / 0.3399\tMAE: 0.1305 / 0.1462\n",
      "Epoch [80/300]\tTrain/Valid Loss: 0.1881 / 0.2642\tMAE: 0.1056 / 0.1305\n",
      "Epoch [81/300]\tTrain/Valid Loss: 0.1992 / 0.2834\tMAE: 0.1053 / 0.1505\n",
      "Epoch [82/300]\tTrain/Valid Loss: 0.2479 / 0.3342\tMAE: 0.1305 / 0.1488\n",
      "Epoch [83/300]\tTrain/Valid Loss: 0.2307 / 0.2337\tMAE: 0.1229 / 0.1307\n",
      "Epoch [84/300]\tTrain/Valid Loss: 0.1832 / 0.1917\tMAE: 0.1009 / 0.1329\n",
      "Epoch [85/300]\tTrain/Valid Loss: 0.2198 / 0.4051\tMAE: 0.1086 / 0.1743\n",
      "Epoch [86/300]\tTrain/Valid Loss: 0.2426 / 0.4749\tMAE: 0.1095 / 0.2910\n",
      "Epoch [87/300]\tTrain/Valid Loss: 0.2706 / 0.2801\tMAE: 0.1310 / 0.1802\n",
      "Epoch [88/300]\tTrain/Valid Loss: 0.3580 / 0.4168\tMAE: 0.1598 / 0.1553\n",
      "Epoch [89/300]\tTrain/Valid Loss: 0.2287 / 0.2461\tMAE: 0.1063 / 0.1347\n",
      "Epoch [90/300]\tTrain/Valid Loss: 0.2739 / 0.4535\tMAE: 0.1174 / 0.1868\n",
      "Epoch [91/300]\tTrain/Valid Loss: 0.2384 / 0.4231\tMAE: 0.1083 / 0.1381\n",
      "Epoch [92/300]\tTrain/Valid Loss: 0.2152 / 0.2167\tMAE: 0.1112 / 0.1252\n",
      "Epoch [93/300]\tTrain/Valid Loss: 0.2719 / 0.3786\tMAE: 0.1220 / 0.1303\n",
      "Epoch [94/300]\tTrain/Valid Loss: 0.1982 / 0.2883\tMAE: 0.1003 / 0.1386\n",
      "Epoch [95/300]\tTrain/Valid Loss: 0.1574 / 0.3205\tMAE: 0.0883 / 0.1213\n",
      "Epoch [96/300]\tTrain/Valid Loss: 0.1823 / 0.2794\tMAE: 0.0927 / 0.1338\n",
      "Epoch [97/300]\tTrain/Valid Loss: 0.2007 / 0.2710\tMAE: 0.0962 / 0.1277\n",
      "Epoch [98/300]\tTrain/Valid Loss: 0.2410 / 0.2671\tMAE: 0.1138 / 0.1355\n",
      "Epoch [99/300]\tTrain/Valid Loss: 0.2108 / 0.2910\tMAE: 0.1143 / 0.1695\n",
      "Epoch [100/300]\tTrain/Valid Loss: 0.1940 / 0.3626\tMAE: 0.1080 / 0.1227\n",
      "Epoch [101/300]\tTrain/Valid Loss: 0.1779 / 0.2029\tMAE: 0.0924 / 0.1268\n",
      "Epoch [102/300]\tTrain/Valid Loss: 0.2153 / 0.2291\tMAE: 0.1051 / 0.1327\n",
      "Epoch [103/300]\tTrain/Valid Loss: 0.1759 / 0.3766\tMAE: 0.0896 / 0.1650\n",
      "Epoch [104/300]\tTrain/Valid Loss: 0.2342 / 0.3148\tMAE: 0.1213 / 0.1631\n",
      "Epoch [105/300]\tTrain/Valid Loss: 0.1613 / 0.2083\tMAE: 0.1117 / 0.1319\n",
      "Epoch [106/300]\tTrain/Valid Loss: 0.2566 / 0.4199\tMAE: 0.1213 / 0.1929\n",
      "Epoch [107/300]\tTrain/Valid Loss: 0.2269 / 0.2377\tMAE: 0.1062 / 0.1641\n",
      "Epoch [108/300]\tTrain/Valid Loss: 0.2013 / 0.2461\tMAE: 0.0990 / 0.1306\n",
      "Epoch [109/300]\tTrain/Valid Loss: 0.1919 / 0.3229\tMAE: 0.0982 / 0.1276\n",
      "Epoch [110/300]\tTrain/Valid Loss: 0.1907 / 0.4511\tMAE: 0.1116 / 0.1398\n",
      "Epoch [111/300]\tTrain/Valid Loss: 0.2058 / 0.4349\tMAE: 0.0981 / 0.1341\n",
      "Epoch [112/300]\tTrain/Valid Loss: 0.1900 / 0.2155\tMAE: 0.0928 / 0.1458\n",
      "Epoch [113/300]\tTrain/Valid Loss: 0.2338 / 0.4626\tMAE: 0.0852 / 0.1262\n",
      "Epoch [114/300]\tTrain/Valid Loss: 0.1749 / 0.3825\tMAE: 0.0892 / 0.1687\n",
      "Epoch [115/300]\tTrain/Valid Loss: 0.1657 / 0.3536\tMAE: 0.0871 / 0.1403\n",
      "Epoch [116/300]\tTrain/Valid Loss: 0.2457 / 0.2497\tMAE: 0.1016 / 0.1564\n",
      "Epoch [117/300]\tTrain/Valid Loss: 0.1879 / 0.2406\tMAE: 0.1078 / 0.1321\n",
      "Epoch [118/300]\tTrain/Valid Loss: 0.2082 / 0.3118\tMAE: 0.1010 / 0.2000\n",
      "Epoch [119/300]\tTrain/Valid Loss: 0.2375 / 0.7092\tMAE: 0.1061 / 0.1587\n",
      "Epoch [120/300]\tTrain/Valid Loss: 0.2049 / 0.2832\tMAE: 0.0896 / 0.1625\n",
      "Epoch [121/300]\tTrain/Valid Loss: 0.2461 / 0.1949\tMAE: 0.1171 / 0.1384\n",
      "Epoch [122/300]\tTrain/Valid Loss: 0.1863 / 0.2429\tMAE: 0.0809 / 0.1228\n",
      "Epoch [123/300]\tTrain/Valid Loss: 0.2194 / 0.7546\tMAE: 0.0834 / 0.1318\n",
      "Epoch [124/300]\tTrain/Valid Loss: 0.2672 / 0.1873\tMAE: 0.0883 / 0.1236\n",
      "Epoch [125/300]\tTrain/Valid Loss: 0.1201 / 0.2425\tMAE: 0.0764 / 0.1337\n",
      "Epoch [126/300]\tTrain/Valid Loss: 0.1234 / 0.3884\tMAE: 0.0763 / 0.1245\n",
      "Epoch [127/300]\tTrain/Valid Loss: 0.1808 / 0.4479\tMAE: 0.0894 / 0.1447\n",
      "Epoch [128/300]\tTrain/Valid Loss: 0.1660 / 0.2492\tMAE: 0.0769 / 0.1330\n",
      "Epoch [129/300]\tTrain/Valid Loss: 0.1658 / 0.3409\tMAE: 0.0877 / 0.1355\n",
      "Epoch [130/300]\tTrain/Valid Loss: 0.2125 / 0.1990\tMAE: 0.1043 / 0.1258\n",
      "Epoch [131/300]\tTrain/Valid Loss: 0.1729 / 0.3441\tMAE: 0.0816 / 0.1659\n",
      "Epoch [132/300]\tTrain/Valid Loss: 0.2296 / 0.3257\tMAE: 0.1050 / 0.1796\n",
      "Epoch [133/300]\tTrain/Valid Loss: 0.1432 / 0.3051\tMAE: 0.0768 / 0.1358\n",
      "Epoch [134/300]\tTrain/Valid Loss: 0.1169 / 0.2264\tMAE: 0.0644 / 0.1370\n",
      "Epoch [135/300]\tTrain/Valid Loss: 0.1301 / 0.3811\tMAE: 0.0737 / 0.1222\n",
      "Epoch [136/300]\tTrain/Valid Loss: 0.1879 / 0.2374\tMAE: 0.0688 / 0.1207\n",
      "Epoch [137/300]\tTrain/Valid Loss: 0.1750 / 0.2720\tMAE: 0.0875 / 0.1228\n",
      "Epoch [138/300]\tTrain/Valid Loss: 0.1623 / 0.1936\tMAE: 0.0848 / 0.1238\n",
      "Epoch [139/300]\tTrain/Valid Loss: 0.2306 / 0.5774\tMAE: 0.0760 / 0.1221\n",
      "Epoch [140/300]\tTrain/Valid Loss: 0.1772 / 0.2178\tMAE: 0.0676 / 0.1206\n",
      "Epoch [141/300]\tTrain/Valid Loss: 0.2212 / 0.5232\tMAE: 0.0721 / 0.1236\n",
      "Epoch [142/300]\tTrain/Valid Loss: 0.2628 / 0.2111\tMAE: 0.0724 / 0.1262\n",
      "Epoch [143/300]\tTrain/Valid Loss: 0.2004 / 0.2389\tMAE: 0.1003 / 0.1410\n",
      "Epoch [144/300]\tTrain/Valid Loss: 0.2573 / 0.2536\tMAE: 0.0851 / 0.1433\n",
      "Epoch [145/300]\tTrain/Valid Loss: 0.1582 / 0.2925\tMAE: 0.0747 / 0.1462\n",
      "Epoch [146/300]\tTrain/Valid Loss: 0.1710 / 0.4333\tMAE: 0.0819 / 0.1760\n",
      "Epoch [147/300]\tTrain/Valid Loss: 0.1974 / 0.4284\tMAE: 0.0838 / 0.1342\n",
      "Epoch [148/300]\tTrain/Valid Loss: 0.2825 / 0.3935\tMAE: 0.0792 / 0.1235\n",
      "Epoch [149/300]\tTrain/Valid Loss: 0.1964 / 0.3054\tMAE: 0.0639 / 0.1260\n",
      "Epoch [150/300]\tTrain/Valid Loss: 0.1473 / 0.3714\tMAE: 0.0685 / 0.1254\n",
      "Epoch [151/300]\tTrain/Valid Loss: 0.1883 / 0.1912\tMAE: 0.0668 / 0.1271\n",
      "Epoch [152/300]\tTrain/Valid Loss: 0.1033 / 0.1852\tMAE: 0.0657 / 0.1233\n",
      "Epoch [153/300]\tTrain/Valid Loss: 0.1340 / 0.2057\tMAE: 0.0613 / 0.1371\n",
      "Epoch [154/300]\tTrain/Valid Loss: 0.1579 / 0.2866\tMAE: 0.0772 / 0.1353\n",
      "Epoch [155/300]\tTrain/Valid Loss: 0.1447 / 0.3942\tMAE: 0.0646 / 0.1773\n",
      "Epoch [156/300]\tTrain/Valid Loss: 0.2580 / 0.4188\tMAE: 0.0778 / 0.1698\n",
      "Epoch [157/300]\tTrain/Valid Loss: 0.1897 / 0.2804\tMAE: 0.0811 / 0.1466\n",
      "Epoch [158/300]\tTrain/Valid Loss: 0.1232 / 0.2728\tMAE: 0.0746 / 0.1337\n",
      "Epoch [159/300]\tTrain/Valid Loss: 0.1327 / 0.2485\tMAE: 0.0808 / 0.1380\n",
      "Epoch [160/300]\tTrain/Valid Loss: 0.1160 / 0.2888\tMAE: 0.0763 / 0.1226\n",
      "Epoch [161/300]\tTrain/Valid Loss: 0.1544 / 0.2083\tMAE: 0.0743 / 0.1451\n",
      "Epoch [162/300]\tTrain/Valid Loss: 0.1329 / 0.2119\tMAE: 0.0723 / 0.1208\n",
      "Epoch [163/300]\tTrain/Valid Loss: 0.1764 / 0.5234\tMAE: 0.0785 / 0.1404\n",
      "Epoch [164/300]\tTrain/Valid Loss: 0.2426 / 0.2047\tMAE: 0.0711 / 0.1232\n",
      "Epoch [165/300]\tTrain/Valid Loss: 0.1593 / 0.5426\tMAE: 0.0663 / 0.1526\n",
      "Epoch [166/300]\tTrain/Valid Loss: 0.1767 / 0.3650\tMAE: 0.0622 / 0.1220\n",
      "Epoch [167/300]\tTrain/Valid Loss: 0.2143 / 0.3558\tMAE: 0.0760 / 0.2235\n",
      "Epoch [168/300]\tTrain/Valid Loss: 0.1253 / 0.2126\tMAE: 0.0818 / 0.1668\n",
      "Epoch [169/300]\tTrain/Valid Loss: 0.1146 / 0.2031\tMAE: 0.0718 / 0.1274\n",
      "Epoch [170/300]\tTrain/Valid Loss: 0.1555 / 0.3221\tMAE: 0.0709 / 0.1878\n",
      "Epoch [171/300]\tTrain/Valid Loss: 0.1152 / 0.2030\tMAE: 0.0962 / 0.1389\n",
      "Epoch [172/300]\tTrain/Valid Loss: 0.1472 / 0.3729\tMAE: 0.0835 / 0.1657\n",
      "Epoch [173/300]\tTrain/Valid Loss: 0.1308 / 0.2420\tMAE: 0.0780 / 0.1324\n",
      "Epoch [174/300]\tTrain/Valid Loss: 0.1349 / 0.2763\tMAE: 0.0889 / 0.1400\n",
      "Epoch [175/300]\tTrain/Valid Loss: 0.1498 / 0.2474\tMAE: 0.0818 / 0.1923\n",
      "Epoch [176/300]\tTrain/Valid Loss: 0.1297 / 0.2181\tMAE: 0.1176 / 0.1262\n",
      "Epoch [177/300]\tTrain/Valid Loss: 0.1608 / 0.5622\tMAE: 0.0678 / 0.1238\n",
      "Epoch [178/300]\tTrain/Valid Loss: 0.1868 / 0.2822\tMAE: 0.0765 / 0.1490\n",
      "Epoch [179/300]\tTrain/Valid Loss: 0.1370 / 0.1972\tMAE: 0.0654 / 0.1244\n",
      "Epoch [180/300]\tTrain/Valid Loss: 0.1205 / 0.2534\tMAE: 0.0556 / 0.1429\n",
      "Epoch [181/300]\tTrain/Valid Loss: 0.1725 / 0.3839\tMAE: 0.0662 / 0.1260\n",
      "Epoch [182/300]\tTrain/Valid Loss: 0.1092 / 0.1950\tMAE: 0.0622 / 0.1277\n",
      "Epoch [183/300]\tTrain/Valid Loss: 0.1053 / 0.4143\tMAE: 0.0843 / 0.2006\n",
      "Epoch [184/300]\tTrain/Valid Loss: 0.1530 / 0.2350\tMAE: 0.1190 / 0.1986\n",
      "Epoch [185/300]\tTrain/Valid Loss: 0.1781 / 0.3066\tMAE: 0.1280 / 0.1534\n",
      "Epoch [186/300]\tTrain/Valid Loss: 0.1122 / 0.2365\tMAE: 0.0938 / 0.1375\n",
      "Epoch [187/300]\tTrain/Valid Loss: 0.1022 / 0.2543\tMAE: 0.0651 / 0.1267\n",
      "Epoch [188/300]\tTrain/Valid Loss: 0.1470 / 0.3460\tMAE: 0.0618 / 0.1249\n",
      "Epoch [189/300]\tTrain/Valid Loss: 0.1342 / 0.4824\tMAE: 0.0590 / 0.1369\n",
      "Epoch [190/300]\tTrain/Valid Loss: 0.1985 / 0.2523\tMAE: 0.0905 / 0.1524\n",
      "Epoch [191/300]\tTrain/Valid Loss: 0.1131 / 0.2686\tMAE: 0.0936 / 0.1890\n",
      "Epoch [192/300]\tTrain/Valid Loss: 0.1190 / 0.2274\tMAE: 0.0966 / 0.2050\n",
      "Epoch [193/300]\tTrain/Valid Loss: 0.0800 / 0.2225\tMAE: 0.0873 / 0.1219\n",
      "Epoch [194/300]\tTrain/Valid Loss: 0.1217 / 0.2431\tMAE: 0.0569 / 0.1298\n",
      "Epoch [195/300]\tTrain/Valid Loss: 0.1617 / 0.3779\tMAE: 0.0904 / 0.2402\n",
      "Epoch [196/300]\tTrain/Valid Loss: 0.1512 / 0.3000\tMAE: 0.0943 / 0.1747\n",
      "Epoch [197/300]\tTrain/Valid Loss: 0.1578 / 0.2481\tMAE: 0.0874 / 0.1252\n",
      "Epoch [198/300]\tTrain/Valid Loss: 0.0790 / 0.2405\tMAE: 0.0639 / 0.1307\n",
      "Epoch [199/300]\tTrain/Valid Loss: 0.1572 / 0.2584\tMAE: 0.0637 / 0.1256\n",
      "Epoch [200/300]\tTrain/Valid Loss: 0.0880 / 0.3269\tMAE: 0.0686 / 0.1591\n",
      "Epoch [201/300]\tTrain/Valid Loss: 0.1095 / 0.3802\tMAE: 0.0879 / 0.1272\n",
      "Epoch [202/300]\tTrain/Valid Loss: 0.1119 / 0.3054\tMAE: 0.0742 / 0.1465\n",
      "Epoch [203/300]\tTrain/Valid Loss: 0.0815 / 0.2228\tMAE: 0.0715 / 0.1235\n",
      "Epoch [204/300]\tTrain/Valid Loss: 0.0850 / 0.2288\tMAE: 0.0796 / 0.1569\n",
      "Epoch [205/300]\tTrain/Valid Loss: 0.0973 / 0.3082\tMAE: 0.1110 / 0.1892\n",
      "Epoch [206/300]\tTrain/Valid Loss: 0.1360 / 0.2113\tMAE: 0.0741 / 0.1281\n",
      "Epoch [207/300]\tTrain/Valid Loss: 0.0718 / 0.1904\tMAE: 0.0636 / 0.1228\n",
      "Epoch [208/300]\tTrain/Valid Loss: 0.1245 / 0.2354\tMAE: 0.0846 / 0.1230\n",
      "Epoch [209/300]\tTrain/Valid Loss: 0.1022 / 0.2694\tMAE: 0.0579 / 0.1234\n",
      "Epoch [210/300]\tTrain/Valid Loss: 0.1441 / 0.2323\tMAE: 0.0887 / 0.1279\n",
      "Epoch [211/300]\tTrain/Valid Loss: 0.1018 / 0.2142\tMAE: 0.0946 / 0.1310\n",
      "Epoch [212/300]\tTrain/Valid Loss: 0.0811 / 0.2513\tMAE: 0.0655 / 0.1287\n",
      "Epoch [213/300]\tTrain/Valid Loss: 0.0948 / 0.3376\tMAE: 0.0750 / 0.1247\n",
      "Epoch [214/300]\tTrain/Valid Loss: 0.1178 / 0.2531\tMAE: 0.0659 / 0.1264\n",
      "Epoch [215/300]\tTrain/Valid Loss: 0.0879 / 0.2774\tMAE: 0.0881 / 0.2842\n",
      "Epoch [216/300]\tTrain/Valid Loss: 0.1475 / 0.2501\tMAE: 0.1059 / 0.1354\n",
      "Epoch [217/300]\tTrain/Valid Loss: 0.0945 / 0.2378\tMAE: 0.0846 / 0.1492\n",
      "Epoch [218/300]\tTrain/Valid Loss: 0.1732 / 0.2091\tMAE: 0.0772 / 0.1274\n",
      "Epoch [219/300]\tTrain/Valid Loss: 0.0761 / 0.2719\tMAE: 0.0860 / 0.1315\n",
      "Epoch [220/300]\tTrain/Valid Loss: 0.0967 / 0.2408\tMAE: 0.0828 / 0.1558\n",
      "Epoch [221/300]\tTrain/Valid Loss: 0.0698 / 0.2036\tMAE: 0.0997 / 0.1857\n",
      "Epoch [222/300]\tTrain/Valid Loss: 0.0879 / 0.3258\tMAE: 0.0887 / 0.1239\n",
      "Epoch [223/300]\tTrain/Valid Loss: 0.1250 / 0.2788\tMAE: 0.1391 / 0.2471\n",
      "Epoch [224/300]\tTrain/Valid Loss: 0.0972 / 0.2827\tMAE: 0.0994 / 0.1362\n",
      "Epoch [225/300]\tTrain/Valid Loss: 0.0908 / 0.2511\tMAE: 0.0950 / 0.1309\n",
      "Epoch [226/300]\tTrain/Valid Loss: 0.1153 / 0.2608\tMAE: 0.1907 / 0.1536\n",
      "Epoch [227/300]\tTrain/Valid Loss: 0.0941 / 0.2154\tMAE: 0.0974 / 0.1368\n",
      "Epoch [228/300]\tTrain/Valid Loss: 0.1088 / 0.1917\tMAE: 0.0784 / 0.1344\n",
      "Epoch [229/300]\tTrain/Valid Loss: 0.1241 / 0.1896\tMAE: 0.0770 / 0.1305\n",
      "Epoch [230/300]\tTrain/Valid Loss: 0.0906 / 0.2896\tMAE: 0.0760 / 0.1719\n",
      "Epoch [231/300]\tTrain/Valid Loss: 0.1020 / 0.2058\tMAE: 0.0787 / 0.1766\n",
      "Epoch [232/300]\tTrain/Valid Loss: 0.0854 / 0.2122\tMAE: 0.0589 / 0.1546\n",
      "Epoch [233/300]\tTrain/Valid Loss: 0.0844 / 0.2113\tMAE: 0.0819 / 0.1288\n",
      "Epoch [234/300]\tTrain/Valid Loss: 0.0749 / 0.1867\tMAE: 0.1039 / 0.1306\n",
      "Epoch [235/300]\tTrain/Valid Loss: 0.0876 / 0.1873\tMAE: 0.0850 / 0.1302\n",
      "Epoch [236/300]\tTrain/Valid Loss: 0.0707 / 0.2142\tMAE: 0.0662 / 0.1294\n",
      "Epoch [237/300]\tTrain/Valid Loss: 0.0771 / 0.2065\tMAE: 0.0965 / 0.1432\n",
      "Epoch [238/300]\tTrain/Valid Loss: 0.0802 / 0.1833\tMAE: 0.0611 / 0.1235\n",
      "Epoch [239/300]\tTrain/Valid Loss: 0.0919 / 0.2701\tMAE: 0.0802 / 0.1474\n",
      "Epoch [240/300]\tTrain/Valid Loss: 0.0889 / 0.2489\tMAE: 0.1168 / 0.2028\n",
      "Epoch [241/300]\tTrain/Valid Loss: 0.1163 / 0.1950\tMAE: 0.1678 / 0.1308\n",
      "Epoch [242/300]\tTrain/Valid Loss: 0.0689 / 0.2097\tMAE: 0.0636 / 0.1283\n",
      "Epoch [243/300]\tTrain/Valid Loss: 0.0604 / 0.2134\tMAE: 0.0792 / 0.1270\n",
      "Epoch [244/300]\tTrain/Valid Loss: 0.0746 / 0.2034\tMAE: 0.0864 / 0.1694\n",
      "Epoch [245/300]\tTrain/Valid Loss: 0.0849 / 0.2473\tMAE: 0.1181 / 0.2757\n",
      "Epoch [246/300]\tTrain/Valid Loss: 0.0832 / 0.2474\tMAE: 0.1515 / 0.3072\n",
      "Epoch [247/300]\tTrain/Valid Loss: 0.0827 / 0.2416\tMAE: 0.1362 / 0.1444\n",
      "Epoch [248/300]\tTrain/Valid Loss: 0.0748 / 0.2222\tMAE: 0.0757 / 0.1344\n",
      "Epoch [249/300]\tTrain/Valid Loss: 0.0585 / 0.2041\tMAE: 0.0811 / 0.1278\n",
      "Epoch [250/300]\tTrain/Valid Loss: 0.0636 / 0.2204\tMAE: 0.0714 / 0.1285\n",
      "Epoch [251/300]\tTrain/Valid Loss: 0.0715 / 0.2227\tMAE: 0.0784 / 0.1297\n",
      "Epoch [252/300]\tTrain/Valid Loss: 0.0772 / 0.2196\tMAE: 0.0951 / 0.1678\n",
      "Epoch [253/300]\tTrain/Valid Loss: 0.0801 / 0.2285\tMAE: 0.0988 / 0.1295\n",
      "Epoch [254/300]\tTrain/Valid Loss: 0.0804 / 0.2154\tMAE: 0.1201 / 0.1902\n",
      "Epoch [255/300]\tTrain/Valid Loss: 0.0664 / 0.2161\tMAE: 0.0765 / 0.1419\n",
      "Epoch [256/300]\tTrain/Valid Loss: 0.0663 / 0.1954\tMAE: 0.0788 / 0.1333\n",
      "Epoch [257/300]\tTrain/Valid Loss: 0.0782 / 0.2307\tMAE: 0.1410 / 0.2598\n",
      "Epoch [258/300]\tTrain/Valid Loss: 0.0802 / 0.2218\tMAE: 0.1134 / 0.2452\n",
      "Epoch [259/300]\tTrain/Valid Loss: 0.0653 / 0.2026\tMAE: 0.0923 / 0.1460\n",
      "Epoch [260/300]\tTrain/Valid Loss: 0.0707 / 0.2074\tMAE: 0.1305 / 0.1375\n",
      "Epoch [261/300]\tTrain/Valid Loss: 0.0579 / 0.2096\tMAE: 0.1023 / 0.1477\n",
      "Epoch [262/300]\tTrain/Valid Loss: 0.0486 / 0.2050\tMAE: 0.0647 / 0.1369\n",
      "Epoch [263/300]\tTrain/Valid Loss: 0.0610 / 0.2648\tMAE: 0.0992 / 0.3690\n",
      "Epoch [264/300]\tTrain/Valid Loss: 0.0960 / 0.2070\tMAE: 0.2010 / 0.1691\n",
      "Epoch [265/300]\tTrain/Valid Loss: 0.1119 / 0.2018\tMAE: 0.2562 / 0.1459\n",
      "Epoch [266/300]\tTrain/Valid Loss: 0.0565 / 0.1941\tMAE: 0.0968 / 0.1372\n",
      "Epoch [267/300]\tTrain/Valid Loss: 0.0671 / 0.2274\tMAE: 0.1349 / 0.2675\n",
      "Epoch [268/300]\tTrain/Valid Loss: 0.0681 / 0.2062\tMAE: 0.1387 / 0.1565\n",
      "Epoch [269/300]\tTrain/Valid Loss: 0.0882 / 0.2032\tMAE: 0.1971 / 0.1525\n",
      "Epoch [270/300]\tTrain/Valid Loss: 0.0710 / 0.1988\tMAE: 0.1513 / 0.1317\n",
      "Epoch [271/300]\tTrain/Valid Loss: 0.0482 / 0.1972\tMAE: 0.0759 / 0.1567\n",
      "Epoch [272/300]\tTrain/Valid Loss: 0.0502 / 0.2046\tMAE: 0.0930 / 0.1250\n",
      "Epoch [273/300]\tTrain/Valid Loss: 0.0603 / 0.2206\tMAE: 0.1245 / 0.2181\n",
      "Epoch [274/300]\tTrain/Valid Loss: 0.0490 / 0.1964\tMAE: 0.0937 / 0.1535\n",
      "Epoch [275/300]\tTrain/Valid Loss: 0.0620 / 0.2013\tMAE: 0.1221 / 0.1737\n",
      "Epoch [276/300]\tTrain/Valid Loss: 0.0635 / 0.2805\tMAE: 0.1361 / 0.3278\n",
      "Epoch [277/300]\tTrain/Valid Loss: 0.0633 / 0.1977\tMAE: 0.1376 / 0.1606\n",
      "Epoch [278/300]\tTrain/Valid Loss: 0.0534 / 0.1875\tMAE: 0.1107 / 0.1270\n",
      "Epoch [279/300]\tTrain/Valid Loss: 0.0406 / 0.1926\tMAE: 0.0726 / 0.1474\n",
      "Epoch [280/300]\tTrain/Valid Loss: 0.0584 / 0.1943\tMAE: 0.1304 / 0.1331\n",
      "Epoch [281/300]\tTrain/Valid Loss: 0.0540 / 0.1842\tMAE: 0.1117 / 0.1280\n",
      "Epoch [282/300]\tTrain/Valid Loss: 0.0628 / 0.2264\tMAE: 0.1402 / 0.2659\n",
      "Epoch [283/300]\tTrain/Valid Loss: 0.0563 / 0.2103\tMAE: 0.1174 / 0.1938\n",
      "Epoch [284/300]\tTrain/Valid Loss: 0.0465 / 0.2003\tMAE: 0.0934 / 0.1715\n",
      "Epoch [285/300]\tTrain/Valid Loss: 0.0792 / 0.2241\tMAE: 0.1922 / 0.2803\n",
      "Epoch [286/300]\tTrain/Valid Loss: 0.0459 / 0.2188\tMAE: 0.0860 / 0.2213\n",
      "Epoch [287/300]\tTrain/Valid Loss: 0.0492 / 0.1991\tMAE: 0.0944 / 0.1428\n",
      "Epoch [288/300]\tTrain/Valid Loss: 0.0549 / 0.2028\tMAE: 0.1074 / 0.1857\n",
      "Epoch [289/300]\tTrain/Valid Loss: 0.0613 / 0.2132\tMAE: 0.1340 / 0.2126\n",
      "Epoch [290/300]\tTrain/Valid Loss: 0.0616 / 0.1887\tMAE: 0.1348 / 0.1338\n",
      "Epoch [291/300]\tTrain/Valid Loss: 0.0505 / 0.2060\tMAE: 0.1057 / 0.1584\n",
      "Epoch [292/300]\tTrain/Valid Loss: 0.0524 / 0.2154\tMAE: 0.1115 / 0.2009\n",
      "Epoch [293/300]\tTrain/Valid Loss: 0.0582 / 0.1884\tMAE: 0.1310 / 0.1273\n",
      "Epoch [294/300]\tTrain/Valid Loss: 0.0478 / 0.2009\tMAE: 0.0975 / 0.1763\n",
      "Epoch [295/300]\tTrain/Valid Loss: 0.0480 / 0.2056\tMAE: 0.0899 / 0.1470\n",
      "Epoch [296/300]\tTrain/Valid Loss: 0.0517 / 0.2008\tMAE: 0.1063 / 0.1399\n",
      "Epoch [297/300]\tTrain/Valid Loss: 0.0415 / 0.2078\tMAE: 0.0676 / 0.1879\n",
      "Epoch [298/300]\tTrain/Valid Loss: 0.0552 / 0.1896\tMAE: 0.1209 / 0.1321\n",
      "Epoch [299/300]\tTrain/Valid Loss: 0.0547 / 0.1915\tMAE: 0.1125 / 0.1363\n",
      "Epoch [300/300]\tTrain/Valid Loss: 0.0543 / 0.2206\tMAE: 0.1114 / 0.2284\n"
     ]
    }
   ],
   "source": [
    "from model.model_03r import DistNN\n",
    "import util.trainer_log as tr\n",
    "\n",
    "for scale in ['metal_FFF','metal_TFF','metal_TTT']:\n",
    "    exec_model(scale=scale, model_type='M03R', comment='log', batch_size=256)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "cfb2e760b55082f7e18274ad9b6beeb89af4df0a5c88a9ce379e413b137aeb47"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('ex01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
