{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, gc, os\n",
    "from tgnn.util import crystal_conv as cc\n",
    "import tgnn.util.trainer as tr\n",
    "import torch.nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tgnn.model.TGNN import TGNN, TGNNX\n",
    "from tgnn.model.CGCNN import CGCNN, CGCNNR\n",
    "from tgnn.util.AdaBound import AdaBound\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "root_data  = 'c:/WORKSPACE_KRICT/DATA/data_snu/with_metal'\n",
    "root_model = 'd:/MODELS/202204/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10387/10387 [01:59<00:00, 87.04it/s] \n",
      "c:\\WORKSPACE_KRICT\\CODES\\band_gap_model\\notebooks\\tgnn\\util\\AdaBound.py:91: UserWarning: This overload of add is deprecated:\n",
      "\tadd(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd(Tensor other, *, Number alpha) (Triggered internally at  ..\\torch\\csrc\\utils\\python_arg_parser.cpp:1050.)\n",
      "  grad = grad.add(group['weight_decay'], p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\tTrain loss: 2.0936\tVal loss: 0.9687 (1.3683)\n",
      "Epoch [2/300]\tTrain loss: 0.8684\tVal loss: 0.8347 (1.3832)\n",
      "Epoch [3/300]\tTrain loss: 0.8185\tVal loss: 0.8114 (1.3059)\n",
      "Epoch [4/300]\tTrain loss: 0.7947\tVal loss: 0.7808 (1.6831)\n",
      "Epoch [5/300]\tTrain loss: 0.7741\tVal loss: 0.7888 (1.9485)\n",
      "Epoch [6/300]\tTrain loss: 0.7769\tVal loss: 0.7653 (1.7679)\n",
      "Epoch [7/300]\tTrain loss: 0.7562\tVal loss: 0.7515 (1.7169)\n",
      "Epoch [8/300]\tTrain loss: 0.7594\tVal loss: 0.7422 (1.5756)\n",
      "Epoch [9/300]\tTrain loss: 0.7451\tVal loss: 0.7331 (1.5884)\n",
      "Epoch [10/300]\tTrain loss: 0.7248\tVal loss: 0.8297 (1.9085)\n",
      "Epoch [11/300]\tTrain loss: 0.7326\tVal loss: 0.7419 (1.4234)\n",
      "Epoch [12/300]\tTrain loss: 0.7185\tVal loss: 0.8014 (1.5301)\n",
      "Epoch [13/300]\tTrain loss: 0.7148\tVal loss: 0.7089 (1.5828)\n",
      "Epoch [14/300]\tTrain loss: 0.7298\tVal loss: 0.7122 (1.4610)\n",
      "Epoch [15/300]\tTrain loss: 0.7009\tVal loss: 0.8394 (1.4919)\n",
      "Epoch [16/300]\tTrain loss: 0.6960\tVal loss: 0.7229 (1.4868)\n",
      "Epoch [17/300]\tTrain loss: 0.6879\tVal loss: 0.7276 (1.4352)\n",
      "Epoch [18/300]\tTrain loss: 0.7064\tVal loss: 0.6994 (1.6136)\n",
      "Epoch [19/300]\tTrain loss: 0.6705\tVal loss: 0.6890 (1.4034)\n",
      "Epoch [20/300]\tTrain loss: 0.6829\tVal loss: 0.6906 (1.4387)\n",
      "Epoch [21/300]\tTrain loss: 0.6861\tVal loss: 0.7240 (1.5615)\n",
      "Epoch [22/300]\tTrain loss: 0.7086\tVal loss: 0.8540 (2.0630)\n",
      "Epoch [23/300]\tTrain loss: 0.6518\tVal loss: 0.6856 (1.4372)\n",
      "Epoch [24/300]\tTrain loss: 0.6656\tVal loss: 0.6880 (1.7156)\n",
      "Epoch [25/300]\tTrain loss: 0.6488\tVal loss: 0.6797 (1.5585)\n",
      "Epoch [26/300]\tTrain loss: 0.6418\tVal loss: 0.6924 (1.5578)\n",
      "Epoch [27/300]\tTrain loss: 0.6378\tVal loss: 0.9873 (1.9563)\n",
      "Epoch [28/300]\tTrain loss: 0.6378\tVal loss: 0.6255 (1.4216)\n",
      "Epoch [29/300]\tTrain loss: 0.6409\tVal loss: 0.6765 (1.1360)\n",
      "Epoch [30/300]\tTrain loss: 0.6335\tVal loss: 0.6858 (1.3472)\n",
      "Epoch [31/300]\tTrain loss: 0.6382\tVal loss: 0.6798 (1.4658)\n",
      "Epoch [32/300]\tTrain loss: 0.6409\tVal loss: 0.6546 (1.4681)\n",
      "Epoch [33/300]\tTrain loss: 0.6550\tVal loss: 0.6489 (1.5487)\n",
      "Epoch [34/300]\tTrain loss: 0.6430\tVal loss: 0.6776 (1.5648)\n",
      "Epoch [35/300]\tTrain loss: 0.6308\tVal loss: 0.6854 (1.4935)\n",
      "Epoch [36/300]\tTrain loss: 0.6246\tVal loss: 0.6463 (1.4548)\n",
      "Epoch [37/300]\tTrain loss: 0.6295\tVal loss: 0.6396 (1.4631)\n",
      "Epoch [38/300]\tTrain loss: 0.5997\tVal loss: 0.6772 (1.6209)\n",
      "Epoch [39/300]\tTrain loss: 0.6061\tVal loss: 0.6672 (1.4946)\n",
      "Epoch [40/300]\tTrain loss: 0.5923\tVal loss: 0.6893 (1.2631)\n",
      "Epoch [41/300]\tTrain loss: 0.6016\tVal loss: 0.7864 (1.4921)\n",
      "Epoch [42/300]\tTrain loss: 0.6057\tVal loss: 0.6709 (1.5083)\n",
      "Epoch [43/300]\tTrain loss: 0.5888\tVal loss: 0.7121 (1.3797)\n",
      "Epoch [44/300]\tTrain loss: 0.5841\tVal loss: 0.6445 (1.3407)\n",
      "Epoch [45/300]\tTrain loss: 0.6019\tVal loss: 0.6573 (1.2774)\n",
      "Epoch [46/300]\tTrain loss: 0.6034\tVal loss: 0.7039 (1.2101)\n",
      "Epoch [47/300]\tTrain loss: 0.6717\tVal loss: 0.8394 (1.9045)\n",
      "Epoch [48/300]\tTrain loss: 0.5888\tVal loss: 0.6009 (1.3116)\n",
      "Epoch [49/300]\tTrain loss: 0.5684\tVal loss: 0.6199 (1.4551)\n",
      "Epoch [50/300]\tTrain loss: 0.5784\tVal loss: 0.7985 (1.2847)\n",
      "Epoch [51/300]\tTrain loss: 0.5933\tVal loss: 0.6456 (1.4034)\n",
      "Epoch [52/300]\tTrain loss: 0.5577\tVal loss: 0.5963 (1.4120)\n",
      "Epoch [53/300]\tTrain loss: 0.5999\tVal loss: 0.6367 (1.2780)\n",
      "Epoch [54/300]\tTrain loss: 0.5704\tVal loss: 0.6157 (1.1804)\n",
      "Epoch [55/300]\tTrain loss: 0.5601\tVal loss: 0.6084 (1.2345)\n",
      "Epoch [56/300]\tTrain loss: 0.5737\tVal loss: 0.6687 (1.6047)\n",
      "Epoch [57/300]\tTrain loss: 0.5531\tVal loss: 0.6541 (1.3049)\n",
      "Epoch [58/300]\tTrain loss: 0.5522\tVal loss: 0.6031 (1.1327)\n",
      "Epoch [59/300]\tTrain loss: 0.5650\tVal loss: 0.6592 (1.5698)\n",
      "Epoch [60/300]\tTrain loss: 0.5572\tVal loss: 0.6088 (1.4127)\n",
      "Epoch [61/300]\tTrain loss: 0.5572\tVal loss: 0.6093 (1.3364)\n",
      "Epoch [62/300]\tTrain loss: 0.5482\tVal loss: 0.7853 (1.7225)\n",
      "Epoch [63/300]\tTrain loss: 0.5448\tVal loss: 0.5975 (1.2276)\n",
      "Epoch [64/300]\tTrain loss: 0.5218\tVal loss: 0.5913 (1.4159)\n",
      "Epoch [65/300]\tTrain loss: 0.5389\tVal loss: 0.5844 (1.4696)\n",
      "Epoch [66/300]\tTrain loss: 0.5305\tVal loss: 0.5791 (1.1769)\n",
      "Epoch [67/300]\tTrain loss: 0.5406\tVal loss: 0.5839 (1.3280)\n",
      "Epoch [68/300]\tTrain loss: 0.5621\tVal loss: 0.6881 (1.5382)\n",
      "Epoch [69/300]\tTrain loss: 0.5417\tVal loss: 0.6149 (1.1874)\n",
      "Epoch [70/300]\tTrain loss: 0.5597\tVal loss: 0.6662 (1.4645)\n",
      "Epoch [71/300]\tTrain loss: 0.5202\tVal loss: 0.5945 (1.3860)\n",
      "Epoch [72/300]\tTrain loss: 0.5530\tVal loss: 0.5810 (1.2050)\n",
      "Epoch [73/300]\tTrain loss: 0.5412\tVal loss: 0.5902 (1.2646)\n",
      "Epoch [74/300]\tTrain loss: 0.5062\tVal loss: 0.6077 (1.2548)\n",
      "Epoch [75/300]\tTrain loss: 0.5153\tVal loss: 0.5859 (1.2093)\n",
      "Epoch [76/300]\tTrain loss: 0.5358\tVal loss: 0.6009 (1.1199)\n",
      "Epoch [77/300]\tTrain loss: 0.5261\tVal loss: 0.5868 (1.3048)\n",
      "Epoch [78/300]\tTrain loss: 0.5078\tVal loss: 0.5680 (1.1854)\n",
      "Epoch [79/300]\tTrain loss: 0.5147\tVal loss: 0.6598 (1.3674)\n",
      "Epoch [80/300]\tTrain loss: 0.5070\tVal loss: 0.6185 (1.4199)\n",
      "Epoch [81/300]\tTrain loss: 0.4995\tVal loss: 0.5795 (1.2665)\n",
      "Epoch [82/300]\tTrain loss: 0.5102\tVal loss: 0.6266 (1.0800)\n",
      "Epoch [83/300]\tTrain loss: 0.5145\tVal loss: 0.5565 (1.2341)\n",
      "Epoch [84/300]\tTrain loss: 0.4854\tVal loss: 0.5852 (1.2100)\n",
      "Epoch [85/300]\tTrain loss: 0.4873\tVal loss: 0.5589 (1.2869)\n",
      "Epoch [86/300]\tTrain loss: 0.4872\tVal loss: 0.5611 (1.2815)\n",
      "Epoch [87/300]\tTrain loss: 0.4973\tVal loss: 0.5655 (1.2284)\n",
      "Epoch [88/300]\tTrain loss: 0.4940\tVal loss: 0.5804 (1.3436)\n",
      "Epoch [89/300]\tTrain loss: 0.4816\tVal loss: 0.5912 (1.2257)\n",
      "Epoch [90/300]\tTrain loss: 0.4873\tVal loss: 0.5889 (1.4005)\n",
      "Epoch [91/300]\tTrain loss: 0.4765\tVal loss: 0.5787 (1.2956)\n",
      "Epoch [92/300]\tTrain loss: 0.4776\tVal loss: 0.5744 (1.2542)\n",
      "Epoch [93/300]\tTrain loss: 0.4699\tVal loss: 0.6152 (1.2785)\n",
      "Epoch [94/300]\tTrain loss: 0.4633\tVal loss: 0.5491 (1.1205)\n",
      "Epoch [95/300]\tTrain loss: 0.4895\tVal loss: 0.5803 (1.2421)\n",
      "Epoch [96/300]\tTrain loss: 0.4581\tVal loss: 0.5784 (1.0288)\n",
      "Epoch [97/300]\tTrain loss: 0.4754\tVal loss: 0.5737 (1.2537)\n",
      "Epoch [98/300]\tTrain loss: 0.4593\tVal loss: 0.5630 (1.0427)\n",
      "Epoch [99/300]\tTrain loss: 0.4648\tVal loss: 0.5713 (1.1484)\n",
      "Epoch [100/300]\tTrain loss: 0.4542\tVal loss: 0.5827 (1.3996)\n",
      "Epoch [101/300]\tTrain loss: 0.4620\tVal loss: 0.5906 (1.1374)\n",
      "Epoch [102/300]\tTrain loss: 0.4529\tVal loss: 0.5831 (1.2401)\n",
      "Epoch [103/300]\tTrain loss: 0.4703\tVal loss: 0.6170 (1.4580)\n",
      "Epoch [104/300]\tTrain loss: 0.4650\tVal loss: 0.6116 (1.2945)\n",
      "Epoch [105/300]\tTrain loss: 0.4686\tVal loss: 0.5802 (1.3293)\n",
      "Epoch [106/300]\tTrain loss: 0.4705\tVal loss: 0.5790 (1.2306)\n",
      "Epoch [107/300]\tTrain loss: 0.4497\tVal loss: 0.5564 (1.1199)\n",
      "Epoch [108/300]\tTrain loss: 0.4419\tVal loss: 0.6809 (1.0576)\n",
      "Epoch [109/300]\tTrain loss: 0.4464\tVal loss: 0.5522 (1.3279)\n",
      "Epoch [110/300]\tTrain loss: 0.4443\tVal loss: 0.5761 (1.2629)\n",
      "Epoch [111/300]\tTrain loss: 0.4690\tVal loss: 0.5460 (1.1050)\n",
      "Epoch [112/300]\tTrain loss: 0.4445\tVal loss: 0.5712 (1.3847)\n",
      "Epoch [113/300]\tTrain loss: 0.4360\tVal loss: 0.5396 (1.1398)\n",
      "Epoch [114/300]\tTrain loss: 0.4350\tVal loss: 0.5536 (1.2315)\n",
      "Epoch [115/300]\tTrain loss: 0.4359\tVal loss: 0.5717 (1.2830)\n",
      "Epoch [116/300]\tTrain loss: 0.4384\tVal loss: 0.6310 (1.3863)\n",
      "Epoch [117/300]\tTrain loss: 0.4502\tVal loss: 0.5457 (1.2750)\n",
      "Epoch [118/300]\tTrain loss: 0.4293\tVal loss: 0.5634 (1.1062)\n",
      "Epoch [119/300]\tTrain loss: 0.4383\tVal loss: 0.6139 (1.4272)\n",
      "Epoch [120/300]\tTrain loss: 0.4413\tVal loss: 0.5937 (1.1392)\n",
      "Epoch [121/300]\tTrain loss: 0.4263\tVal loss: 0.6339 (1.4562)\n",
      "Epoch [122/300]\tTrain loss: 0.4405\tVal loss: 0.5281 (1.2177)\n",
      "Epoch [123/300]\tTrain loss: 0.4242\tVal loss: 0.5967 (1.2905)\n",
      "Epoch [124/300]\tTrain loss: 0.4131\tVal loss: 0.5361 (1.1557)\n",
      "Epoch [125/300]\tTrain loss: 0.4195\tVal loss: 0.5728 (1.1611)\n",
      "Epoch [126/300]\tTrain loss: 0.4183\tVal loss: 0.5515 (1.1980)\n",
      "Epoch [127/300]\tTrain loss: 0.4249\tVal loss: 0.6038 (1.0537)\n",
      "Epoch [128/300]\tTrain loss: 0.4452\tVal loss: 0.7271 (1.5368)\n",
      "Epoch [129/300]\tTrain loss: 0.4191\tVal loss: 0.5562 (1.0373)\n",
      "Epoch [130/300]\tTrain loss: 0.4068\tVal loss: 0.5529 (1.2533)\n",
      "Epoch [131/300]\tTrain loss: 0.4180\tVal loss: 0.5277 (1.2524)\n",
      "Epoch [132/300]\tTrain loss: 0.4237\tVal loss: 0.5703 (0.9650)\n",
      "Epoch [133/300]\tTrain loss: 0.4082\tVal loss: 0.5483 (1.1106)\n",
      "Epoch [134/300]\tTrain loss: 0.4194\tVal loss: 0.5432 (1.1074)\n",
      "Epoch [135/300]\tTrain loss: 0.4071\tVal loss: 0.5606 (1.2667)\n",
      "Epoch [136/300]\tTrain loss: 0.4127\tVal loss: 0.6306 (1.3978)\n",
      "Epoch [137/300]\tTrain loss: 0.4185\tVal loss: 0.5325 (1.1169)\n",
      "Epoch [138/300]\tTrain loss: 0.3998\tVal loss: 0.5448 (1.1509)\n",
      "Epoch [139/300]\tTrain loss: 0.4165\tVal loss: 0.5302 (1.1597)\n",
      "Epoch [140/300]\tTrain loss: 0.4156\tVal loss: 0.5692 (1.2416)\n",
      "Epoch [141/300]\tTrain loss: 0.4189\tVal loss: 0.5484 (1.2502)\n",
      "Epoch [142/300]\tTrain loss: 0.4047\tVal loss: 0.5370 (1.0013)\n",
      "Epoch [143/300]\tTrain loss: 0.4008\tVal loss: 0.5647 (0.9700)\n",
      "Epoch [144/300]\tTrain loss: 0.4084\tVal loss: 0.5535 (1.0404)\n",
      "Epoch [145/300]\tTrain loss: 0.3919\tVal loss: 0.5458 (1.1417)\n",
      "Epoch [146/300]\tTrain loss: 0.3839\tVal loss: 0.5831 (1.2049)\n",
      "Epoch [147/300]\tTrain loss: 0.3840\tVal loss: 0.5281 (0.8787)\n",
      "Epoch [148/300]\tTrain loss: 0.3877\tVal loss: 0.5392 (1.1093)\n",
      "Epoch [149/300]\tTrain loss: 0.3751\tVal loss: 0.5802 (1.2040)\n",
      "Epoch [150/300]\tTrain loss: 0.3714\tVal loss: 0.5439 (1.1208)\n",
      "Epoch [151/300]\tTrain loss: 0.3826\tVal loss: 0.5161 (1.2042)\n",
      "Epoch [152/300]\tTrain loss: 0.3757\tVal loss: 0.5455 (1.0513)\n",
      "Epoch [153/300]\tTrain loss: 0.3894\tVal loss: 0.5315 (1.0452)\n",
      "Epoch [154/300]\tTrain loss: 0.3704\tVal loss: 0.5464 (1.1240)\n",
      "Epoch [155/300]\tTrain loss: 0.3556\tVal loss: 0.5475 (1.1762)\n",
      "Epoch [156/300]\tTrain loss: 0.3752\tVal loss: 0.5333 (1.0739)\n",
      "Epoch [157/300]\tTrain loss: 0.3768\tVal loss: 0.5244 (1.0801)\n",
      "Epoch [158/300]\tTrain loss: 0.4000\tVal loss: 0.5676 (1.1558)\n",
      "Epoch [159/300]\tTrain loss: 0.3670\tVal loss: 0.5336 (1.1184)\n",
      "Epoch [160/300]\tTrain loss: 0.3836\tVal loss: 0.5320 (0.8790)\n",
      "Epoch [161/300]\tTrain loss: 0.3954\tVal loss: 0.5352 (1.1594)\n",
      "Epoch [162/300]\tTrain loss: 0.3544\tVal loss: 0.5410 (1.1342)\n",
      "Epoch [163/300]\tTrain loss: 0.3776\tVal loss: 0.5360 (1.0537)\n",
      "Epoch [164/300]\tTrain loss: 0.3627\tVal loss: 0.5105 (1.0012)\n",
      "Epoch [165/300]\tTrain loss: 0.3619\tVal loss: 0.5355 (0.9789)\n",
      "Epoch [166/300]\tTrain loss: 0.3487\tVal loss: 0.5544 (1.0352)\n",
      "Epoch [167/300]\tTrain loss: 0.3738\tVal loss: 0.5815 (0.9670)\n",
      "Epoch [168/300]\tTrain loss: 0.3671\tVal loss: 0.5523 (1.0428)\n",
      "Epoch [169/300]\tTrain loss: 0.3574\tVal loss: 0.5250 (1.0371)\n",
      "Epoch [170/300]\tTrain loss: 0.3650\tVal loss: 0.5387 (1.0347)\n",
      "Epoch [171/300]\tTrain loss: 0.3610\tVal loss: 0.5270 (1.0423)\n",
      "Epoch [172/300]\tTrain loss: 0.3450\tVal loss: 0.5634 (1.0202)\n",
      "Epoch [173/300]\tTrain loss: 0.3534\tVal loss: 0.5300 (1.0009)\n",
      "Epoch [174/300]\tTrain loss: 0.3574\tVal loss: 0.5288 (1.0403)\n",
      "Epoch [175/300]\tTrain loss: 0.3628\tVal loss: 0.5518 (1.1040)\n",
      "Epoch [176/300]\tTrain loss: 0.3463\tVal loss: 0.5232 (0.9293)\n",
      "Epoch [177/300]\tTrain loss: 0.3333\tVal loss: 0.5439 (1.0521)\n",
      "Epoch [178/300]\tTrain loss: 0.3234\tVal loss: 0.5454 (0.8669)\n",
      "Epoch [179/300]\tTrain loss: 0.3622\tVal loss: 0.5134 (1.1319)\n",
      "Epoch [180/300]\tTrain loss: 0.3728\tVal loss: 0.5258 (1.0419)\n",
      "Epoch [181/300]\tTrain loss: 0.3449\tVal loss: 0.5191 (0.9723)\n",
      "Epoch [182/300]\tTrain loss: 0.3708\tVal loss: 0.5673 (0.9662)\n",
      "Epoch [183/300]\tTrain loss: 0.3493\tVal loss: 0.5130 (0.9528)\n",
      "Epoch [184/300]\tTrain loss: 0.3273\tVal loss: 0.5341 (0.9364)\n",
      "Epoch [185/300]\tTrain loss: 0.3309\tVal loss: 0.5281 (1.2071)\n",
      "Epoch [186/300]\tTrain loss: 0.3472\tVal loss: 0.5506 (1.0764)\n",
      "Epoch [187/300]\tTrain loss: 0.3277\tVal loss: 0.5168 (1.0600)\n",
      "Epoch [188/300]\tTrain loss: 0.3338\tVal loss: 0.5313 (1.0069)\n",
      "Epoch [189/300]\tTrain loss: 0.3382\tVal loss: 0.5361 (0.9312)\n",
      "Epoch [190/300]\tTrain loss: 0.3252\tVal loss: 0.5099 (0.9776)\n",
      "Epoch [191/300]\tTrain loss: 0.3228\tVal loss: 0.5781 (1.1013)\n",
      "Epoch [192/300]\tTrain loss: 0.3398\tVal loss: 0.5225 (1.1266)\n",
      "Epoch [193/300]\tTrain loss: 0.3099\tVal loss: 0.5161 (1.0296)\n",
      "Epoch [194/300]\tTrain loss: 0.3249\tVal loss: 0.5210 (0.9364)\n",
      "Epoch [195/300]\tTrain loss: 0.3298\tVal loss: 0.5082 (0.9198)\n",
      "Epoch [196/300]\tTrain loss: 0.3242\tVal loss: 0.5701 (0.9965)\n",
      "Epoch [197/300]\tTrain loss: 0.3157\tVal loss: 0.5904 (1.1613)\n",
      "Epoch [198/300]\tTrain loss: 0.3273\tVal loss: 0.5278 (0.9544)\n",
      "Epoch [199/300]\tTrain loss: 0.3189\tVal loss: 0.5067 (0.9389)\n",
      "Epoch [200/300]\tTrain loss: 0.3310\tVal loss: 0.5101 (0.8326)\n",
      "Epoch [201/300]\tTrain loss: 0.3049\tVal loss: 0.5166 (0.9447)\n",
      "Epoch [202/300]\tTrain loss: 0.3438\tVal loss: 0.5274 (1.1587)\n",
      "Epoch [203/300]\tTrain loss: 0.3227\tVal loss: 0.5202 (0.9891)\n",
      "Epoch [204/300]\tTrain loss: 0.3718\tVal loss: 0.5193 (0.9631)\n",
      "Epoch [205/300]\tTrain loss: 0.3187\tVal loss: 0.5459 (1.0259)\n",
      "Epoch [206/300]\tTrain loss: 0.3431\tVal loss: 0.6013 (1.2281)\n",
      "Epoch [207/300]\tTrain loss: 0.3188\tVal loss: 0.6009 (1.1849)\n",
      "Epoch [208/300]\tTrain loss: 0.3205\tVal loss: 0.5130 (1.0303)\n",
      "Epoch [209/300]\tTrain loss: 0.3093\tVal loss: 0.5346 (0.9401)\n",
      "Epoch [210/300]\tTrain loss: 0.3090\tVal loss: 0.5142 (1.0815)\n",
      "Epoch [211/300]\tTrain loss: 0.3079\tVal loss: 0.5100 (1.0737)\n",
      "Epoch [212/300]\tTrain loss: 0.3008\tVal loss: 0.5435 (1.0497)\n",
      "Epoch [213/300]\tTrain loss: 0.3065\tVal loss: 0.5139 (1.0522)\n",
      "Epoch [214/300]\tTrain loss: 0.3163\tVal loss: 0.5378 (1.1119)\n",
      "Epoch [215/300]\tTrain loss: 0.2966\tVal loss: 0.5131 (1.0572)\n",
      "Epoch [216/300]\tTrain loss: 0.3256\tVal loss: 0.5615 (0.9426)\n",
      "Epoch [217/300]\tTrain loss: 0.3186\tVal loss: 0.5247 (0.9182)\n",
      "Epoch [218/300]\tTrain loss: 0.2961\tVal loss: 0.5153 (1.0817)\n",
      "Epoch [219/300]\tTrain loss: 0.3058\tVal loss: 0.6984 (1.3339)\n",
      "Epoch [220/300]\tTrain loss: 0.3213\tVal loss: 0.5069 (1.0464)\n",
      "Epoch [221/300]\tTrain loss: 0.2883\tVal loss: 0.5052 (1.0602)\n",
      "Epoch [222/300]\tTrain loss: 0.3019\tVal loss: 0.5274 (1.1571)\n",
      "Epoch [223/300]\tTrain loss: 0.2885\tVal loss: 0.5453 (1.1958)\n",
      "Epoch [224/300]\tTrain loss: 0.3011\tVal loss: 0.5164 (1.0118)\n",
      "Epoch [225/300]\tTrain loss: 0.3008\tVal loss: 0.5318 (0.9669)\n",
      "Epoch [226/300]\tTrain loss: 0.3268\tVal loss: 0.5699 (1.0327)\n",
      "Epoch [227/300]\tTrain loss: 0.2910\tVal loss: 0.5136 (1.0695)\n",
      "Epoch [228/300]\tTrain loss: 0.2970\tVal loss: 0.5209 (0.9893)\n",
      "Epoch [229/300]\tTrain loss: 0.2934\tVal loss: 0.5188 (1.1382)\n",
      "Epoch [230/300]\tTrain loss: 0.2914\tVal loss: 0.5384 (1.1315)\n",
      "Epoch [231/300]\tTrain loss: 0.2890\tVal loss: 0.5079 (1.1469)\n",
      "Epoch [232/300]\tTrain loss: 0.2844\tVal loss: 0.5121 (1.0340)\n",
      "Epoch [233/300]\tTrain loss: 0.3178\tVal loss: 0.5288 (1.0167)\n",
      "Epoch [234/300]\tTrain loss: 0.2940\tVal loss: 0.5266 (0.9329)\n",
      "Epoch [235/300]\tTrain loss: 0.2970\tVal loss: 0.5109 (0.9612)\n",
      "Epoch [236/300]\tTrain loss: 0.2951\tVal loss: 0.5139 (0.9798)\n",
      "Epoch [237/300]\tTrain loss: 0.2923\tVal loss: 0.5360 (1.0889)\n",
      "Epoch [238/300]\tTrain loss: 0.2855\tVal loss: 0.5097 (1.0365)\n",
      "Epoch [239/300]\tTrain loss: 0.3005\tVal loss: 0.4985 (0.8897)\n",
      "Epoch [240/300]\tTrain loss: 0.2802\tVal loss: 0.5101 (1.0508)\n",
      "Epoch [241/300]\tTrain loss: 0.2753\tVal loss: 0.5116 (1.0721)\n",
      "Epoch [242/300]\tTrain loss: 0.2711\tVal loss: 0.5191 (0.9483)\n",
      "Epoch [243/300]\tTrain loss: 0.2863\tVal loss: 0.6103 (1.1947)\n",
      "Epoch [244/300]\tTrain loss: 0.2814\tVal loss: 0.5854 (0.9780)\n",
      "Epoch [245/300]\tTrain loss: 0.2828\tVal loss: 0.5322 (1.0271)\n",
      "Epoch [246/300]\tTrain loss: 0.2810\tVal loss: 0.5088 (1.0134)\n",
      "Epoch [247/300]\tTrain loss: 0.2789\tVal loss: 0.5391 (1.1528)\n",
      "Epoch [248/300]\tTrain loss: 0.2840\tVal loss: 0.5529 (1.2545)\n",
      "Epoch [249/300]\tTrain loss: 0.2813\tVal loss: 0.4990 (1.1173)\n",
      "Epoch [250/300]\tTrain loss: 0.2949\tVal loss: 0.5268 (1.0583)\n",
      "Epoch [251/300]\tTrain loss: 0.2872\tVal loss: 0.5141 (1.1114)\n",
      "Epoch [252/300]\tTrain loss: 0.2904\tVal loss: 0.5153 (0.9712)\n",
      "Epoch [253/300]\tTrain loss: 0.2756\tVal loss: 0.5056 (1.1492)\n",
      "Epoch [254/300]\tTrain loss: 0.2666\tVal loss: 0.4992 (1.0255)\n",
      "Epoch [255/300]\tTrain loss: 0.2791\tVal loss: 0.4957 (1.0106)\n",
      "Epoch [256/300]\tTrain loss: 0.2856\tVal loss: 0.5696 (1.1216)\n",
      "Epoch [257/300]\tTrain loss: 0.2855\tVal loss: 0.5110 (0.8837)\n",
      "Epoch [258/300]\tTrain loss: 0.2858\tVal loss: 0.5052 (1.0959)\n",
      "Epoch [259/300]\tTrain loss: 0.2802\tVal loss: 0.4980 (1.0066)\n",
      "Epoch [260/300]\tTrain loss: 0.2699\tVal loss: 0.4904 (1.1077)\n",
      "Epoch [261/300]\tTrain loss: 0.2696\tVal loss: 0.5075 (0.9666)\n",
      "Epoch [262/300]\tTrain loss: 0.2658\tVal loss: 0.4955 (0.9611)\n",
      "Epoch [263/300]\tTrain loss: 0.2642\tVal loss: 0.5079 (1.1048)\n",
      "Epoch [264/300]\tTrain loss: 0.2652\tVal loss: 0.4941 (1.1179)\n",
      "Epoch [265/300]\tTrain loss: 0.2976\tVal loss: 0.5256 (1.0661)\n",
      "Epoch [266/300]\tTrain loss: 0.2919\tVal loss: 0.5063 (1.1265)\n",
      "Epoch [267/300]\tTrain loss: 0.2572\tVal loss: 0.5039 (1.0918)\n",
      "Epoch [268/300]\tTrain loss: 0.2511\tVal loss: 0.4987 (0.9623)\n",
      "Epoch [269/300]\tTrain loss: 0.2672\tVal loss: 0.5557 (1.0440)\n",
      "Epoch [270/300]\tTrain loss: 0.2729\tVal loss: 0.6032 (0.9325)\n",
      "Epoch [271/300]\tTrain loss: 0.2577\tVal loss: 0.5053 (0.9077)\n",
      "Epoch [272/300]\tTrain loss: 0.2723\tVal loss: 0.5230 (0.9701)\n",
      "Epoch [273/300]\tTrain loss: 0.2554\tVal loss: 0.4973 (0.9361)\n",
      "Epoch [274/300]\tTrain loss: 0.2563\tVal loss: 0.5327 (1.0379)\n",
      "Epoch [275/300]\tTrain loss: 0.2630\tVal loss: 0.4969 (0.9953)\n",
      "Epoch [276/300]\tTrain loss: 0.2616\tVal loss: 0.5461 (1.2469)\n",
      "Epoch [277/300]\tTrain loss: 0.2529\tVal loss: 0.5103 (1.0268)\n",
      "Epoch [278/300]\tTrain loss: 0.2628\tVal loss: 0.5100 (0.9872)\n",
      "Epoch [279/300]\tTrain loss: 0.2479\tVal loss: 0.5062 (1.0684)\n",
      "Epoch [280/300]\tTrain loss: 0.2754\tVal loss: 0.5382 (1.0135)\n",
      "Epoch [281/300]\tTrain loss: 0.2789\tVal loss: 0.5311 (0.9077)\n",
      "Epoch [282/300]\tTrain loss: 0.2601\tVal loss: 0.5056 (1.0110)\n",
      "Epoch [283/300]\tTrain loss: 0.2550\tVal loss: 0.5134 (1.0799)\n",
      "Epoch [284/300]\tTrain loss: 0.2663\tVal loss: 0.5224 (1.1608)\n",
      "Epoch [285/300]\tTrain loss: 0.2551\tVal loss: 0.5194 (1.0298)\n",
      "Epoch [286/300]\tTrain loss: 0.2393\tVal loss: 0.5214 (1.0473)\n",
      "Epoch [287/300]\tTrain loss: 0.2606\tVal loss: 0.5070 (1.0331)\n",
      "Epoch [288/300]\tTrain loss: 0.2614\tVal loss: 0.5201 (1.0401)\n",
      "Epoch [289/300]\tTrain loss: 0.2582\tVal loss: 0.5462 (1.0240)\n",
      "Epoch [290/300]\tTrain loss: 0.2421\tVal loss: 0.5094 (1.1139)\n",
      "Epoch [291/300]\tTrain loss: 0.2651\tVal loss: 0.4984 (1.0172)\n",
      "Epoch [292/300]\tTrain loss: 0.2391\tVal loss: 0.4916 (0.9854)\n",
      "Epoch [293/300]\tTrain loss: 0.2408\tVal loss: 0.5143 (0.9460)\n",
      "Epoch [294/300]\tTrain loss: 0.2460\tVal loss: 0.5231 (1.0699)\n",
      "Epoch [295/300]\tTrain loss: 0.2482\tVal loss: 0.4995 (0.9881)\n",
      "Epoch [296/300]\tTrain loss: 0.2447\tVal loss: 0.5070 (0.9878)\n",
      "Epoch [297/300]\tTrain loss: 0.2424\tVal loss: 0.5422 (1.1890)\n",
      "Epoch [298/300]\tTrain loss: 0.2382\tVal loss: 0.5045 (1.1907)\n",
      "Epoch [299/300]\tTrain loss: 0.2466\tVal loss: 0.4947 (1.1076)\n",
      "Epoch [300/300]\tTrain loss: 0.2521\tVal loss: 0.4956 (0.9633)\n",
      "Epoch [1/300]\tTrain loss: 1.9949\tVal loss: 0.9953 (1.2999)\n",
      "Epoch [2/300]\tTrain loss: 0.8387\tVal loss: 0.7574 (1.2398)\n",
      "Epoch [3/300]\tTrain loss: 0.7099\tVal loss: 0.6345 (1.2707)\n",
      "Epoch [4/300]\tTrain loss: 0.5496\tVal loss: 0.4531 (0.8190)\n",
      "Epoch [5/300]\tTrain loss: 0.3676\tVal loss: 0.2751 (0.5584)\n",
      "Epoch [6/300]\tTrain loss: 0.2939\tVal loss: 0.2778 (0.5362)\n",
      "Epoch [7/300]\tTrain loss: 0.2787\tVal loss: 0.3737 (0.6989)\n",
      "Epoch [8/300]\tTrain loss: 0.2977\tVal loss: 0.2443 (0.5003)\n",
      "Epoch [9/300]\tTrain loss: 0.2594\tVal loss: 0.2416 (0.4971)\n",
      "Epoch [10/300]\tTrain loss: 0.2496\tVal loss: 0.2987 (0.4936)\n",
      "Epoch [11/300]\tTrain loss: 0.2676\tVal loss: 0.2538 (0.4671)\n",
      "Epoch [12/300]\tTrain loss: 0.3086\tVal loss: 0.3079 (0.5809)\n",
      "Epoch [13/300]\tTrain loss: 0.2701\tVal loss: 0.2266 (0.4424)\n",
      "Epoch [14/300]\tTrain loss: 0.2512\tVal loss: 0.2780 (0.5575)\n",
      "Epoch [15/300]\tTrain loss: 0.2455\tVal loss: 0.2821 (0.4484)\n",
      "Epoch [16/300]\tTrain loss: 0.2324\tVal loss: 0.2490 (0.5296)\n",
      "Epoch [17/300]\tTrain loss: 0.2761\tVal loss: 0.4738 (0.7852)\n",
      "Epoch [18/300]\tTrain loss: 0.2534\tVal loss: 0.2602 (0.4586)\n",
      "Epoch [19/300]\tTrain loss: 0.2587\tVal loss: 0.2235 (0.4490)\n",
      "Epoch [20/300]\tTrain loss: 0.2536\tVal loss: 0.3229 (0.6087)\n",
      "Epoch [21/300]\tTrain loss: 0.2400\tVal loss: 0.2179 (0.4764)\n",
      "Epoch [22/300]\tTrain loss: 0.2795\tVal loss: 0.3524 (0.6246)\n",
      "Epoch [23/300]\tTrain loss: 0.3010\tVal loss: 0.3275 (0.6610)\n",
      "Epoch [24/300]\tTrain loss: 0.2784\tVal loss: 0.2963 (0.4760)\n",
      "Epoch [25/300]\tTrain loss: 0.3240\tVal loss: 0.2090 (0.4424)\n",
      "Epoch [26/300]\tTrain loss: 0.3111\tVal loss: 0.2662 (0.5153)\n",
      "Epoch [27/300]\tTrain loss: 0.2322\tVal loss: 0.2494 (0.4447)\n",
      "Epoch [28/300]\tTrain loss: 0.2336\tVal loss: 0.3482 (0.6459)\n",
      "Epoch [29/300]\tTrain loss: 0.2543\tVal loss: 0.6581 (0.7671)\n",
      "Epoch [30/300]\tTrain loss: 0.3050\tVal loss: 0.2811 (0.4548)\n",
      "Epoch [31/300]\tTrain loss: 0.2231\tVal loss: 0.4387 (0.5590)\n",
      "Epoch [32/300]\tTrain loss: 0.2437\tVal loss: 0.2288 (0.4290)\n",
      "Epoch [33/300]\tTrain loss: 0.2414\tVal loss: 0.5545 (0.6454)\n",
      "Epoch [34/300]\tTrain loss: 0.3150\tVal loss: 0.2213 (0.4881)\n",
      "Epoch [35/300]\tTrain loss: 0.2245\tVal loss: 0.3358 (0.6759)\n",
      "Epoch [36/300]\tTrain loss: 0.2860\tVal loss: 0.2401 (0.4432)\n",
      "Epoch [37/300]\tTrain loss: 0.2180\tVal loss: 0.2307 (0.4669)\n",
      "Epoch [38/300]\tTrain loss: 0.2290\tVal loss: 0.2005 (0.4473)\n",
      "Epoch [39/300]\tTrain loss: 0.2943\tVal loss: 0.2120 (0.3986)\n",
      "Epoch [40/300]\tTrain loss: 0.3211\tVal loss: 0.5059 (0.5787)\n",
      "Epoch [41/300]\tTrain loss: 0.2992\tVal loss: 0.2544 (0.4619)\n",
      "Epoch [42/300]\tTrain loss: 0.2343\tVal loss: 0.3432 (0.6498)\n",
      "Epoch [43/300]\tTrain loss: 0.2248\tVal loss: 0.2128 (0.4848)\n",
      "Epoch [44/300]\tTrain loss: 0.2518\tVal loss: 0.2065 (0.4383)\n",
      "Epoch [45/300]\tTrain loss: 0.2600\tVal loss: 0.1905 (0.4417)\n",
      "Epoch [46/300]\tTrain loss: 0.2248\tVal loss: 0.2202 (0.5185)\n",
      "Epoch [47/300]\tTrain loss: 0.2570\tVal loss: 0.1983 (0.4460)\n",
      "Epoch [48/300]\tTrain loss: 0.2168\tVal loss: 0.2373 (0.5421)\n",
      "Epoch [49/300]\tTrain loss: 0.2308\tVal loss: 0.3090 (0.6116)\n",
      "Epoch [50/300]\tTrain loss: 0.2351\tVal loss: 0.2540 (0.5346)\n",
      "Epoch [51/300]\tTrain loss: 0.2009\tVal loss: 0.2660 (0.5622)\n",
      "Epoch [52/300]\tTrain loss: 0.2437\tVal loss: 0.5793 (0.7129)\n",
      "Epoch [53/300]\tTrain loss: 0.3339\tVal loss: 0.4203 (0.5942)\n",
      "Epoch [54/300]\tTrain loss: 0.2561\tVal loss: 0.3563 (0.6783)\n",
      "Epoch [55/300]\tTrain loss: 0.3239\tVal loss: 0.2580 (0.5634)\n",
      "Epoch [56/300]\tTrain loss: 0.2192\tVal loss: 0.3085 (0.5256)\n",
      "Epoch [57/300]\tTrain loss: 0.2159\tVal loss: 0.2316 (0.4708)\n",
      "Epoch [58/300]\tTrain loss: 0.2685\tVal loss: 0.3100 (0.4710)\n",
      "Epoch [59/300]\tTrain loss: 0.2174\tVal loss: 0.1912 (0.4481)\n",
      "Epoch [60/300]\tTrain loss: 0.2052\tVal loss: 0.2478 (0.5757)\n",
      "Epoch [61/300]\tTrain loss: 0.1906\tVal loss: 0.1981 (0.5127)\n",
      "Epoch [62/300]\tTrain loss: 0.2616\tVal loss: 0.2451 (0.4836)\n",
      "Epoch [63/300]\tTrain loss: 0.2130\tVal loss: 0.1891 (0.4759)\n",
      "Epoch [64/300]\tTrain loss: 0.2379\tVal loss: 0.2545 (0.4511)\n",
      "Epoch [65/300]\tTrain loss: 0.2338\tVal loss: 0.2715 (0.5670)\n",
      "Epoch [66/300]\tTrain loss: 0.2322\tVal loss: 0.2034 (0.4565)\n",
      "Epoch [67/300]\tTrain loss: 0.2270\tVal loss: 0.2036 (0.4675)\n",
      "Epoch [68/300]\tTrain loss: 0.2114\tVal loss: 0.2870 (0.4512)\n",
      "Epoch [69/300]\tTrain loss: 0.2036\tVal loss: 0.2257 (0.5436)\n",
      "Epoch [70/300]\tTrain loss: 0.2876\tVal loss: 0.2492 (0.5069)\n",
      "Epoch [71/300]\tTrain loss: 0.1961\tVal loss: 0.1848 (0.4514)\n",
      "Epoch [72/300]\tTrain loss: 0.1959\tVal loss: 0.2305 (0.4778)\n",
      "Epoch [73/300]\tTrain loss: 0.2200\tVal loss: 0.2198 (0.4135)\n",
      "Epoch [74/300]\tTrain loss: 0.2219\tVal loss: 0.2096 (0.4609)\n",
      "Epoch [75/300]\tTrain loss: 0.1769\tVal loss: 0.1892 (0.5047)\n",
      "Epoch [76/300]\tTrain loss: 0.2454\tVal loss: 0.2898 (0.6081)\n",
      "Epoch [77/300]\tTrain loss: 0.2325\tVal loss: 0.3242 (0.4948)\n",
      "Epoch [78/300]\tTrain loss: 0.2468\tVal loss: 0.3271 (0.5146)\n",
      "Epoch [79/300]\tTrain loss: 0.2653\tVal loss: 0.2377 (0.4758)\n",
      "Epoch [80/300]\tTrain loss: 0.1970\tVal loss: 0.3252 (0.4756)\n",
      "Epoch [81/300]\tTrain loss: 0.2109\tVal loss: 0.2386 (0.4989)\n",
      "Epoch [82/300]\tTrain loss: 0.2099\tVal loss: 0.4031 (0.5433)\n",
      "Epoch [83/300]\tTrain loss: 0.2116\tVal loss: 0.2489 (0.5416)\n",
      "Epoch [84/300]\tTrain loss: 0.1976\tVal loss: 0.2229 (0.4885)\n",
      "Epoch [85/300]\tTrain loss: 0.2357\tVal loss: 0.2622 (0.4472)\n",
      "Epoch [86/300]\tTrain loss: 0.1826\tVal loss: 0.2235 (0.4195)\n",
      "Epoch [87/300]\tTrain loss: 0.1919\tVal loss: 0.1888 (0.4475)\n",
      "Epoch [88/300]\tTrain loss: 0.1976\tVal loss: 0.1947 (0.4544)\n",
      "Epoch [89/300]\tTrain loss: 0.2156\tVal loss: 0.1826 (0.4830)\n",
      "Epoch [90/300]\tTrain loss: 0.2259\tVal loss: 0.2511 (0.4951)\n",
      "Epoch [91/300]\tTrain loss: 0.2075\tVal loss: 0.2067 (0.5364)\n",
      "Epoch [92/300]\tTrain loss: 0.1821\tVal loss: 0.2435 (0.4354)\n",
      "Epoch [93/300]\tTrain loss: 0.1686\tVal loss: 0.1954 (0.4375)\n",
      "Epoch [94/300]\tTrain loss: 0.1902\tVal loss: 0.2620 (0.5113)\n",
      "Epoch [95/300]\tTrain loss: 0.2043\tVal loss: 0.1819 (0.4756)\n",
      "Epoch [96/300]\tTrain loss: 0.1771\tVal loss: 0.1806 (0.4433)\n",
      "Epoch [97/300]\tTrain loss: 0.1638\tVal loss: 0.1992 (0.5325)\n",
      "Epoch [98/300]\tTrain loss: 0.1774\tVal loss: 0.2494 (0.5540)\n",
      "Epoch [99/300]\tTrain loss: 0.1707\tVal loss: 0.1882 (0.4588)\n",
      "Epoch [100/300]\tTrain loss: 0.1876\tVal loss: 0.2008 (0.4474)\n",
      "Epoch [101/300]\tTrain loss: 0.1757\tVal loss: 0.2004 (0.4688)\n",
      "Epoch [102/300]\tTrain loss: 0.1941\tVal loss: 0.1865 (0.4513)\n",
      "Epoch [103/300]\tTrain loss: 0.1903\tVal loss: 0.2502 (0.5471)\n",
      "Epoch [104/300]\tTrain loss: 0.1789\tVal loss: 0.2091 (0.4934)\n",
      "Epoch [105/300]\tTrain loss: 0.1890\tVal loss: 0.2031 (0.4982)\n",
      "Epoch [106/300]\tTrain loss: 0.1935\tVal loss: 0.3593 (0.6489)\n",
      "Epoch [107/300]\tTrain loss: 0.1846\tVal loss: 0.1922 (0.4663)\n",
      "Epoch [108/300]\tTrain loss: 0.1840\tVal loss: 0.1908 (0.4659)\n",
      "Epoch [109/300]\tTrain loss: 0.1864\tVal loss: 0.3299 (0.5358)\n",
      "Epoch [110/300]\tTrain loss: 0.1962\tVal loss: 0.2295 (0.5005)\n",
      "Epoch [111/300]\tTrain loss: 0.1823\tVal loss: 0.2017 (0.5380)\n",
      "Epoch [112/300]\tTrain loss: 0.1594\tVal loss: 0.1922 (0.4516)\n",
      "Epoch [113/300]\tTrain loss: 0.1723\tVal loss: 0.1942 (0.4525)\n",
      "Epoch [114/300]\tTrain loss: 0.1619\tVal loss: 0.1779 (0.4787)\n",
      "Epoch [115/300]\tTrain loss: 0.1727\tVal loss: 0.1973 (0.4760)\n",
      "Epoch [116/300]\tTrain loss: 0.1664\tVal loss: 0.2048 (0.5268)\n",
      "Epoch [117/300]\tTrain loss: 0.1725\tVal loss: 0.1900 (0.5338)\n",
      "Epoch [118/300]\tTrain loss: 0.1783\tVal loss: 0.1865 (0.4778)\n",
      "Epoch [119/300]\tTrain loss: 0.1748\tVal loss: 0.1956 (0.5043)\n",
      "Epoch [120/300]\tTrain loss: 0.1725\tVal loss: 0.1907 (0.4128)\n",
      "Epoch [121/300]\tTrain loss: 0.1944\tVal loss: 0.2096 (0.5201)\n",
      "Epoch [122/300]\tTrain loss: 0.1791\tVal loss: 0.1871 (0.4927)\n",
      "Epoch [123/300]\tTrain loss: 0.1742\tVal loss: 0.1892 (0.4686)\n",
      "Epoch [124/300]\tTrain loss: 0.1843\tVal loss: 0.1977 (0.5318)\n",
      "Epoch [125/300]\tTrain loss: 0.1614\tVal loss: 0.1878 (0.4631)\n",
      "Epoch [126/300]\tTrain loss: 0.1526\tVal loss: 0.1973 (0.5236)\n",
      "Epoch [127/300]\tTrain loss: 0.1575\tVal loss: 0.1797 (0.4429)\n",
      "Epoch [128/300]\tTrain loss: 0.1929\tVal loss: 0.2182 (0.5274)\n",
      "Epoch [129/300]\tTrain loss: 0.1477\tVal loss: 0.1990 (0.5504)\n",
      "Epoch [130/300]\tTrain loss: 0.1755\tVal loss: 0.1960 (0.4749)\n",
      "Epoch [131/300]\tTrain loss: 0.1752\tVal loss: 0.3094 (0.5052)\n",
      "Epoch [132/300]\tTrain loss: 0.1965\tVal loss: 0.1950 (0.4612)\n",
      "Epoch [133/300]\tTrain loss: 0.1571\tVal loss: 0.2291 (0.5185)\n",
      "Epoch [134/300]\tTrain loss: 0.1790\tVal loss: 0.2576 (0.5295)\n",
      "Epoch [135/300]\tTrain loss: 0.1695\tVal loss: 0.1862 (0.4235)\n",
      "Epoch [136/300]\tTrain loss: 0.1433\tVal loss: 0.1799 (0.4647)\n",
      "Epoch [137/300]\tTrain loss: 0.1518\tVal loss: 0.1775 (0.4950)\n",
      "Epoch [138/300]\tTrain loss: 0.1567\tVal loss: 0.2056 (0.3982)\n",
      "Epoch [139/300]\tTrain loss: 0.1518\tVal loss: 0.1765 (0.5064)\n",
      "Epoch [140/300]\tTrain loss: 0.1725\tVal loss: 0.1973 (0.4546)\n",
      "Epoch [141/300]\tTrain loss: 0.1666\tVal loss: 0.2470 (0.4341)\n",
      "Epoch [142/300]\tTrain loss: 0.1663\tVal loss: 0.1837 (0.4618)\n",
      "Epoch [143/300]\tTrain loss: 0.1767\tVal loss: 0.1866 (0.4759)\n",
      "Epoch [144/300]\tTrain loss: 0.1540\tVal loss: 0.2031 (0.5112)\n",
      "Epoch [145/300]\tTrain loss: 0.1983\tVal loss: 0.1940 (0.4658)\n",
      "Epoch [146/300]\tTrain loss: 0.1493\tVal loss: 0.1808 (0.4906)\n",
      "Epoch [147/300]\tTrain loss: 0.1827\tVal loss: 0.1857 (0.4414)\n",
      "Epoch [148/300]\tTrain loss: 0.1518\tVal loss: 0.2429 (0.4681)\n",
      "Epoch [149/300]\tTrain loss: 0.1706\tVal loss: 0.2117 (0.5777)\n",
      "Epoch [150/300]\tTrain loss: 0.1494\tVal loss: 0.1980 (0.5136)\n",
      "Epoch [151/300]\tTrain loss: 0.1491\tVal loss: 0.2039 (0.4330)\n",
      "Epoch [152/300]\tTrain loss: 0.1402\tVal loss: 0.1730 (0.4253)\n",
      "Epoch [153/300]\tTrain loss: 0.1591\tVal loss: 0.1805 (0.4739)\n",
      "Epoch [154/300]\tTrain loss: 0.1534\tVal loss: 0.1882 (0.4820)\n",
      "Epoch [155/300]\tTrain loss: 0.1609\tVal loss: 0.2017 (0.4783)\n",
      "Epoch [156/300]\tTrain loss: 0.1523\tVal loss: 0.1771 (0.4958)\n",
      "Epoch [157/300]\tTrain loss: 0.1696\tVal loss: 0.2374 (0.4065)\n",
      "Epoch [158/300]\tTrain loss: 0.1517\tVal loss: 0.2043 (0.4629)\n",
      "Epoch [159/300]\tTrain loss: 0.1397\tVal loss: 0.1739 (0.4444)\n",
      "Epoch [160/300]\tTrain loss: 0.1569\tVal loss: 0.1919 (0.4346)\n",
      "Epoch [161/300]\tTrain loss: 0.1466\tVal loss: 0.1762 (0.4677)\n",
      "Epoch [162/300]\tTrain loss: 0.1794\tVal loss: 0.1883 (0.4990)\n",
      "Epoch [163/300]\tTrain loss: 0.1504\tVal loss: 0.2535 (0.4136)\n",
      "Epoch [164/300]\tTrain loss: 0.1556\tVal loss: 0.1924 (0.4602)\n",
      "Epoch [165/300]\tTrain loss: 0.1451\tVal loss: 0.2134 (0.5254)\n",
      "Epoch [166/300]\tTrain loss: 0.1397\tVal loss: 0.2087 (0.4762)\n",
      "Epoch [167/300]\tTrain loss: 0.1463\tVal loss: 0.1872 (0.5006)\n",
      "Epoch [168/300]\tTrain loss: 0.1491\tVal loss: 0.2094 (0.4305)\n",
      "Epoch [169/300]\tTrain loss: 0.1524\tVal loss: 0.1758 (0.4466)\n",
      "Epoch [170/300]\tTrain loss: 0.1634\tVal loss: 0.1834 (0.4392)\n",
      "Epoch [171/300]\tTrain loss: 0.1507\tVal loss: 0.2288 (0.5514)\n",
      "Epoch [172/300]\tTrain loss: 0.1830\tVal loss: 0.2053 (0.4171)\n",
      "Epoch [173/300]\tTrain loss: 0.1510\tVal loss: 0.1776 (0.5135)\n",
      "Epoch [174/300]\tTrain loss: 0.1442\tVal loss: 0.1668 (0.4403)\n",
      "Epoch [175/300]\tTrain loss: 0.1250\tVal loss: 0.1742 (0.5047)\n",
      "Epoch [176/300]\tTrain loss: 0.1417\tVal loss: 0.1930 (0.4716)\n",
      "Epoch [177/300]\tTrain loss: 0.1463\tVal loss: 0.2055 (0.4496)\n",
      "Epoch [178/300]\tTrain loss: 0.1428\tVal loss: 0.1784 (0.4680)\n",
      "Epoch [179/300]\tTrain loss: 0.1367\tVal loss: 0.1795 (0.4420)\n",
      "Epoch [180/300]\tTrain loss: 0.1274\tVal loss: 0.1855 (0.4326)\n",
      "Epoch [181/300]\tTrain loss: 0.1384\tVal loss: 0.1772 (0.4470)\n",
      "Epoch [182/300]\tTrain loss: 0.1469\tVal loss: 0.1688 (0.4181)\n",
      "Epoch [183/300]\tTrain loss: 0.1411\tVal loss: 0.1908 (0.5108)\n",
      "Epoch [184/300]\tTrain loss: 0.1451\tVal loss: 0.2048 (0.3949)\n",
      "Epoch [185/300]\tTrain loss: 0.1451\tVal loss: 0.1711 (0.5026)\n",
      "Epoch [186/300]\tTrain loss: 0.1280\tVal loss: 0.2124 (0.4884)\n",
      "Epoch [187/300]\tTrain loss: 0.1398\tVal loss: 0.1749 (0.4676)\n",
      "Epoch [188/300]\tTrain loss: 0.1487\tVal loss: 0.2256 (0.4450)\n",
      "Epoch [189/300]\tTrain loss: 0.1437\tVal loss: 0.1722 (0.4719)\n",
      "Epoch [190/300]\tTrain loss: 0.1293\tVal loss: 0.2302 (0.5381)\n",
      "Epoch [191/300]\tTrain loss: 0.1359\tVal loss: 0.1764 (0.4184)\n",
      "Epoch [192/300]\tTrain loss: 0.1316\tVal loss: 0.1724 (0.4151)\n",
      "Epoch [193/300]\tTrain loss: 0.1236\tVal loss: 0.1687 (0.4670)\n",
      "Epoch [194/300]\tTrain loss: 0.1306\tVal loss: 0.1827 (0.5061)\n",
      "Epoch [195/300]\tTrain loss: 0.1377\tVal loss: 0.1728 (0.5217)\n",
      "Epoch [196/300]\tTrain loss: 0.1449\tVal loss: 0.2150 (0.4198)\n",
      "Epoch [197/300]\tTrain loss: 0.1431\tVal loss: 0.1875 (0.5003)\n",
      "Epoch [198/300]\tTrain loss: 0.1347\tVal loss: 0.1842 (0.5170)\n",
      "Epoch [199/300]\tTrain loss: 0.1324\tVal loss: 0.1870 (0.4768)\n",
      "Epoch [200/300]\tTrain loss: 0.1308\tVal loss: 0.2013 (0.4865)\n",
      "Epoch [201/300]\tTrain loss: 0.1363\tVal loss: 0.1708 (0.4535)\n",
      "Epoch [202/300]\tTrain loss: 0.1378\tVal loss: 0.2215 (0.4341)\n",
      "Epoch [203/300]\tTrain loss: 0.1408\tVal loss: 0.1854 (0.4565)\n",
      "Epoch [204/300]\tTrain loss: 0.1249\tVal loss: 0.1822 (0.4411)\n",
      "Epoch [205/300]\tTrain loss: 0.1396\tVal loss: 0.1808 (0.4999)\n",
      "Epoch [206/300]\tTrain loss: 0.1344\tVal loss: 0.1934 (0.4825)\n",
      "Epoch [207/300]\tTrain loss: 0.1510\tVal loss: 0.2347 (0.4853)\n",
      "Epoch [208/300]\tTrain loss: 0.1437\tVal loss: 0.1942 (0.4537)\n",
      "Epoch [209/300]\tTrain loss: 0.1306\tVal loss: 0.1711 (0.4093)\n",
      "Epoch [210/300]\tTrain loss: 0.1337\tVal loss: 0.1666 (0.4781)\n",
      "Epoch [211/300]\tTrain loss: 0.1254\tVal loss: 0.1682 (0.4704)\n",
      "Epoch [212/300]\tTrain loss: 0.1295\tVal loss: 0.1732 (0.4738)\n",
      "Epoch [213/300]\tTrain loss: 0.1219\tVal loss: 0.1780 (0.4420)\n",
      "Epoch [214/300]\tTrain loss: 0.1331\tVal loss: 0.1648 (0.4609)\n",
      "Epoch [215/300]\tTrain loss: 0.1355\tVal loss: 0.2216 (0.4629)\n",
      "Epoch [216/300]\tTrain loss: 0.1357\tVal loss: 0.1704 (0.4761)\n",
      "Epoch [217/300]\tTrain loss: 0.1344\tVal loss: 0.1719 (0.4274)\n",
      "Epoch [218/300]\tTrain loss: 0.1341\tVal loss: 0.1833 (0.4841)\n",
      "Epoch [219/300]\tTrain loss: 0.1281\tVal loss: 0.1737 (0.4786)\n",
      "Epoch [220/300]\tTrain loss: 0.1346\tVal loss: 0.2115 (0.4436)\n",
      "Epoch [221/300]\tTrain loss: 0.1311\tVal loss: 0.2115 (0.5556)\n",
      "Epoch [222/300]\tTrain loss: 0.1453\tVal loss: 0.1794 (0.4411)\n",
      "Epoch [223/300]\tTrain loss: 0.1580\tVal loss: 0.1787 (0.4650)\n",
      "Epoch [224/300]\tTrain loss: 0.1208\tVal loss: 0.1702 (0.4423)\n",
      "Epoch [225/300]\tTrain loss: 0.1229\tVal loss: 0.2057 (0.4454)\n",
      "Epoch [226/300]\tTrain loss: 0.1573\tVal loss: 0.1766 (0.4694)\n",
      "Epoch [227/300]\tTrain loss: 0.1316\tVal loss: 0.2195 (0.5100)\n",
      "Epoch [228/300]\tTrain loss: 0.1424\tVal loss: 0.2120 (0.5399)\n",
      "Epoch [229/300]\tTrain loss: 0.1511\tVal loss: 0.1947 (0.4712)\n",
      "Epoch [230/300]\tTrain loss: 0.1490\tVal loss: 0.2135 (0.4277)\n",
      "Epoch [231/300]\tTrain loss: 0.1259\tVal loss: 0.1671 (0.5199)\n",
      "Epoch [232/300]\tTrain loss: 0.1241\tVal loss: 0.1720 (0.4664)\n",
      "Epoch [233/300]\tTrain loss: 0.1660\tVal loss: 0.2318 (0.3699)\n",
      "Epoch [234/300]\tTrain loss: 0.1380\tVal loss: 0.1780 (0.4934)\n",
      "Epoch [235/300]\tTrain loss: 0.1311\tVal loss: 0.1685 (0.4427)\n",
      "Epoch [236/300]\tTrain loss: 0.1506\tVal loss: 0.1707 (0.4914)\n",
      "Epoch [237/300]\tTrain loss: 0.1291\tVal loss: 0.2273 (0.5950)\n",
      "Epoch [238/300]\tTrain loss: 0.1359\tVal loss: 0.1772 (0.4532)\n",
      "Epoch [239/300]\tTrain loss: 0.1417\tVal loss: 0.1730 (0.4630)\n",
      "Epoch [240/300]\tTrain loss: 0.1455\tVal loss: 0.1794 (0.5127)\n",
      "Epoch [241/300]\tTrain loss: 0.1232\tVal loss: 0.1978 (0.5036)\n",
      "Epoch [242/300]\tTrain loss: 0.1612\tVal loss: 0.2463 (0.5744)\n",
      "Epoch [243/300]\tTrain loss: 0.1344\tVal loss: 0.2425 (0.5559)\n",
      "Epoch [244/300]\tTrain loss: 0.1353\tVal loss: 0.1783 (0.4534)\n",
      "Epoch [245/300]\tTrain loss: 0.1214\tVal loss: 0.1893 (0.5112)\n",
      "Epoch [246/300]\tTrain loss: 0.1435\tVal loss: 0.2239 (0.5056)\n",
      "Epoch [247/300]\tTrain loss: 0.1483\tVal loss: 0.2170 (0.5458)\n",
      "Epoch [248/300]\tTrain loss: 0.1300\tVal loss: 0.1745 (0.4678)\n",
      "Epoch [249/300]\tTrain loss: 0.1475\tVal loss: 0.2000 (0.4499)\n",
      "Epoch [250/300]\tTrain loss: 0.1132\tVal loss: 0.1923 (0.5817)\n",
      "Epoch [251/300]\tTrain loss: 0.1369\tVal loss: 0.1985 (0.5364)\n",
      "Epoch [252/300]\tTrain loss: 0.1319\tVal loss: 0.1840 (0.4871)\n",
      "Epoch [253/300]\tTrain loss: 0.1223\tVal loss: 0.1763 (0.4897)\n",
      "Epoch [254/300]\tTrain loss: 0.1313\tVal loss: 0.1702 (0.4778)\n",
      "Epoch [255/300]\tTrain loss: 0.1192\tVal loss: 0.1778 (0.4626)\n",
      "Epoch [256/300]\tTrain loss: 0.1117\tVal loss: 0.1651 (0.4470)\n",
      "Epoch [257/300]\tTrain loss: 0.1225\tVal loss: 0.1660 (0.4893)\n",
      "Epoch [258/300]\tTrain loss: 0.1148\tVal loss: 0.1950 (0.4220)\n",
      "Epoch [259/300]\tTrain loss: 0.1277\tVal loss: 0.1642 (0.4556)\n",
      "Epoch [260/300]\tTrain loss: 0.1344\tVal loss: 0.1936 (0.4227)\n",
      "Epoch [261/300]\tTrain loss: 0.1556\tVal loss: 0.1896 (0.4840)\n",
      "Epoch [262/300]\tTrain loss: 0.1201\tVal loss: 0.1819 (0.4833)\n",
      "Epoch [263/300]\tTrain loss: 0.1140\tVal loss: 0.1859 (0.4340)\n",
      "Epoch [264/300]\tTrain loss: 0.1385\tVal loss: 0.1724 (0.4234)\n",
      "Epoch [265/300]\tTrain loss: 0.1222\tVal loss: 0.1751 (0.5051)\n",
      "Epoch [266/300]\tTrain loss: 0.1268\tVal loss: 0.2119 (0.3965)\n",
      "Epoch [267/300]\tTrain loss: 0.1152\tVal loss: 0.1761 (0.4449)\n",
      "Epoch [268/300]\tTrain loss: 0.1255\tVal loss: 0.1739 (0.5028)\n",
      "Epoch [269/300]\tTrain loss: 0.1402\tVal loss: 0.1771 (0.4807)\n",
      "Epoch [270/300]\tTrain loss: 0.1137\tVal loss: 0.1993 (0.5211)\n",
      "Epoch [271/300]\tTrain loss: 0.1384\tVal loss: 0.1668 (0.4233)\n",
      "Epoch [272/300]\tTrain loss: 0.1183\tVal loss: 0.1899 (0.5042)\n",
      "Epoch [273/300]\tTrain loss: 0.1187\tVal loss: 0.1739 (0.5039)\n",
      "Epoch [274/300]\tTrain loss: 0.1129\tVal loss: 0.1805 (0.4826)\n",
      "Epoch [275/300]\tTrain loss: 0.1404\tVal loss: 0.2184 (0.4672)\n",
      "Epoch [276/300]\tTrain loss: 0.1309\tVal loss: 0.2652 (0.6578)\n",
      "Epoch [277/300]\tTrain loss: 0.1463\tVal loss: 0.1670 (0.4407)\n",
      "Epoch [278/300]\tTrain loss: 0.1198\tVal loss: 0.2039 (0.4915)\n",
      "Epoch [279/300]\tTrain loss: 0.1367\tVal loss: 0.1720 (0.4961)\n",
      "Epoch [280/300]\tTrain loss: 0.1162\tVal loss: 0.1774 (0.4199)\n",
      "Epoch [281/300]\tTrain loss: 0.1143\tVal loss: 0.1707 (0.4308)\n",
      "Epoch [282/300]\tTrain loss: 0.1399\tVal loss: 0.1826 (0.4674)\n",
      "Epoch [283/300]\tTrain loss: 0.1143\tVal loss: 0.1620 (0.4858)\n",
      "Epoch [284/300]\tTrain loss: 0.1057\tVal loss: 0.1667 (0.4427)\n",
      "Epoch [285/300]\tTrain loss: 0.1127\tVal loss: 0.1798 (0.5066)\n",
      "Epoch [286/300]\tTrain loss: 0.1339\tVal loss: 0.1827 (0.5355)\n",
      "Epoch [287/300]\tTrain loss: 0.1149\tVal loss: 0.1604 (0.4530)\n",
      "Epoch [288/300]\tTrain loss: 0.1135\tVal loss: 0.1710 (0.4383)\n",
      "Epoch [289/300]\tTrain loss: 0.1221\tVal loss: 0.1908 (0.4948)\n",
      "Epoch [290/300]\tTrain loss: 0.1391\tVal loss: 0.1847 (0.5141)\n",
      "Epoch [291/300]\tTrain loss: 0.1113\tVal loss: 0.2195 (0.5734)\n",
      "Epoch [292/300]\tTrain loss: 0.1231\tVal loss: 0.1713 (0.4839)\n",
      "Epoch [293/300]\tTrain loss: 0.1219\tVal loss: 0.2020 (0.5027)\n",
      "Epoch [294/300]\tTrain loss: 0.1396\tVal loss: 0.1844 (0.5197)\n",
      "Epoch [295/300]\tTrain loss: 0.1094\tVal loss: 0.1792 (0.5070)\n",
      "Epoch [296/300]\tTrain loss: 0.1084\tVal loss: 0.1680 (0.4382)\n",
      "Epoch [297/300]\tTrain loss: 0.1099\tVal loss: 0.1626 (0.5421)\n",
      "Epoch [298/300]\tTrain loss: 0.1189\tVal loss: 0.1683 (0.4413)\n",
      "Epoch [299/300]\tTrain loss: 0.1093\tVal loss: 0.1887 (0.4530)\n",
      "Epoch [300/300]\tTrain loss: 0.1119\tVal loss: 0.1618 (0.4404)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "num_epochs = 300\n",
    "batch_size = 128\n",
    "\n",
    "cc.load_mat_atom_feats()\n",
    "list_test_mae = list()\n",
    "list_test_rmse = list()\n",
    "\n",
    "train_data, val_data, test_data = cc.load_dataset(root_data, fn='id_target.ins.csv', target_idx=3, ref_idx=1, radius=8, train_ratio=0.7, val_ratio=0.2, model_type='cgcnn')\n",
    "\n",
    "for mtype in ['cx','cr']:\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=tr.collate_cgcnn)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=tr.collate_cgcnn)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate_cgcnn)\n",
    "\n",
    "    if mtype == 'cx':\n",
    "        model = CGCNN(cc.num_atom_feats, cc.num_edge_feats, 1).cuda()\n",
    "        odir  = 'cgcnn_x'\n",
    "    elif mtype == 'cr':\n",
    "        model = CGCNNR(cc.num_atom_feats, cc.num_edge_feats, 1).cuda()\n",
    "        odir  = 'cgcnn_r'\n",
    "\n",
    "    optimizer = AdaBound(model.parameters(), lr=1e-6, weight_decay=1e-8)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    for i in range(99):\n",
    "        output_root = os.path.join(root_model, odir, '{:02d}'.format(i))\n",
    "        if not os.path.isdir(output_root): \n",
    "            os.makedirs(output_root)\n",
    "            break\n",
    "    writer = SummaryWriter(output_root)\n",
    "\n",
    "    for epoch in range(0, num_epochs):\n",
    "        train_loss = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "        val_loss, val_rmse, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "        print('Epoch [{}/{}]\\tTrain loss: {:.4f}\\tVal loss: {:.4f} ({:.4f})'\n",
    "                .format(epoch + 1, num_epochs, train_loss, val_loss, val_rmse))\n",
    "\n",
    "        writer.add_scalar('train/loss', train_loss, epoch)\n",
    "        writer.add_scalar('train/MAE', train_loss, epoch)\n",
    "        writer.add_scalar('valid/loss', val_loss, epoch)\n",
    "        writer.add_scalar('valid/MAE', val_loss, epoch)\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(output_root, 'model.{:05d}.pt'.format(epoch)))\n",
    "        if epoch%10 == 0:\n",
    "            _, _, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "            numpy.savetxt(os.path.join(output_root, 'pred.{:05d}.txt'.format(epoch)), numpy.hstack([idxs, targets, preds]), delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10387/10387 [09:01<00:00, 19.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300]\tTrain loss: 1.8539\tVal loss: 1.0065 (1.2785)\n",
      "Epoch [2/300]\tTrain loss: 0.9246\tVal loss: 0.8814 (1.1015)\n",
      "Epoch [3/300]\tTrain loss: 0.8789\tVal loss: 0.9009 (1.1710)\n",
      "Epoch [4/300]\tTrain loss: 0.8613\tVal loss: 0.8340 (1.0819)\n",
      "Epoch [5/300]\tTrain loss: 0.8493\tVal loss: 0.9106 (1.1205)\n",
      "Epoch [6/300]\tTrain loss: 0.8431\tVal loss: 0.8157 (1.0734)\n",
      "Epoch [7/300]\tTrain loss: 0.8286\tVal loss: 0.7969 (1.0803)\n",
      "Epoch [8/300]\tTrain loss: 0.8071\tVal loss: 0.7926 (1.0587)\n",
      "Epoch [9/300]\tTrain loss: 0.7930\tVal loss: 0.7890 (1.0841)\n",
      "Epoch [10/300]\tTrain loss: 0.7928\tVal loss: 0.7630 (1.0649)\n",
      "Epoch [11/300]\tTrain loss: 0.7846\tVal loss: 0.7678 (1.0503)\n",
      "Epoch [12/300]\tTrain loss: 0.7770\tVal loss: 0.7741 (1.0987)\n",
      "Epoch [13/300]\tTrain loss: 0.7691\tVal loss: 0.7413 (1.0706)\n",
      "Epoch [14/300]\tTrain loss: 0.7788\tVal loss: 0.7965 (1.0374)\n",
      "Epoch [15/300]\tTrain loss: 0.7531\tVal loss: 0.7800 (1.1523)\n",
      "Epoch [16/300]\tTrain loss: 0.7480\tVal loss: 0.7654 (1.1164)\n",
      "Epoch [17/300]\tTrain loss: 0.7363\tVal loss: 0.7165 (1.0735)\n",
      "Epoch [18/300]\tTrain loss: 0.7363\tVal loss: 0.7376 (1.0766)\n",
      "Epoch [19/300]\tTrain loss: 0.7266\tVal loss: 0.7006 (1.0554)\n",
      "Epoch [20/300]\tTrain loss: 0.7205\tVal loss: 0.7138 (1.0323)\n",
      "Epoch [21/300]\tTrain loss: 0.7111\tVal loss: 0.7002 (1.0684)\n",
      "Epoch [22/300]\tTrain loss: 0.7046\tVal loss: 0.7193 (1.0700)\n",
      "Epoch [23/300]\tTrain loss: 0.6846\tVal loss: 0.6840 (1.0165)\n",
      "Epoch [24/300]\tTrain loss: 0.6848\tVal loss: 0.6903 (1.0310)\n",
      "Epoch [25/300]\tTrain loss: 0.6888\tVal loss: 0.8157 (1.2300)\n",
      "Epoch [26/300]\tTrain loss: 0.6793\tVal loss: 0.7002 (1.0426)\n",
      "Epoch [27/300]\tTrain loss: 0.6679\tVal loss: 0.6876 (1.0837)\n",
      "Epoch [28/300]\tTrain loss: 0.6618\tVal loss: 0.7497 (1.1331)\n",
      "Epoch [29/300]\tTrain loss: 0.6733\tVal loss: 0.6707 (0.9970)\n",
      "Epoch [30/300]\tTrain loss: 0.6644\tVal loss: 0.6904 (1.0090)\n",
      "Epoch [31/300]\tTrain loss: 0.6640\tVal loss: 0.6927 (1.0674)\n"
     ]
    }
   ],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()\n",
    "num_epochs = 300\n",
    "batch_size = 32\n",
    "\n",
    "cc.load_mat_atom_feats()\n",
    "list_test_mae = list()\n",
    "list_test_rmse = list()\n",
    "\n",
    "train_data, val_data, test_data = cc.load_dataset(root_data, fn='id_target.ins.csv', target_idx=3, ref_idx=1, radius=4, train_ratio=0.7, val_ratio=0.2, model_type='tgnn')\n",
    "\n",
    "for mtype in ['tx','tr']:\n",
    "    train_data_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True, collate_fn=tr.collate)\n",
    "    val_data_loader = DataLoader(val_data, batch_size=batch_size, collate_fn=tr.collate)\n",
    "    test_data_loader = DataLoader(test_data, batch_size=batch_size, collate_fn=tr.collate)\n",
    "\n",
    "    if mtype == 'tx':\n",
    "        model = TGNNX(cc.num_atom_feats, cc.num_bond_feats, 1).cuda()\n",
    "        odir  = 'tgnn_x'\n",
    "    elif mtype == 'tr':\n",
    "        model = TGNN(cc.num_atom_feats, cc.num_bond_feats, 1).cuda()\n",
    "        odir  = 'tgnn_r'\n",
    "\n",
    "    optimizer = AdaBound(model.parameters(), lr=1e-6, weight_decay=1e-8)\n",
    "    # optimizer = optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-8)\n",
    "    criterion = torch.nn.L1Loss()\n",
    "\n",
    "    for i in range(99):\n",
    "        output_root = os.path.join(root_model, odir, '{:02d}'.format(i))\n",
    "        if not os.path.isdir(output_root): \n",
    "            os.makedirs(output_root)\n",
    "            break\n",
    "    writer = SummaryWriter(output_root)\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        train_loss = tr.train(model, optimizer, train_data_loader, criterion)\n",
    "        val_loss, val_rmse, _, _, _ = tr.test(model, val_data_loader, criterion)\n",
    "        print('Epoch [{}/{}]\\tTrain loss: {:.4f}\\tVal loss: {:.4f} ({:.4f})'\n",
    "                .format(epoch, num_epochs, train_loss, val_loss, val_rmse))\n",
    "\n",
    "        writer.add_scalar('train/loss', train_loss, epoch)\n",
    "        writer.add_scalar('train/MAE', train_loss, epoch)\n",
    "        writer.add_scalar('valid/loss', val_loss, epoch)\n",
    "        writer.add_scalar('valid/MAE', val_loss, epoch)\n",
    "\n",
    "        if epoch%20 == 0:\n",
    "            torch.save(model.state_dict(), os.path.join(output_root, 'model.{:05d}.pt'.format(epoch)))\n",
    "        if epoch%10 == 0:\n",
    "            _, _, idxs, targets, preds = tr.test(model, test_data_loader, criterion)\n",
    "            numpy.savetxt(os.path.join(output_root, 'pred.{:05d}.txt'.format(epoch)), numpy.hstack([idxs, targets, preds]), delimiter=',')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d8d7440ee69033cedfb03ec9102fb4d61ffc9ff7ad7c8cf3ed72a45b7e84428f"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('test01')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
